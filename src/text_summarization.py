# -*- coding: utf-8 -*-
"""text_summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1njZ6GMTqN-Q-pFo1x0CfkvFesYBhmzF7

## Import Library
"""

import pandas as pd
import numpy as np
import torch
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from google.colab import drive
drive.mount('/content/drive')

"""## Data Processing

#### Paragraphs texts
"""

paragraphs = pd.read_json("/content/drive/MyDrive/vip_project_spring_2023/data/textbook_df.json")

paragraphs

paragraphs["Textbook_Data"][0]

length = len(paragraphs)
length

"""#### Sub_chapters in the textbook"""

sections = {i for i in paragraphs["Chapter.Section"]}
sections = [s for s in sections]
sections.sort()
print(sections)

"""#### Explore with Chapter 1 - Section 0"""

section_1_0_text = [paragraphs["Textbook_Data"][i] for i in range(length) if paragraphs["Chapter.Section"][i] == 1.0]
section_1_0_text

section_1_0_string = ''
for i in section_1_0_text:
  section_1_0_string += ' ' + str(i)
section_1_0_string

"""#### Append paragraphs of each sub-chapters to the string of the whole sub_chapter"""

def to_string(section):
  section_text = [paragraphs["Textbook_Data"][i] for i in range(length) if paragraphs["Chapter.Section"][i] == section]
  section_string = ''
  for i in section_text:
    section_string += ' ' + str(i)
  return section_string

to_string(1.0)

"""## Text Summarization algorithm

#### Import NLTK
"""

from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize, sent_tokenize
sentences = sent_tokenize(section_1_0_string)
sentences

def create_dictionary_table(text_string) -> dict:
   
    # Removing stop words
    stop_words = set(stopwords.words("english"))
    stop_words.add(';')
    stop_words.add('“')
    stop_words.add('.')

    stop_words.add('(')
    stop_words.add(')')
    stop_words.add(',')
    stop_words.add('”')
    # stop_words.add('※')
    stop_words.add(':')
    stop_words.add('{')
    stop_words.add('}')
    
    words = word_tokenize(text_string.lower())
    
    # Reducing words to their root form
    # stemer = SnowballStemmer("english")
    
    # Creating dictionary for the word frequency table
    frequency_table = dict()
    for wd in words:
        # wd = stemer.stem(wd)
        if wd in stop_words:
            continue
        if wd in frequency_table:
            frequency_table[wd] += 1
        else:
            frequency_table[wd] = 1

    return frequency_table

frequency_table = create_dictionary_table(section_1_0_string)
frequency_table

def _calculate_sentence_scores(sentences, frequency_table) -> dict:   

    # Algorithm for scoring a sentence by its words
    sentence_weight = dict()

    for sentence in sentences:
        sentence_wordcount = (len(word_tokenize(sentence)))
        sentence_wordcount_without_stop_words = 0
        for word_weight in frequency_table:
            if word_weight in sentence.lower():
                sentence_wordcount_without_stop_words += 1
                if sentence[:7] in sentence_weight:
                    sentence_weight[sentence[:7]] += frequency_table[word_weight]
                else:
                    sentence_weight[sentence[:7]] = frequency_table[word_weight]

        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words
      
    return sentence_weight

def _calculate_threshold(sentence_weight, length) -> int:
  
    weight = list(sentence_weight.values())
    if len(weight) <= length:
      #min
      th = np.min(weight)
    else:
      weight.sort(reverse=True)
      th = weight[length-1]

    return th

weights = _calculate_sentence_scores(sentences, frequency_table)
weights

threshold = _calculate_threshold(weights, 4)
threshold

def _get_article_summary(sentences, sentence_weight, threshold):
    sentence_counter = 0
    article_summary = ''

    for sentence in sentences:
        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):
            article_summary += " " + sentence
            sentence_counter += 1

    return article_summary

_get_article_summary(sentences, weights, threshold)

"""## **Apply** for all sub_chapters"""

summary = {}
length_of_summary = []
for s in sections:
  section_string = to_string(s)
  section_sentences = sent_tokenize(section_string)
  section_frequency_table = create_dictionary_table(section_string)
  secction_sentence_weights = _calculate_sentence_scores(section_sentences, section_frequency_table)
  section_threshold = _calculate_threshold(secction_sentence_weights, 4)
  summary[str(s)] = "Chapter "+ str(s) + ":"+_get_article_summary(section_sentences, secction_sentence_weights, section_threshold)
  sum_len = len(sent_tokenize(summary[str(s)]))
  length_of_summary.append(len(sent_tokenize(summary[str(s)])))
  # if sum_len > 7:
  #   section_threshold = _calculate_threshold(secction_sentence_weights, 1)
  #   summary[str(s)] = _get_article_summary(section_sentences, secction_sentence_weights, section_threshold)
  #   sum_len = len(sent_tokenize(summary[str(s)]))
  if len(sent_tokenize(summary[str(s)])) == 0:
    print("NULL")

summary

"""## Evaluate length of summary"""

print(length_of_summary)

"""#### Average length of summary"""

np.average(length_of_summary)

"""#### Export summary to csv file"""

from google.colab import files
l = [(k, v) for k, v in summary.items()]
df = pd.DataFrame(l, columns=["sub_chapter",'summarization'])
df.to_csv('summary.csv')
files.download('summary.csv')