{
    "Textbook_Data": {
        "0": "CHAPTER -2 Complex Numbers ",
        "1": "The basic manipulations of complex numbers are reviewed in this appendix. The algebraic rules for combining complex numbers are reviewed, and then a geometric viewpoint is taken to visualize these operations by drawing vector diagrams. This geometric view is a key to understanding how complex numbers can be used to represent signals. Specifically, the following three significant ideas about complex numbers are treated in this appendix:",
        "2": " Simple algebraic rules: Operations on complex numbers follow exactly the same rules as real numbers, with replaced everywhere by. <a type=\"button\" class=\"flink\" data-toggle=\"popover\" data-trigger=\"hover\" data-placement=\"top\" data-html=\"true\" data-content=\"Mathematicians and physicists use the symbol for ; electrical engineers prefer to reserve the symbol for current in electric circuits. }",
        "3": " Elimination of trigonometry: Eulers formula for the complex exponential provides a connection between trigonometric identities and simple algebraic operations on complex numbers.",
        "4": " Representation by vectors: A vector drawn from the origin to a point in a two-dimensional plane is equivalent to. The algebraic rules for are, in effect, the basic rules of vector operations. More important, however, is the visualization gained from the vector diagrams. ",
        "5": "The first two ideas concern the algebraic nature of, the other its role as a representer of signals. Skill in algebraic manipulations is important, but the use of complex numbers in representation is more important in the long run. Complex numbers in electrical engineering are used as a convenience because when they stand for the sinusoidal signals they can simplify manipulations of the signals. Thus a sinusoidal problem (such as the solution to a differential equation) is converted into a complex number problem that can be (1) solved by the simple rules of algebra and (2) visualized through vector geometry. The key to all this is the higher-order thinking that permits abstraction of the problem into the world of complex numbers. Ultimately, we are led to the notion of a \u201ctransform\u201d such as the Fourier or Laplace transform to reduce many other sophisticated problems to algebra. Because complex numbers are so crucial to almost everything we do in the study of signals and systems, a careful study of this appendix is advised.",
        "6": "Once such insight is gained, it still will be necessary to return occasionally to the lower-level drudgery of calculations. When you have to manipulate complex numbers, a calculator will be most useful, especially one with built-in complex arithmetic capability. It is worthwhile to learn how to use this feature on your calculator. However, it is also important to do some calculations by hand, so that you will understand what your calculator is doing!",
        "7": "Finally, it is too bad that complex numbers are called \u201ccomplex.\u201d Most students, therefore, think of them as complicated. However, their elegant mathematical properties usually simplify calculations quite a bit.",
        "8": "Introduction A complex number system is an extension of the real number system. Complex numbers are necessary to solve equations such as The symbol is introduced to stand for, so the previous equation has the two solutions. More generally, complex numbers are needed to solve for the two roots of a quadratic equation which, according to the quadratic formula, has the two solutions: Whenever the discriminant is negative, the solution must be expressed as a complex number. For example, the roots of are, because ",
        "9": "Notation for Complex Numbers Several different mathematical notations can be used to represent complex numbers. The two basic types are polar form and rectangular form. Converting between them quickly and easily is an important skill.",
        "10": "Rectangular Form In rectangular form, all of the following notations define the same complex number. The ordered pair can be interpreted as a point in the two-dimensional plane. A complex number can also be drawn as a vector whose tail is at the origin and whose head is at the point, in which case is the horizontal coordinate of the vector and the vertical coordinate (see Fig.~\u203b).",
        "11": " Complex number represented as a vector from the origin to. Figure~\u203b shows some numerical examples.",
        "12": " Complex numbers plotted as vectors in the two-dimensional \u201ccomplex plane.\u201d Each is represented by a vector from the origin to the point with coordinates in the complex plane. The complex number is represented by the point, which lies in the first quadrant of the two-dimensional plane; likewise, lies in the fourth quadrant at the location. Since the complex number notation represents the point in the two-dimensional plane, the number represents, which is drawn as a vertical vector from the origin up to, as in Fig.~\u203b. Thus multiplying a real number, such as, by changes it from pointing along the horizontal axis to pointing vertically, i.e.,.",
        "13": "Rectangular form is also referred to as Cartesian form. The horizontal coordinate is called the real part, and the vertical coordinate the imaginary part. The operators and are provided to extract the real and imaginary parts of :",
        "14": " Polar Form In polar form, the vector is defined by its length and its direction as in Fig.~\u203b. Therefore, we use the following descriptive notation sometimes: Some examples are shown in Fig.~\u203b where the direction is given in degrees.",
        "15": " Several complex numbers plotted in terms of length and direction of their vector representation. The angle is always measured with respect to the positive real axis; its units are usually radians, but are shown as degrees in this case. The angle is always measured from the positive -axis and may be either positive (counterclockwise) or negative (clockwise). However, we generally specify the principal value of the angle so that. This requires that integer multiples of be subtracted from or added to the angle until the result is between and. Thus the vector is the principal value of.",
        "16": "Conversion: Rectangular and Polar Both the polar and rectangular forms are commonly used to represent complex numbers. The prevalence of the polar form, for sinusoidal signal representation, makes it necessary to convert quickly and accurately between the two representations.",
        "17": " Demo: ZDrill From Fig.~\u203b, we see that the and coordinates of the vector are given by",
        "18": " Therefore, a valid formula for is Basic layout for relating Cartesian coordinates to polar coordinates and. EXAMPLE:\u00a0 Polar to Rectangular In Fig.~\u203b, the three complex numbers are The conversion from polar to rectangular was done via. The conversion from to is a bit trickier. From Fig.~\u203b, the formulas are The arctangent must give a four-quadrant answer, and the direction is usually given in radians rather than degrees.",
        "19": "At this point, the reader should convert to polar form the five complex numbers shown in Fig.~\u203b. The answers, in a random order, are:,,,, and. In Section~\u203b, we will introduce two other polar notations: where is called the magnitude of the vector and is its phase in radians (not degrees). This exponential notation, which relies on Eulers formula, has the advantage that when it is used in algebraic expressions, the standard laws of exponents apply.",
        "20": "Difficulty in Second or Third Quadrant The formula for the angle as the must be used with care, especially when the real part is negative (see Fig.~\u203b).",
        "21": " In the second quadrant, the interior angle is easily calculated from and, but is not the correct angle for the polar form, which requires the exterior angle with respect to the positive real axis. For example, the complex number would require that we evaluate to get the angle; the same calculation would be needed if. The arctangent of is rad, or about, which is the correct angle for. However, for, the vector lies in the second quadrant and the angle must satisfy. We get the correct angle by adding. In this case, the correct angle is rad, or about. The general method for getting the angle of in polar form is Since and are both angles to, when we should pick the appropriate one to make the final answer satisfy.",
        "22": "Eulers Formula The conversion from polar to rectangular form suggests the following formula: Equation defines the complex exponential, which is equivalent to, i.e., a vector of length 1 at angle. A proof of Eulers formula based on power series is outlined in Problem 2.4 of Chapter \u203b.",
        "23": "The amazing discovery was that the laws of exponents apply to. Eulers formula is so important that it must be instantly recalled; likewise, the inverse Euler formulas and should also be committed to memory.",
        "24": "EXAMPLE:\u00a0 Eulers Formula Here are some examples: EXAMPLE:\u00a0 Degrees to Radians Referring back to Fig.~\u203b, the three complex numbers can be rewritten as: Numbers like rad are difficult to visualize, because we are more used to thinking of angles in degrees. It may be helpful to express the angles used in the exponents as a fraction of rad, i.e., rad. This is a good habit to adopt, because it simplifies the conversion between degrees and radians. If is given in radians, the conversion is Inverse Euler Formulas Eulers formula can be solved separately for the cosine and sine parts. The result will be called the inverse Euler relations Recalling that and, the proof of the version is as follows: ",
        "25": "Algebraic Rules for Complex Numbers The basic arithmetic operators for complex numbers follow the usual rules of algebra as long as the symbol is treated as a special token that satisfies. In rectangular form, all of these rules are relatively straightforward. The five fundamental rules are the following:",
        "26": " Addition and subtraction are straightforward because we need only add or subtract the real and imaginary parts. On the other hand, addition (or subtraction) in polar form cannot be carried out directly on and ; instead, an intermediate conversion to rectangular form is required. In contrast, the operations of multiplication and division, which are rather messy in rectangular form, reduce to simple manipulations in polar form. For multiplication, multiply the magnitudes and add the angles; for division, divide the magnitudes and subtract the angles. The conjugate in polar form requires only a change of sign of the angle. The inverse or reciprocal of a complex number is the number such that A common mistake with the inverse is to invert by taking the inverse of and separately. To show that this is wrong, take the specific case where and. Show that is not the inverse of, because. Determine the correct inverse of. Polar form presents difficulties when adding (or subtracting) two complex numbers and expressing the final answer in polar form. An intermediate conversion to rectangular form must be done. Here is the recipe for adding complex numbers in polar form.",
        "27": " Starting in polar form, we have Convert both and to Cartesian form: Perform the addition in Cartesian form: Identify the real and imaginary parts of : Convert back to polar form using and : If you have a calculator that converts between polar and rectangular form, learn how to use it; it will save many hours of hand calculation and also be more accurate. Most \u201cscientific\u201d calculators even have the capability to use both notations, so the conversion is transparent to the user.",
        "28": "EXAMPLE:\u00a0 Adding Polar Forms Here is a numerical example of adding two complex numbers given in polar form: Remember: When the angle appears in the exponent, its units must be in radians. Complex Number Exercises To practice computations for complex numbers, try the following exercises.",
        "29": "Add and multiply the following, then plot the results: The answers are and. For the conjugate the simple rule is to change the sign of all the terms. Work the following: Prove that the following identities are true: More drill problems can be generated by using the MATLAB program zdrill.m, which presents a GUI (graphical user interface) that asks questions for each of the complex operations, and also plots the vectors that represent the solutions. The zdrill GUI has both a novice and an advanced level.",
        "30": "A screen shot of the GUI is shown in Fig.~\u203b. Demo: ZDrill ",
        "31": "Geometric Views of Complex Operations It is important to develop an ability to visualize complex number operations. This is done by plotting the vectors that represent the numbers in the plane, where and. The key to this is to recall that, as shown in Fig.~\u203b, the complex number is a vector whose tail is at the origin and whose head is at.",
        "32": " The MATLAB GUI zdrill for practicing complex number operations. Geometric View of Addition For complex addition,, both and are viewed as vectors with their tails at the origin. The sum is the result of vector addition, and is constructed as follows (see Fig.~\u203b):",
        "33": " Draw a copy of with its tail at the head of. Call this displaced vector. Draw the vector from the origin to the head of. This vector is the sum. Graphical construction of complex number addition. This method of addition can be generalized to many vectors. Figure~\u203b shows the result of adding the four complex numbers where the answer happens to be zero.",
        "34": " Result of adding the four vectors shown in (a) is zero. The \u201chead-to-tail\u201d graphical method is illustrated in (b). Geometric View of Subtraction The visualization of subtraction requires that we interpret the operation as addition: Thus we must flip to and then add the result to. This method is shown in Fig.~\u203b and is equivalent to the visualization of addition, as in Fig.~\u203b. There are two additional comments to be made about subtraction: Since, a displaced version of the difference vector could be drawn with its tail at the head of and its head at the head of. The triangle with vertices at the three points 0,, and has sides equal to,, and. ",
        "35": " Geometric view of subtraction. The vector is flipped to make and then a displaced version is added to to obtain. Define a vector to have its tail at and its head at. Prove that has the same length and direction as. Prove the triangle inequality: Use either an algebraic method by squaring both sides, or a geometric argument based on the intuitive idea that \u201cthe shortest distance between two points is a straight line.\u201d Geometric View of Multiplication It is best to view multiplication in terms of polar form where we multiply the magnitudes, and add the angles. In order to draw the product vector, we must decide whether or not and/or are greater than 1. In Fig.~\u203b, it is assumed that only is larger than 1.",
        "36": " Geometric view of complex multiplication. A special case occurs when, because then there is no scaling, and the multiplication by becomes a rotation. Figure~\u203b shows the case where, which gives a rotation by or, because.",
        "37": " Complex multiplication becomes a rotation when. The case where gives a rotation by. Geometric View of Division Division is very similar to the visualization of multiplication, except that we must now subtract the angles and divide the magnitudes (Fig.~\u203b). Complex multiplication becomes a rotation when. The case where gives a rotation by. Given two complex numbers and, as in Fig.~\u203b, where the angle between them is and the magnitude of is twice that of. Evaluate. Geometric View of the Inverse, This is a special case of division where, so we just negate the angle and take the reciprocal of the magnitude. Refer to Fig.~\u203b for examples of the inverse.",
        "38": " Graphical construction of complex number inverse. For the vectors shown, and. Geometric View of the Conjugate, In this case, we negate the angle, which has the effect of flipping the vector about the horizontal axis. The length of the vector remains the same. Figure \u203b shows two examples of the geometric interpretation of complex conjugation.",
        "39": " Graphical construction of the complex conjugate ; only the imaginary part changes sign. The vectors flip about the real axis: flips down, and flips up. Prove the following fact: Plot an example for ; also plot and.",
        "40": "Powers and Roots Integer powers of a complex number can be defined in the following manner: In other words, the rules of exponents still apply, so the angle is multiplied by and the magnitude is raised to the th power. Figure~\u203b shows a sequence of these: where the angle steps by a constant amount; in this case, exactly rad. The magnitude of is less than one, so the successive powers spiral in toward the origin. If, the points would spiral outward; if, all the powers would lie on the unit circle (a circle of radius 1). One famous identity is DeMoivres formula: The proof of this seemingly difficult trigonometric identity is actually trivial if we invoke Eulers formula for.",
        "41": " A sequence of powers for. Since, the vectors spiral in toward the origin. The angular change between successive powers is constant,. ",
        "42": "Let,, and be three consecutive members of a sequence such as shown in Fig.~\u203b. If and, plot the three numbers.",
        "43": "Roots of Unity In a surprising number of cases, the following equation must be solved: In this equation, is an integer. One solution is, but there are many others, because is equivalent to finding all the roots of the th-degree polynomial, which must have roots. It turns out that all the solutions are given by which are called the th roots of unity. As shown in Fig.~\u203b, these solutions are complex numbers equally spaced around the unit circle. The angular spacing between them is.",
        "44": " Graphical display of the th roots of unity ( ). These are the solutions of. Notice that there are only distinct roots. Procedure for Finding Multiple Roots Once we know that the solutions to are the th roots of unity, we can describe a structured approach to solving equations with multiple roots. In order to get a general result, we consider a slightly more general situation where is a complex constant,. The procedure involves the following six steps: Write as. Write as where is an integer. Note that when we would write the number 1 as Equate the two sides and solve for the magnitudes and angles separately. The magnitude is the positive th root of a positive number : The angle contains the interesting information, because there are different solutions: Thus the different solutions all have the same magnitude, but their angles are equally spaced with a difference of between each one. Graphical display of the th roots of unity ( ). These are the solutions of. Notice that there are only distinct roots. EXAMPLE:\u00a0 7-th Roots of Unity Solve, using the procedure above. Therefore, these solutions are equally spaced around the unit circle, as shown in Fig.~\u203b. In this case, the solutions are called the seventh roots of unity. Solve the following equation: Use the fact that. Plot all the solutions.",
        "45": "Summary and Links This appendix has presented a brief review of complex numbers and their visualization as vectors in the two-dimensional complex plane. Although this material should have been seen before by most students during high-school algebra, our intense use of complex notation demands much greater familiarity. The labs in Chapter 2 deal with various aspects of complex numbers, and also introduce MATLAB. In Lab #2, we also have included a number of MATLAB functions for plotting vectors from complex numbers ( zvect, zcat) and for changing between Cartesian and polar forms ( zprint).",
        "46": " Lab: #2, Adding Sinusoids via Complex Amplitudes The Complex Numbers via MATLAB demo is a quick reference to these routines.",
        "47": " Demo: Complex Numbers via MATLAB In addition to the labs, we have written a MATLAB GUI (graphical user interface) that will generate drill problems for each of the complex operations studied here: addition, subtraction, multiplication, division, inverse, and conjugate. A screen shot of the GUI is shown in Fig.~\u203b.",
        "48": " Demo: ZDrill ",
        "49": "CHAPTER -1 Programming in MATLAB MATLAB will be used extensively in the laboratory exercises on the CD-ROM and in Appendix \u203b. This appendix provides an overview of MATLAB and some of its capabilities. We focus on programming issues since a wealth of information is already available on syntax and basic commands. MATLAB has an extensive on-line help system, which can be used to answer any questions not answered in this brief presentation. In fact, an ideal way to read this appendix would be to have MATLAB running so that help can be used whenever necessary, and the examples can be run and modified.",
        "50": "MATLAB (short for Matrix Laboratory) is an environment for numerical analysis and computing. It originated as an interface to the collections of numerical routines from the LINPACK and EISPACK projects, but it is now a commercial product of The Mathworks, Inc. MATLAB has evolved into a powerful programming environment containing many built-in functions for doing signal processing, linear algebra, and other mathematical calculations. The language has also been extended by means of toolboxes containing additional functions for MATLAB. For example, the CD-ROM that accompanies this book contains a toolbox of functions needed for the laboratory exercises. The toolboxes are installed as separate directories within the MATLAB directory. Note: SP First Toolbox Please follow the instructions on the CD-ROM to install the SP First toolbox before doing the laboratory exercises.",
        "51": "Since MATLAB is extensible, users find it convenient to write new functions whenever the built-in functions fail to do a vital task. The programming necessary to create new functions and scripts is not too difficult if the user has some experience with C, PASCAL, or FORTRAN. This appendix gives a brief overview of MATLAB for the purpose of programming.",
        "52": "{Version 5.2.1 or greater is needed for SP First.}",
        "53": "MATLAB Help MATLAB provides an on-line help system accessible by using the help command. For example, to get information about the function filter, enter the following at the command prompt:",
        "54": " >> help filter ",
        "55": "The command prompt is indicated by >> in the command window. The help command will return text information in the command window. Help is also available for categories; for example, help punct summarizes punctuation as used in MATLABs syntax. The help system now has a Web-browser interface, so the commands helpdesk, helpwin and doc bring up this interface.",
        "56": "A useful command for getting started is intro, which covers the basic concepts in the MATLAB language. Also, there are many demonstration programs that illustrate the various capabilities of MATLAB; these can be started with the command demo.",
        "57": "Finally, if you are searching for other tutorials, some are freely available on the Web.",
        "58": "When unsure about a command, use help.",
        "59": "Matrix Operations and Variables The basic variable type in MATLAB is a matrix. To declare a variable, simply assign it a value at the MATLAB prompt. For example,",
        "60": " >> M = [1 2 6; 5 2 1] M = 1 2 6 5 2 1 When the definition of a matrix involves a long formula or many entries, then a very long MATLAB command can be broken onto two (or more) lines by placing an ellipses ( ) at the end of the line to be continued. For example,",
        "61": " P = [ 1, 2, 4, 6, 8 ] +... [ pi, 4, exp(1), 0, -1 ] +... [ cos(0.1*pi), sin(pi/3),... tan(3), atan(2), sqrt(pi) ];",
        "62": " If an expression is followed by a semicolon (;), then the result is not echoed to the screen. This is very useful when dealing with large matrices. Also note that ; is used to separate rows in a matrix definition. Likewise, spaces or commas (,) are used to separate column entries.",
        "63": "The size of the matrix can always be extracted with the size operator:",
        "64": " >> Msize = size(M) Msize = 2 3 Therefore, it becomes unnecessary to assign separate variables to track the number of rows and number of columns. Two special types of matrix variables are worthy of mention: scalars and vectors. A scalar is a matrix with only one element; its size is. A vector is a matrix that has only one row or one column. In the SP First laboratory exercises, signals will often be stored as vectors.",
        "65": "Individual elements of a matrix variable may be accessed by giving the row index and the column index, for example:",
        "66": " >> M13 = M(1,3) M13 = 6 Submatrices may be accessed in a similar manner by using the colon operator as explained in Section \u203b.",
        "67": "The Colon Operator The colon operator (:) is useful for creating index arrays, creating vectors of evenly spaced values, and accessing submatrices. Use help colon for a detailed description of its capabilities.",
        "68": "The colon notation is based on the idea that an index range can be generated by giving a start, a skip, and then the end. Therefore, a regularly spaced vector of numbers is obtained by means of",
        "69": " iii = start:skip:end Without the skip parameter, the default increment is 1. This sort of counting is similar to the notation used in FORTRAN DO loops. However, MATLAB takes it one step further by combining it with matrix indexing. For a matrix A, A(2,3) is the scalar element located at the 2nd row and 3rd column of A, so a submatrix can be extracted with A(2:5,1:3). The colon also serves as a wild card; i.e., A(2,:) is a row vector consisting of the entire 2nd row of the matrix A. Indexing backwards flips a vector, e.g., x(9:-1:1) for a length\u20139 vector. Finally, it is sometimes necessary to work with a list of all the values in a matrix, so A(:) gives a column vector that is merely the columns of A concatenated together. This is an example of reshaping the matrix. More general reshaping of the matrix A can be accomplished with the reshape(A,M,N) function. For example, the matrix A can be reshaped into a matrix with: Anew = reshape(A,12,6). Matrix and Array Operations The default operations in MATLAB are matrix operations. Thus A*B means matrix multiplication, which is defined and reviewed next.",
        "70": "A Review of Matrix Multiplication The operation of matrix multiplication can be carried out only if the two matrices have compatible dimensions, i.e., the number of columns in must equal the number of rows in. For example, a matrix can multiply an matrix to give a result that is. In general, if is, then must be, and the product would be. Usually matrix multiplication is not commutative, i.e.,. If, then the product cannot be defined, but even when is defined, we find that the commutative property applies only in special cases.",
        "71": "Each element in the product matrix is calculated with an inner product. To generate the first element in the product matrix,, simply take the first row of and multiply point by point with the first column of, and then sum. For example, if then the first element of is which is, in fact, the inner product between the first row of and the first column of. Likewise, is found by taking the inner product between the second row of and the first column of, and so on for and. The final result would be ",
        "72": "Some special cases of matrix multiplication are the outer product and the inner product. In the outer product, a column vector multiplies a row vector to give a matrix. If we let one of the vectors be all 1s, then we can get a repeating result: With all 1s in the row vector, we end up repeating the column vector four times.",
        "73": "For the inner product, a row vector multiplies a column vector, so the result is a scalar. If we let one of the vectors be all 1s, then we will sum the elements in the other vector: Pointwise Array Operations If we want to do a pointwise multiplication between two arrays, some confusion can arise. In the pointwise case, we want to multiply the matrices together element-by-element, so they must have exactly the same size in both dimensions. For example, two matrices can be multiplied pointwise, although we cannot do matrix multiplication between two matrices. To obtain pointwise multiplication in MATLAB, we use the \u201cpoint-star\u201d operator A.* B. For example, if and are both, then where. We will refer to this type of multiplication as array multiplication. Notice that array multiplication is commutative because we would get the same result if we computed D = B.*A.",
        "74": "A general rule in MATLAB is that when \u201cpoint\u201d is used with another arithmetic operator, it modifies that operators usual matrix definition to a pointwise one. Thus we have./ for pointwise division and.^ for pointwise exponentiation. For example, xx = (0.9). (0:49) generates a vector whose values are equal to, for.",
        "75": "Plots and Graphics MATLAB is capable of producing two-dimensional - plots and three-dimensional plots, displaying images, and even creating and playing movies. The two most common plotting functions that will be used in the SP First laboratory exercises are plot and stem. The calling syntax for both plot and stem takes two vectors, one for the -axis points, and the other for the -axis. The statement plot(x,y) produces a connected plot with straight lines between the data points",
        "76": "(x(1),y(1)),(x(2),y(2)),...,(x(N),y(N))",
        "77": "as shown in the top panel of Fig.~\u203b. The same call with stem(x,y) produces the \u201clollipop\u201d presentation of the same data in the bottom panel of Fig.~\u203b. MATLAB has numerous plotting options that can be studied by using help plotxy, help plotxyz,",
        "78": "or by using help graph2d, help graph3d, or help specgraph.",
        "79": " Example of two different plotting formats, plot and stem. Figure Windows Whenever MATLAB makes a plot, it writes the graphics to a figure window. You can have multiple figure windows open, but only one of the them is considered the active window. Any plot command executed in the command window will direct its graphical output to the active window. The command figure(n) will pop up a new figure window that can be refered to by the number n, or makes it active if it already exists. Control over many of the window attributes (size, location, color, etc.) is also possible with the figure command, which does initialization of the plot window. Multiple Plots Multiple plots per window can be done with the subplot function. This function does not do the actual plotting; it merely divides the window into tiles. To set up a tiling of the figure window, use subplot(3,2,tile_number). For example, subplot(3,2,3) will direct the next plot to the third tile, which is in the second row, left side. The graphs in Fig.~\u203b were done with subplot(2,1,1) and subplot(2,1,2). The program that creates Fig.~\u203b is given in Section~\u203b. Printing and Saving Graphics Plots and graphics may be printed to a printer or saved to a file using the print command. To send the current figure window to the default printer, simply type print without arguments. To save the plot to a file, a device format and filename must be specified. The device format specifies which language will be used to store the graphics commands. For example, a useful format for including the file in a document is encapsulated PostScript (EPS), which can be produced as follows: >> print -deps myplot The postscript format is also convenient when the plots are kept for printing at a later time. For a complete list of available file formats, supported printers, and other options, see help print.",
        "80": "Programming Constructs MATLAB supports the paradigm of \u201cfunctional programming\u201d in which it is possible to nest a sequence of function calls. Consider the following equation, which can be implemented with one line of MATLAB code. Here is the MATLAB equivalent: where x is a vector containing the elements. This example illustrates MATLAB in its most efficient form, where individual functions are combined to get the output. Writing efficient MATLAB code requires a programming style that generates small functions that are vectorized. Loops should be avoided. The primary way to avoid loops is to use calls to toolbox functions as much as possible.",
        "81": "MATLAB Built-in Functions Many MATLAB functions operate on arrays just as easily as they operate on scalars. For example, if x is an array, then cos(x) returns an array of the same size as x containing the cosine of each element of x. Notice that no loop is needed, even though cos(x) does apply the cosine function to every array element. Most transcendental functions follow this pointwise rule. In some cases, it is crucial to make this distinction, such as the matrix exponential (expm) versus the pointwise exponential (exp): ",
        "82": "Program Flow Program flow can be controlled in MATLAB using if statements, while loops, and for loops. There is also a switch statement. These are similar to any high-level language. Descriptions and examples for each of these program constructs can be viewed by using the MATLAB help command.",
        "83": "MATLAB Scripts Any expression that can be entered at the MATLAB prompt can also be stored in a text file and executed as a script. The text file can be created with any plain ASCII editor such as notepad on Windows, emacs or vi on UNIX or Linux, and the built-in MATLAB editor on a Macintosh or Windows platform. The file extension must be.m, and the script is executed in MATLAB simply by typing the filename (without the extension). These programs are usually called M-files. Here is an example:",
        "84": " tt = 0:0.3:4; xx = sin(0.7*pi*tt); subplot(2,1,1) plot( tt, xx ) title(Example of plot( tt, xx)) subplot(2,1,2) stem( tt, xx ) title(Example of stem( tt, xx)) ",
        "85": "If these commands are saved in a file named plotstem.m, then typing plotstem at the command prompt will run the file, and all eight commands will be executed as if they had been typed in at the command prompt. The result is the two plots that were shown in Fig.~\u203b.",
        "86": "Writing a MATLAB Function You can write your own functions and add them to the MATLAB environment. These functions are another type of M-file, and are created as an ASCII file with a text editor. The first word in the M-file must be the keyword function to tell MATLAB that this file is to be treated as a function with arguments. On the same line as the word function is the calling template that specifies the input and output arguments of the function. The filename for the M-file must end in.m, and the filename will become the name of the new command for MATLAB. For example, consider the following file, which extracts the last elements from a vector: function y = foo( x, L ) %FOO get last L points of x % usage: % y = foo( x, L ) % where: % x = input vector % L = number of points to get % y = output vector N = length(x); if( L > N ) error(input vector too short) end y = x((N-L+1):N); If this file is called foo.m, the operation may be invoked from the MATLAB command line by typing",
        "87": "aa = foo( (1:2:37), 7 );",
        "88": "The output will be the last seven elements of the vector (1:2:37), i.e.,",
        "89": "aa = [ 25 27 29 31 33 35 37 ]",
        "90": "Creating A Clip Function Most functions can be written according to a standard format. Consider a clip function M-file that takes two input arguments (a signal vector and a scalar threshold) and returns an output signal vector. You can use an editor to create an ASCII file clip.m that contains the statements in Fig.~\u203b.",
        "91": " Illustration of a MATLAB function called clip.m. We can break down the M-file clip.m into four elements: Definition of Input\u2013Output: Function M-files must have the word function as the very first item in the file. The information that follows function on the same line is a declaration of how the function is to be called and what arguments are to be passed. The name of the function should match the name of the M-file; if there is a conflict, it is the name of the M-file on the disk that is known to the MATLAB command environment.",
        "92": "Input arguments are listed inside the parentheses following the function name. Each input is a matrix. The output argument (also a matrix) is on the left side of the equals sign. Multiple output arguments are also possible if square brackets surround the list of output arguments; e.g., notice how the size(x) function in line 11 returns the number of rows and number of columns into separate output variables. Finally, observe that there is no explicit command for returning the outputs; instead, MATLAB returns whatever value is contained in the output matrix when the function completes. For clip the last line of the function assigns the clipped vector to y, so that the clipped vector is returned. MATLAB does have a command called return, but it just exits the function, it does not take an argument.",
        "93": "The essential difference between the function M-file and the script M-file is dummy variables versus permanent variables. MATLAB uses call by value so that the function makes local copies of its arguments. These local variables disappear after the function completes. For example, the following statement creates a clipped vector wwclipped from the input vector ww.",
        "94": " >> wwclipped = clip(ww, 0.9999); ",
        "95": "The arrays ww and wwclipped are permanent variables in the MATLAB workspace. The temporary arrays created inside clip (i.e., y, nrows, ncols, Lx and i) exist only while clip runs; then they are deleted. Furthermore, these variable names are local to clip.m, so the name x may also be used in the workspace as a permanent name. These ideas should be familiar to anyone experienced with a high-level computer language like C, Java, FORTRAN, or PASCAL.",
        "96": " Self-Documentation: A line beginning with the % sign is a comment line. The first group of these in a function is used by MATLABs help facility to make M-files automatically self-documenting. That is, you can now type help clip and the comment lines from your M-file will appear on the screen as help information. The format suggested in clip.m (lines 2\u20139) follows the convention of giving the function name, its calling sequence, a brief explanation, and then the definitions of the input and output arguments. Size and Error Checking: The function should determine the size of each vector or matrix that it will operate on. This information does not have to be passed as a separate input argument, but can be extracted with the size function (line 11). In the case of the clip function, we want to restrict the function to operating on vectors, but we would like to permit either a row or a column. Therefore, one of the variables nrows or ncols must be equal to 1; if not, we terminate the function with the bail-out function error, which prints a message to the command line and quits the function (lines 13\u201315). Actual Function Operations: In the case of the clip function, the actual clipping is done by a for loop (lines 18\u201322), which examines each element of the x vector for its size compared to the threshold Limit. In the case of negative numbers, the clipped value must be set to -Limit, hence the multiplication by sign(x(n)). This assumes that Limit is passed in as a positive number, a fact that might also be tested in the error-checking phase.",
        "97": "This particular implementation of clip is very inefficient owing to the for loop. In Section~\u203b, we will show how to vectorize this program for speed. Debugging a MATLAB M-file Since MATLAB is an interactive environment, debugging can be done by examining variables in the workspace. MATLAB contains a symbolic debugger that is now integrated with the text editor. Since different functions can use the same variable names, it is important to keep track of the local context when setting break points and examining variables. Several useful debugging commands are listed here, others can be found with help debug. dbstop is used to set a breakpoint in an M-file. It can also be used to give you a prompt when an error occurs by typing dbstop if error before executing the M-file. This allows you to examine variables within functions and also the calling workspace (by typing dbup). dbstep incrementally steps through your M-file, returning you to a prompt after each line is executed. dbcont causes normal program execution to resume, or, if there was an error, returns you to the MATLAB command prompt. dbquit quits the debug mode and returns you to the MATLAB command prompt. keyboard can be inserted into the M-file to cause program execution to pause, giving you a MATLAB prompt of the form K> to indicate that it is not the command-line prompt.",
        "98": "Programming Tips This section presents a few programming tips that should improve the speed of your MATLAB programs. For more ideas and tips, list some of the function M-files in the toolboxes of MATLAB by using the type command. For example, type angle type conv type trapz Copying the style of other (good) programmers is always an efficient way to improve your own knowledge of a computer language. In the following hints, we discuss some of the most important points involved in writing good MATLAB code. These comments will become increasingly useful as you develop more experience in MATLAB. Avoiding Loops Since MATLAB is an interpreted language, certain common programming habits are intrinsically inefficient. The primary one is the use of for loops to perform simple operations over an entire matrix or vector. Whenever possible, you should try to find a vector function (or the nested composition of a few vector functions) that will accomplish the desired result, rather than writing a loop. For example, if the operation is summing all the elements in a matrix, the difference between calling sum and writing a loop that looks like FORTRAN or C code can be astounding; the loop is unbelievably slow owing to the interpreted nature of MATLAB. Consider the three methods for matrix summation in Fig.~\u203b.",
        "99": " Three ways to add all the elements of a matrix. The first method in Fig.~\u203b(a) is the MATLAB equivalent of conventional programming. The last two methods rely on the built-in function sum, which has different characteristics depending on whether its argument is a matrix or a vector (called \u201coperator overloading\u201d). When acting on a matrix as in Fig.~\u203b(b), sum returns a row vector containing the column sums; when acting on a row (or column) vector as in Fig.~\u203b(c), the sum is a scalar. To get the third (and most efficient) method, the matrix x is converted to a column vector with the colon operator. Then one call to sum will suffice. Repeating Rows or Columns Often it is necessary to form a matrix from a vector by replicating the vector in the rows or columns of the matrix. If the matrix is to have all the same values, then functions such as ones(M,N) and zeros(M,N) can be used. But to replicate a column vector x to create a matrix that has identical columns, a loop can be avoided by using the outer-product matrix multiply operation discussed in Section~\u203b. The following MATLAB code fragment will do the job for eleven columns:",
        "100": " x = (12:-2:0); % prime indicates conjugate transpose X = x * ones(1,11) ",
        "101": "If x is a length column vector, then the matrix X formed by the outer product is. In this example,. Note that MATLAB is case-sensitive, so the variables x and X are different. We have used capital X to indicate a matrix, as would be done in mathematics. Vectorizing Logical Operations It is also possible to vectorize programs that contain if, else conditionals. The clip function (Fig.~\u203b) offers an excellent opportunity to demonstrate this type of vectorization. The for loop in that function contains a logical test and might not seem like a candidate for vector operations. However, the relational and logical operators in MATLAB, such as greater than, apply to matrices. For example, a greater than test applied to a matrix returns a matrix of ones and zeros. >> %\u2013 create a test matrix >> x = [ 1 2 -3; 3 -2 1; 4 0 -1] x = [ 1 2 -3 3 -2 1 4 0 -1 ] >> %\u2013 check the greater than condition >> mx = x > 0 mx = [ 1 1 0 1 0 1 1 0 0 ] >> %\u2013 pointwise multiply by mask, mx >> y = mx.* x y = [ 1 2 0 3 0 1 4 0 0 ] The zeros mark where the condition was false; the ones denote where the condition was true. Thus, when we do the pointwise multiply of x by the masking matrix mx, we get a result that has all negative elements set to zero. Note that these last two statements process the entire matrix without ever using a for loop.",
        "102": "Since the saturation done in clip.m requires that we change the large values in x, we can implement the entire for loop with three array multiplications. This leads to a vectorized saturation operator that works for matrices as well as vectors:",
        "103": " y = Limit*(x > Limit)... - Limit*(x < -Limit)... + x.*(abs(x) <= Limit); Three different masking matrices are needed to represent the three cases of positive saturation, negative saturation, and no action. The additions correspond to the logical OR of these cases. The number of arithmetic operations needed to carry out this statement is multiplications and additions, where is the total number of elements in x. This is actually more work than the loop in clip.m if we counted only arithmetic operations. However, the cost of code interpretation is high. This vectorized statement is interpreted only once, whereas the three statements inside the for loop must be reinterpreted times. If the two implementations are timed with etime, the vectorized version will be much faster for long vectors. Creating an Impulse Another simple example of use of logical operators is given by the following trick for creating an impulse signal vector:",
        "104": " nn = [-10:25]; impulse = (nn==0); This result may be plotted with stem(nn, impulse). In a sense, this code fragment is perfect because it captures the essence of the mathematical formula that defines the impulse as existing only when. The Find Function An alternative to masking is to use the find function. This is not necessarily more efficient; it just gives a different approach. The find function will determine the list of indices in a vector where a condition is true. For example, find( x > Limit ) will return the list of indices where the vector is greater than the Limit value. Thus we can do saturation as follows:",
        "105": " y = x; jkl = find(y > Limit); y( jkl ) = Limit; jkl = find(y < -Limit); y( jkl ) = -Limit; Seek to Vectorize The dictum to \u201cavoid for loops\u201d is not always easy to obey, because it means the algorithm must be cast in a vector form. If matrix-vector notation is incorporated into MATLAB programs, the resulting code will run much faster. Even loops with logical tests can be vectorized if masks are created for all possible conditions. Thus, a reasonable goal is the following:",
        "106": " Eliminate all for loops. Programming Style If there were a proverb to summarize good programming style, it would probably read something like",
        "107": " May your functions be short and your variable names long.\u2014 Anon This is certainly true for MATLAB. Each function should have a single purpose. This will lead to short, simple modules that can be linked together by functional composition to produce more complex operations. Avoid the temptation to build super functions with many options and a plethora of outputs.",
        "108": "MATLAB supports long variable names, so you can take advantage of this feature to give variables descriptive names. In this way, the number of comments littering the code can be drastically reduced. Comments should be limited to help information and the documentation of tricks used in the code.",
        "109": "CHAPTER 1 Introduction This is a book about signals and systems. In this age of multimedia computers, audio and video entertainment systems, and digital communication systems, it is almost certain that you, the reader of this text, have formed some impression of the meaning of the terms signal and system, and you probably use the terms often in daily conversation.",
        "110": "It is likely that your usage and understanding of the terms are correct within some rather broad definitions. For example, you may think of a signal as \u201csomething\u201d that carries information. Usually, that something is a pattern of variations of a physical quantity that can be manipulated, stored, or transmitted by physical processes. Examples include speech signals, audio signals, video or image signals, biomedical signals, radar signals, and seismic signals, to name just a few. An important point is that signals can take many equivalent forms or representations. For example, a speech signal is produced as an acoustic signal, but it can be converted to an electrical signal by a microphone, or a pattern of magnetization on a magnetic tape, or even a string of numbers as in digital audio recording.",
        "111": "The term system may be somewhat more ambiguous and subject to interpretation. For example, we often use \u201csystem\u201d to refer to a large organization that administers or implements some process, such as the \u201cSocial Security system\u201d or the \u201cairline transportation system.\u201d However, we will be interested in a much narrower definition that is very closely linked to signals. More specifically, a system, for our purposes, is something that can manipulate, change, record, or transmit signals. For example, an audio compact disk (CD) recording stores or represents a music signal as sequence of numbers. A CD player is a system for converting the numbers stored on the disk (i.e., the numerical representation of the signal) to an acoustic signal that we can hear. In general, systems operate on signals to produce new signals or new signal representations.",
        "112": "Our goal in this text is to develop a framework wherein it is possible to make precise statements about both signals and systems. Specifically, we want to show that mathematics is an appropriate language for describing and understanding signals and systems. We also want to show that the representation of signals and systems by mathematical equations allows us to understand how signals and systems interact and how we can design and implement systems that achieve a prescribed purpose.",
        "113": "Mathematical Representation of Signals Signals are patterns of variations that represent or encode information. They have a central role in measurement, in probing other physical systems, in medical technology, and in telecommunication, to name just a few areas.",
        "114": "Many signals are naturally thought of as a pattern of variations in time. A good example is a speech signal, which initially arises as a pattern of changing air pressure in the vocal tract. This pattern, of course, evolves with time, creating what we often call a time waveform. Figure~\u203b",
        "115": " Plot of part of a speech signal. This signal can be represented as a function of a single (time) variable,. The shaded region is shown in more detail in Fig.~\u203b. shows a plot of a speech waveform. In this plot, the vertical axis represents air pressure, or microphone voltage, and the horizontal axis represents time. Notice that there are four plots in the figure corresponding to four contiguous time segments of the speech waveform. The second plot is a continuation of the first, and so on, with each graph corresponding to a time interval of 50 milliseconds (msec). ",
        "116": "The speech signal in Fig.~\u203b is an example of a one-dimensional continuous-time signal. Such signals can be represented mathematically as a function of a single independent variable, which is normally denoted. Although in this particular case we cannot write a simple equation that describes the graph of Fig.~\u203b in terms of familiar mathematical functions, we can nevertheless associate a function with the graph. Indeed, the graph itself can be taken as a definition of the function that assigns a number to each instant of time (each value of ).",
        "117": "Many, if not most, signals originate as continuous-time signals. However, for a variety of reasons that will become increasingly obvious as we progress through this text, it is often desirable to obtain a discrete-time representation of a signal. This can be done by sampling a continuous-time signal at isolated, equally spaced points in time. The result is a sequence of numbers that can be represented as a function of an index variable that takes on only discrete values. This can be represented mathematically as, where is an integer; i.e.,, and is the sampling period. This is, of course, exactly what we do when we plot values of a function on graph paper or on a computer screen. We cannot evaluate the function at every possible value of a continuous variable, only at a set of discrete points. Intuitively, we know that the closer the spacing of the points, the more the sequence retains the shape of the original continuous-variable function. Figure ~\u203b shows an example of a short segment of a discrete-time signal that was derived by",
        "118": " Example of a discrete-time signal that can be represented by a one-dimensional sequence or function of a discrete variable. Signal samples are taken from the shaded region of Fig.~\u203b. sampling the speech waveform of Fig.~\u203b with a sampling period of msec. In this case, a vertical line with a dot at the end shows the location and the size of each of the isolated sequence values.",
        "119": "While many signals can be thought of as evolving patterns in time, many other signals are not time-varying patterns. For example, an image formed by focusing light through a lens is a spatial pattern, and thus is appropriately represented mathematically as a function of two spatial variables. Such a signal would be considered, in general, as a function of two independent variables; i.e., a picture might be denoted. A photograph is another example, such as the \u201cgray-scale image\u201d shown in Fig.~\u203b. In this case, the value represents the shade of gray at position in the image.",
        "120": " Example of a signal that can be represented by a function of two spatial variables. Images such as that in Fig.~\u203b are generally considered to be two-dimensional continuous-variable signals, since we normally consider space to be a continuum. However, sampling can likewise be used to obtain a discrete-variable two-dimensional signal from a continuous-variable two-dimensional signal. Such a two-dimensional discrete-variable signal would be represented by a two-dimensional sequence or an array of numbers, and would be denoted, where both and would take on only integer values, and and are the horizontal and vertical sampling periods, respectively.",
        "121": "Two-dimensional functions are appropriate mathematical representations of still images that do not change with time; on the other hand, videos are time-varying images that would require a third independent variable for time; i.e.,. Video signals are intrinsically three-dimensional, and, depending on the type of video system, either two or all three variables may be discrete.",
        "122": "Our purpose in this section has been simply to introduce the idea that signals can be represented by mathematical functions. Although we will soon see that many familiar functions are quite valuable in the study of signals and systems, we have not even attempted to demonstrate that fact. Our sole concern is to make the connection between functions and signals, and, at this point, functions simply serve as abstract symbols for signals. Thus, for example, now we can refer to \u201cthe speech signal \u201d or \u201cthe sampled image.\u201d Although this may not seem highly significant, we will see in the next section that it is indeed a very important step toward our goal of using mathematics to describe signals and systems in a systematic way.",
        "123": "Mathematical Representation of Systems As we have already suggested, a system is something that transforms signals into new signals or different signal representations. This is a rather abstract definition, but it is useful as a starting point. To be more specific, we say that a one-dimensional continuous-time system takes an input signal and produces a corresponding output signal. This can be represented mathematically by which means that the input signal (waveform, image, etc.) is operated on by the system (symbolized by the operator ) to produce the output. While this sounds very abstract at first, a simple example will show that this need not be mysterious. Consider a system such that the output signal is the square of the input signal. The mathematical description of this system is simply which says that at each time instant the value of the output is equal to the square of the input signal value at that same time. Such a system would logically be termed a \u201csquarer system.\u201d Figure \u203b shows the output of the squarer for the input of Fig.~\u203b. As would be expected from the properties of the squaring operation, we see that the output signal is always nonnegative and the large signal values are emphasized relative to the small signal values.",
        "124": "The squarer system defined by is an example of a continuous-time system, i.e., a system whose input and output are continuous-time signals. Can we build a physical system that acts like the squarer system? The answer is that the system of can be approximated through appropriate connections of electronic circuits. On the other hand, if the input and output of the system are both discrete-time signals (sequences of numbers) related by then the system would be a discrete-time system. The implementation of the discrete-time squarer system would be trivial; one simply multiplies each discrete signal value by itself.",
        "125": " Output of a squarer system for the speech signal input of Fig.~\u203b. The squarer system is defined by the equation In thinking and writing about systems, it is often useful to have a visual representation of the system. For this purpose, engineers use block diagrams to represent operations performed in an implementation of a system and to show the interrelations among the many signals that may exist in an implementation of a complex system. An example of the general form of a block diagram is shown in Fig.~\u203b. What this diagram shows is simply that the signal is obtained from the signal by the operation.",
        "126": " Block diagram representation of a continuous-time system. Another example of a system was suggested earlier when we discussed the sampling relationship between continuous-time signals and discrete-time signals. Therefore, we would define a sampler as a system whose input is a continuous-time signal and whose output is the corresponding sequence of samples, defined by the equation which simply states that the sampler \u201ctakes an instantaneous snapshot\u201d of the continuous-time input signal once every seconds. Thus, the operation of sampling fits our definition of a system, and it can be represented by the block diagram in Fig.~\u203b. Often we will refer to the sampler system as an \u201cideal continuous-to-discrete converter\u201d or ideal C-to-D converter. In this case, as in the case of the squarer, the name that we give to the system is really just a description of what the system does. ",
        "127": " Block diagram representation of a sampler. ",
        "128": "Thinking About Systems Block diagrams are useful for representing complex systems in terms of simpler systems, which are more easily understood. For example, Fig.~\u203b shows a block diagram representation of the process of recording and playback of an audio CD. This block diagram breaks the operation down into four subsystems. The first operation is A-to-D (analog-to-digital) conversion, which is a physical approximation to the ideal C-to-D converter defined in. An A-to-D converter produces finite-precision numbers as samples of the input signal (quantized to a limited number of bits), while the ideal C-to-D converter produces samples with infinite precision. For the high-accuracy A-to-D converters used in precision audio systems, the difference between an A-to-D converter and our idealized C-to-D converter is slight, but the distinction is very important. Only finite-precision quantized sample values can be stored in digital memory of finite size!",
        "129": " Simplified block diagram for recording and playback of an audio CD. Figure \u203b shows that the output of the A-to-D converter is the input to a system that writes the numbers onto the optical disc. This is a complex process, but for our purposes it is sufficient to simply show it as a single operation. Likewise, the complex mechanical/optical system for reading the numbers off the optical disk is shown as a single operation. Finally, the conversion of the signal from discrete-time form to continuous-time (acoustic) form is shown as a system called a D-to-A (digital-to-analog) converter. This system takes finite precision binary numbers in sequence and fills in a continuous-time function between the samples. The resulting continuous-time signal could then be fed to other systems, such as amplifiers, loudspeakers, and headphones, for conversion to sound.",
        "130": " Systems like the CD audio system are all around us. Most of the time we do not need to think about how such systems work, but this example illustrates the value of thinking about a complex system in a hierarchical form. In this way, we can first understand the individual parts, then the relationship among the parts, and finally the whole system. By looking at the CD audio system in this manner, we see that a very important issue is the conversion from continuous-time to discrete-time and back to continuous-time, and we see that it is possible to consider these operations separately from the other parts of the system. The effect of connecting the parts is then relatively easy to understand. Details of some parts can be left to experts in other fields who, for example, can develop more detailed breakdowns of the optical disk subsystems.",
        "131": "The Next Step The CD audio system is a good example of a discrete-time system. Buried inside the blocks of Fig.~\u203b are many discrete-time subsystems and signals. While we do not promise to explain all the details of CD players or any other complex system, we do hope to establish the foundations for the understanding of discrete- and continuous-time signals and systems so that this knowledge can be applied to understanding components of more complicated systems. In Chapter~\u203b, we will start at a basic mathematical level and show how the well-known sine and cosine functions from trigonometry play a fundamental role in signal and system theory. Next, we will show how complex numbers can simplify the algebra of trigonometric functions. Subsequent chapters will introduce the concept of the frequency spectrum of a signal and the concept of filtering with a linear time-invariant system. By the end of the book, the diligent reader who has worked the problems, experienced the demonstrations, and done the laboratory exercises will be rewarded with a solid understanding of many of the key concepts underlying the digital multimedia information systems that are rapidly becoming commonplace.",
        "132": "CHAPTER 2 Sinusoids We begin our discussion by introducing a general class of signals that are commonly called cosine signals or, equivalently, sine signals. Collectively, such signals are called sinusoidal signals or, more concisely, sinusoids. Although sinusoidal signals have simple mathematical representations, they are the most basic signals in the theory of signals and systems, and it is important to become familiar with their properties. The most general mathematical formula for a cosine signal is where denotes the cosine function that is familiar from the study of trigonometry. When defining a continuous-time signal, we typically use a function whose independent variable is, a continuous variable that represents time. From it follows that is a mathematical function in which the angle of the cosine function is, in turn, a function of the variable. The parameters,, and are fixed numbers for a particular cosine signal. Specifically, { } is called the amplitude, } the radian frequency, and } the phase shift of the cosine signal. ",
        "133": "Figure~\u203b shows a plot of the continuous-time cosine signal ",
        "134": " Sinusoidal signal generated from the formula:. ",
        "135": "i.e.,,, and in. Note that oscillates between and and that it repeats the same pattern of oscillations every sec (approximately). This time interval is called the period of the sinusoid. We will show later in this chapter that most features of the sinusoidal waveform are directly dependent on the choice of the parameters,, and.",
        "136": "Tuning Fork Experiment One of the reasons that cosine waves are so important is that many physical systems generate signals that can be modeled (i.e., represented mathematically) as sine or cosine functions versus time. Among the most prominent of these are signals that are audible to humans. The tones or notes produced by musical instruments are perceived as different pitches. Although it is an oversimplification to equate notes to sinusoids and pitch to frequency, the mathematics of sinusoids is an essential first step to understanding complicated sound signals.",
        "137": "To provide some motivation for our study of sinusoids, we will begin by considering a very simple and familiar system for generating a sinusoidal signal. This system is a tuning fork, an example of which is shown in Fig.~\u203b. When struck sharply, the tines of the tuning fork vibrate and emit a \u201cpure\u201d tone. This tone is at a single frequency, which is usually stamped on the tuning fork. It is common to find \u201cA\u2013440\u201d tuning forks, because 440 hertz (Hz) is the frequency of A above middle C on a musical scale, and is often used as the reference note for tuning a piano and other musical instruments. If you can obtain a tuning fork, perform the following experiment:",
        "138": " Strike the tuning fork against your knee, and then hold it close to your ear. You should hear a distinct \u201chum\u201d at the frequency designated for the tuning fork. The sound will persist for a rather long time if you have struck the tuning fork properly; however, it is easy to do this experiment incorrectly. If you hit the tuning fork sharply on a hard surface such as a table, you will hear a high pitched metallic \u201cting\u201d sound. This is not the characteristic sound that you are seeking. If you hold the tuning fork close to your ear, you will hear two tones: The higher-frequency \u201cting\u201d will die away rapidly, and then the lower-frequency \u201chum\u201d will be heard. Demo: Tuning Fork With a microphone and a computer equipped with an A-to-D converter, we can make a digital recording of the signal produced by the tuning fork. The microphone",
        "139": " Picture of a tuning fork and a microphone. converts the sound into an electrical signal, which in turn is converted to a sequence of numbers stored in the computer. Then these numbers can be plotted on the computer screen. A typical plot is shown in Fig.~\u203b for an A\u2013440 tuning fork. In this case, the A-to-D converter sampled the output of the microphone at a rate of 5563.6 samples/sec. The upper plot was constructed by connecting the sample values by straight lines. It appears that the signal generated by the tuning fork is very much like the cosine signal of Fig.~\u203b. It oscillates between symmetric limits of amplitude and it also repeats periodically with a period of about 2.27 msec (0.00227 sec). As we will see in Section~\u203b, this period is proportional to the reciprocal of ; i.e.,.",
        "140": "This experiment shows that common physical systems produce signals whose graphical representations look very much like cosine signals; i.e., they look very much like the graphical plots of the mathematical functions defined in. Later, in Section~\u203b, we will add further credence to the sinusoidal model for the tuning fork sound by showing that cosine functions arise as solutions to the differential equation that (through the laws of physics) describes the motion of the tuning forks tines. Before looking at the physics of the tuning fork, however, we should become more familiar with sinusoids and sinusoidal signals.",
        "141": " Recording of an A\u2013440 tuning fork signal sampled at a sampling rate of 5563.6 samples/sec. The bottom plot, which consists of the first 3.6 msec taken from the top plot, shows the individual sample values (as circles). ",
        "142": "Review of Sine and Cosine Functions Sinusoidal signals are defined in terms of the familiar sine and cosine functions of trigonometry. A brief review of the properties of these basic trigonometric functions is useful, since these properties determine the properties of sinusoidal signals.",
        "143": "The sine and cosine functions are often introduced and defined through a diagram like Fig.~\u203b. The trigonometric functions sine and cosine take an angle as their argument. We often think of angles in degrees, but where sine and cosine functions are concerned, angles must be dimensionless. Angles are therefore specified in radians. If the angle is in the first quadrant ( rad), then the sine of is the length of the side of the triangle opposite the angle divided by the length of the hypotenuse of the right triangle. Similarly, the cosine of is the ratio of the length of the adjacent side to the length of the hypotenuse.",
        "144": " Basic properties of the sine and cosine functions. Property Equation Equivalence Periodicity, when is an integer Evenness of cosine Oddness of sine Zeros of sine, when is an integer Ones of cosine when is an integer Minus ones of cosine, when is an integer ",
        "145": " Definition of sine and cosine of an angle within a right triangle. ",
        "146": "Note that as increases from 0 to, decreases from 1 to 0 and increases from 0 to 1. When the angle is greater than radians, the algebraic signs of and come into play, being negative in the second and third quadrants and being negative in the third and fourth quadrants. This is most easily shown by plotting the values of and as a function of, as in Fig.~\u203b. Several features of these plots are worthy of comment. The two functions have exactly the same shape. Indeed, the sine function is just a cosine function that is shifted to the right by ; i.e.,. Both functions oscillate between and, and they repeat the same pattern periodically with period. Furthermore, the sine function is an odd function of its argument, and the cosine is an even function. A summary of these and other properties is presented in Table~\u203b.",
        "147": " Sine and cosine functions plotted versus angle. Both functions have a period of. ",
        "148": "Clearly, the sine and cosine functions are very closely related. This often leads to opportunities for simplification of expressions involving both sine and cosine functions. In calculus, we have the interesting property that the sine and cosine functions are derivatives of each other: That is, the cosine function gives the slope of the sine function, and the sine function is the negative of the slope of the cosine function. In trigonometry, there are many identities that can be used in simplifying expressions involving combinations of sinusoidal signals. Table~\u203b gives a brief table of trigonometric identities that will be useful. Recall from your study of trigonometry that these identities are not independent; e.g., identity 3 can be obtained from identity 4 by substituting.",
        "149": " Some basic trigonometric identities. NumberEquation 1. 2. 3. 4. 5. Also, these identities can be combined to derive other identities. For example, combining identity 1 with identity 2 leads to the identities A more extensive table of trigonometric identities may be found in any book on trigonometry or in a book of mathematical tables.",
        "150": "Use trigonometric identity #5 to derive an expression for in terms of,, and.",
        "151": "Sinusoidal Signals The most general mathematical formula for a sinusoidal time signal is obtained by making the argument (i.e., the angle) of the cosine function be a function of. The following equation gives two equivalent forms: The two forms are related by defining. In either form given in, there are three independent parameters. The names and interpretations of these parameters are as follows:",
        "152": " is called the amplitude. The amplitude is a scaling factor that determines how large the cosine signal will be. Since the function oscillates between and, the signal in oscillates between and.",
        "153": " is called the phase shift. The units of phase shift must be radians, since the argument of the cosine must be in radians. We will generally prefer to use the cosine function when defining the phase shift. If we happen to have a formula containing sine, e.g.,, then we can rewrite it in terms of cosine if we use the equivalence property in Table~\u203b. The result is: so we define the phase shift to be in. For simplicity and to prevent confusion, we often avoid using the sine function.",
        "154": " is called the radian frequency. Since the argument of the cosine function must be in radians, which is dimensionless, the quantity must likewise be dimensionless. Thus, must have units of rad/sec if has units of sec. Similarly, is called the cyclic frequency, and must have units of sec. EXAMPLE:\u00a0 Plotting Sinusoids Figure~\u203b shows a plot of the signal In terms of our definitions, the signal parameters are,,, and. The dependence of the signal on the amplitude parameter is obvious; its maximum and minimum values are and, respectively. The maxima occur at and the minima at The time interval between successive maxima of the signal is sec. To understand why the signal has these properties, we will need to do a bit of analysis. Demo: Sinusoids Sinusoidal signal with parameters,,, and. Relation of Frequency to Period The sinusoid in Fig.~\u203b is clearly a periodic signal. The period of the sinusoid, denoted by, is the length of one cycle of the sinusoid. In general, the frequency of the sinusoid determines its period, and the relationship can be found by examining the following equations: Since the cosine function has a period of, the equality above holds for all values of if Since is the period of the signal, is the number of periods (cycles) per second. Therefore, cycles per second is an appropriate unit for, and it was in general use until the 1960s. When dealing with, the unit of radian frequency is rad/sec. The units of are often more convenient when describing the sinusoid, because cycles per second naturally define the period.",
        "155": " Demo: Sine Drill It is very important to understand the effect of the frequency parameter.",
        "156": " Cosine signals for several values of : (a) Hz; (b) Hz; (c). ",
        "157": "Figure~\u203b shows this effect for several choices of in the signal The two plots in Fig.~\u203b(a,b) show the effect of changing. As we expect, the waveform shape is similar for both values of frequency. However, for the higher frequency, the signal varies more rapidly with time; i.e., the cycle length is a shorter time interval. We have already seen in that this is true because the period of a cosine signal is the reciprocal of the frequency. Note that when the frequency doubles ( ), the period is halved. This is an illustration of the general principle that the higher the frequency, the more rapid the signal waveform changes with time. Throughout this book we will see more examples of the inverse relationship between time and frequency.",
        "158": "Finally, notice that is a perfectly acceptable value, and when this value is used, the resulting signal is constant (Fig.~\u203b(c)), since for all values of. Thus, the constant signal, often called DC, is, in fact, a sinusoid of zero frequency. Phase-Shift and Time-Shift The phase shift parameter (together with the frequency) determines the time locations of the maxima and minima of a cosine wave. To be specific, notice that the sinusoid with has a positive peak at. When, the phase shift determines how much the maximum of the cosine signal is shifted away from.",
        "159": "Before we examine this point in detail for sinusoids, it is useful to become familiar with the general concept of time-shifting a signal. Suppose that a signal is defined by a known formula or graph. A simple example is the following triangularly shaped function: This simple function has a slope of for and a negative slope of for. Now consider the function. From the definition of, it follows that is nonzero for Within the time interval, the formula for the shifted signal is: In other words, is simply the function with its origin shifted to the right by 2 seconds. Similarly, is the function shifted to the left by 1 second; its nonzero portion is located in the interval. The three signals,, and are all shown in Fig.~\u203b.",
        "160": " Illustration of time-shifting: (a) the triangular signal ; (b) shifted to the right by 2 secs, ; (c) shifted to the left by 1 sec,. Derive the equations for the shifted signal. We will have many occasions to consider time-shifted signals. Whenever a signal can be expressed in the form, we say that is a time-shifted version of. If is a positive number, then the shift is to the right, and we say that the signal has been delayed in time. When is a negative number, then the shift is to the left, and we say that the signal was advanced in time. In summary, time shift is essentially a redefinition of the time origin of the signal. In general, any function of the form has its origin moved to the location.",
        "161": "One way to determine the time shift for a cosine signal would be to find the positive peak of the sinusoid that is closest to. In the plot of Fig.~\u203b, the time where this positive peak occurs is sec. Since the peak in this case occurs at a positive time (to the right of ), we say that the time shift is a delay of the zero-phase cosine signal. Let denote a cosine signal with zero phase shift. A delay of can be converted to a phase shift by making the following comparison: Since this equation must hold for all, we must have, which leads to Notice that the phase shift is negative when the time shift is positive (a delay). In terms of the period ( ) we get the more intuitive formula which states that the phase shift is times the fraction of a cycle given by the ratio of the time shift to the period.",
        "162": " Demo: Sine Drill Since the positive peak nearest to must always lie within, the phase shift can always be chosen to satisfy. However, the phase shift is also ambiguous because adding a multiple of to the argument of a cosine function does not change the value of the cosine. This is a direct consequence of the fact that the cosine is periodic with period. Each different multiple of corresponds to picking a different peak of the periodic waveform. Thus, another way to compute the phase shift is to find any positive peak of the sinusoid and measure its corresponding time location. After the time location is converted to phase shift using, an integer multiple of can be added to or subtracted from the phase shift to produce a final result between and. This gives a final result identical to locating the peak that is within half a period of. The operation of adding or subtracting multiples of is referred to as reducing modulo, because it is similar to modulo reduction in mathematics, which amounts to dividing by and taking the remainder. The value of phase shift that falls between and is called the principal value of the phase shift. ",
        "163": "In Fig.~\u203b, it is possible to measure both a positive and a negative value of and then calculate the corresponding phase shifts. Which phase shift is within the range ? Verify that the two phase shifts differ by. Starting with the plot in Fig.~\u203b, sketch a plot of when. Repeat for. Make sure that you shift in the correct direction. For each case, compute the phase shift of the resulting shifted sinusoid. For the signal in Fig.~\u203b,, find and so that the signal is equal to ; i.e., obtain an expression for in terms of.",
        "164": "Sampling and Plotting Sinusoids All of the plots of sinusoids in this chapter were created using MATLAB. This had to be done with care, because MATLAB deals only with discrete signals represented by row or column matrices, but we are actually plotting a continuous function. For example, if we wish to plot a function such as which is shown in Fig.~\u203b, we must evaluate at a discrete set of times,, where is an integer. If we do so, we obtain the sequence of samples where is called the sample spacing or sampling period, and is an integer. When plotting the function using the plot function in MATLAB, we must provide a pair of row or column vectors, one containing the time values and the other the computed function values to be plotted. For example, the MATLAB statements n = -7:5; Ts = 0.005; tn = n*Ts; xn = 20*cos(80*pi*tn - 0.4*pi); plot(tn,xn) would create a row vector tn of 13 numbers between and spaced by the sampling period 0.005, and a row vector xn of samples of. Then the MATLAB function plot draws the corresponding points, connecting them with straight line segments. Constructing the curve between sample points in this way is called linear interpolation. The solid gray curve in Fig.~\u203b(a) shows the result of linear interpolation when the sample spacing is. Intuitively, we realize that if the points are very close together, we will see a smooth curve. The important question is, \u201cHow small must we make the sample spacing, so that the cosine signal can be accurately reconstructed between samples by linear interpolation?\u201d A qualitative answer to this question is provided by Fig.~\u203b, which shows plots produced by three different sampling periods.",
        "165": " Sampled cosine signals for several values of : (a) sec; (b) sec; (c) sec. Obviously, the sample spacing of is not sufficiently close to create an accurate plot when the sample points are connected by straight lines. Note that sample points are shown as dots in the upper two plots. With a spacing of, the plot starts to approximate a cosine, but it is still possible to see places where it is clear that the points are connected by straight lines rather than the smooth cosine function. Only in the lower plot of Fig.~\u203b, where the spacing is, does the sampling spacing become so dense that our eye can easily see that the curve is a faithful representation of the cosine function. A precise answer to the question posed above would require a mathematical definition of accuracy; our subjective judgment would be too prone to variability among different observers. However, we learn from this example that as the sampling period decreases, more samples are taken across one cycle of the periodic cosine signal. When, there are 5 samples per cycle; when there are 10 samples per cycle; and when, there are 250 samples per cycle. It seems that 10 samples per cycle is not quite enough, and 250 samples per cycle is probably more than necessary, but, in general, the more samples per cycle, the smoother and more accurate is the linearly interpolated curve.",
        "166": "The choice of also depends on the frequency of the cosine signal, because it is the number of samples per cycle that matters in plotting. For example, if the frequency of the cosine signal were 2000 Hz instead of 40 Hz, then a sample spacing of would yield only 5 samples per cycle. The key to accurate reconstruction is to sample frequently enough so that the cosine signal does not change very much between sample points. This will depend directly on the frequency of the cosine signal.",
        "167": "The problem of plotting a cosine signal from a set of discrete samples depends on the interpolation method used. With MATLABs built-in plotting function, linear interpolation is used to connect points by straight-line segments. An insightful question would be: \u201cIf a more sophisticated interpolation method can be used, how large can the sample spacing be such that the cosine signal can be reconstructed accurately from the samples?\u201d Surprisingly, the theoretical answer to this question is that the cosine signal can be reconstructed exactly from its samples if the sample spacing is less than half the period, i.e., the average number of samples per cycle need be only slightly more than two! Linear interpolation certainly cannot achieve this result, but, in Chapter \u203b, where we examine the sampling process in more detail, we will illustrate how this remarkable result can be achieved. For now, our observation that smooth and accurate sinusoidal curves can be reconstructed from samples if the sampling period is \u201csmall enough\u201d will be adequate for our purposes.",
        "168": "Complex Exponentials and Phasors We have shown that cosine signals are useful mathematical representations for signals that arise in a practical setting, and that they are simple to define and interpret. However, it turns out that the analysis and manipulation of sinusoidal signals is often greatly simplified by dealing with related signals called complex exponential signals. Although the introduction of the unfamiliar and seemingly artificial concept of complex exponential signals may at first seem to be making the problem more difficult, we will soon see the value in this new representation. Before introducing the complex exponential signal, we will first review some basic concepts concerning complex numbers. Review of Complex Numbers A complex number is an ordered pair of real numbers. Complex numbers may be represented by the notation, where is the real part and is the imaginary part of. Electrical engineers use the symbol for instead of, so we can also represent a complex number as. These two representations are called the Cartesian form of the complex number. Complex numbers are often represented as points in a complex plane, where the real and imaginary parts are the horizontal and vertical coordinates, respectively, as shown in Fig.~\u203b(a). With the Cartesian notation and the understanding that any number multiplied by is included in the imaginary part, the operations of complex addition, complex subtraction, complex multiplication, and complex division can be defined in terms of real operations on the real and imaginary parts. For example, the sum of two complex numbers is defined as the complex number whose real part is the sum of the real parts and whose imaginary part is the sum of the imaginary parts.",
        "169": " (a)~Cartesian and (b)~polar representations of complex numbers in the complex plane. Since complex numbers can be represented as points in a plane, it follows that complex numbers are analogous to vectors in a two-dimensional space. This leads to a useful geometric interpretation of a complex number as a vector, shown in Fig.~\u203b(b). Vectors have length and direction, so another way to represent a complex number is the polar form in which the complex number is represented by, its vector length, together with, its angle with respect to the real axis. The length of the vector is also called the magnitude of (denoted ), and the angle with the real axis is called the argument of (denoted ). This is indicated by the descriptive notation, which is interpreted to mean that the vector representing has length and makes an angle with the real axis.",
        "170": "It is important to be able to convert between the Cartesian and polar forms of complex numbers. Figure~\u203b(b) shows a complex number and the quantities involved in both the Cartesian and polar representations. Using this figure, as well as some simple trigonometry and the Pythagorean theorem, we can derive a method for computing the Cartesian coordinates from the polar variables : and, likewise, for going from Cartesian to polar form Many calculators and computer programs have these two sets of equations built in, making the conversion between polar and Cartesian forms simple and convenient.",
        "171": "The notation is clumsy, and does not lend itself to ordinary algebraic rules. A much better polar form is given by using Eulers famous formula for the complex exponential The Cartesian pair can represent any point on a circle of radius 1, so a slight generalization of gives a representation valid for any complex number~ The complex exponential polar form of a complex number is most convenient when calculating a complex multiplication or division (see Appendix \u203b for more details). It also serves as the basis for the complex exponential signal, which is introduced in the next section.",
        "172": " Demo: Z-Drill Complex Exponential Signals The complex exponential signal is defined as Observe that the complex exponential signal is a complex-valued function of, where the magnitude of is and the angle of is. Using Eulers formula, the complex exponential signal can be expressed in Cartesian form as As with the real sinusoid, is the amplitude, and should be a positive real number; is the phase shift; and is the frequency in rad/sec. In it is clear that the real part of the complex exponential signal is a real cosine signal as defined in, and that its imaginary part is a real sine signal. Figure~\u203b shows a plot of the following complex exponential signal: Plotting a complex signal as a function of time requires two graphs, one for the real part and another for the imaginary part. Observe that the real and the imaginary parts of the complex exponential signal are both real sinusoidal signals, and they differ by only a phase shift of rad.",
        "173": "The main reason that we are interested in the complex exponential signal is that it is an alternative representation for the real cosine signal. This is because we can always write ",
        "174": " Real and imaginary parts of the complex exponential signal. The phase difference between the two waves is or rad. In fact, the real part of the complex exponential signal shown in Fig.~\u203b is identical to the cosine signal plotted in Fig.~\u203b. Although it may seem that we have complicated things by first introducing the imaginary part to obtain the complex exponential signal and then throwing it away by taking only the real part, we will see that many calculations are simplified by using properties of the exponents. It is possible, for example, to replace all trigonometric manipulations with algebraic operations on the exponents.",
        "175": "Demonstrate that expanding the real part of will lead to identity #5 in Table~\u203b. Also show that identity #4 is obtained from the imaginary part. The Rotating Phasor Interpretation When two complex numbers are multiplied, it is best to use the polar form for both numbers. To illustrate this, consider, where and and We have used the law of exponents to combine the two complex exponentials. From this result we conclude that to multiply two complex numbers, we multiply the magnitudes and add the angles. If we consider one of the complex numbers to be represented by a fixed vector in the complex plane, then multiplication by a second complex number scales the length of the first vector by the magnitude of the second complex number and rotates it by the angle of the second complex number. This is illustrated in Fig.~\u203b where it is assumed that so that.",
        "176": " Geometric view of complex multiplication. The angles add: This geometric view of complex multiplication leads to a useful interpretation of the complex exponential signal as a complex vector that rotates as time increases. If we define the complex number then can be expressed as i.e., is the product of the complex number and the complex-valued time function. The complex number, which is aptly called the complex amplitude, is a polar representation created from the amplitude and the phase shift of the complex exponential signal. Taken together, the complex amplitude and the frequency are sufficient to represent, as well as the real cosine signal,, using. The complex amplitude is also called a phasor. Use of this terminology is common in electrical circuit theory, where complex exponential signals are used to greatly simplify the analysis and design of circuits. Since it is a complex number, can be represented graphically as a vector in the complex plane, where the vectors magnitude ( ) is the amplitude, and vectors angle ( ) is the phase shift of a complex exponential signal defined by. In the remainder of the text, the terms phasor and complex amplitude will be used interchangeably, because they refer to the same quantity defined in.",
        "177": "The complex exponential signal defined in can also be written as where At a given instant in time,, the value of the complex exponential signal,, is a complex number whose magnitude is and whose argument is. Like any complex number, can be represented as a vector in the complex plane. In this case, the tip of the vector always lies on the perimeter of a circle of radius. Now, if increases, the complex vector will simply rotate at a constant rate, determined by the radian frequency. In other words, multiplying the phasor by as in causes the fixed phasor to rotate. (Since, no scaling occurs.) Thus, another name for the complex exponential signal is rotating phasor. ",
        "178": "If the frequency is positive, the direction of rotation is counterclockwise, because will increase with increasing time. Similarly, when is negative, the angle changes in the negative direction as time increases, so the complex phasor rotates clockwise. Thus, rotating phasors are said to have positive frequency if they rotate counterclockwise, and negative frequency if they rotate clockwise.",
        "179": "A rotating phasor makes one complete revolution every time the angle changes by radians. The time it takes to make one revolution is also equal to the period,, of the complex exponential signal, so Notice that the phase shift defines where the phasor is pointing when. For example, if, then the phasor is pointing straight up when, whereas if, the phasor is pointing to the right when.",
        "180": "The plots in Fig.~\u203b(a) illustrate the relationship between a single complex rotating phasor and the cosine signal waveform. The upper left plot shows the complex plane with two vectors. The vector at an angle in the third quadrant represents the signal at the specific time. The horizontal vector pointing to the left represents the real part of the vector at the particular time ; i.e., As increases, the rotating phasor rotates in the counterclockwise direction, and its real part oscillates left and right along the real axis. This is shown in the lower left plot, which shows how the real part of the rotating phasor has varied over one period, i.e.,.",
        "181": " Demo: Rotating Phasors Rotating phasors: (a) single phasor rotating counter-clockwise; (b) complex conjugate rotating phasors. Inverse Euler Formulas The inverse Euler formulas allow us to write the cosine function in terms of complex exponentials as and also the sine function can be expressed as (See Appendix \u203b for more details.)",
        "182": "Equation can be used to express in terms of a positive and a negative frequency complex exponential as follows: where denotes complex conjugation.",
        "183": "This formula has an interesting interpretation. The real cosine signal with frequency is actually composed of two complex exponential signals; one with positive frequency and the other with negative frequency. The complex amplitude of the positive frequency complex exponential signal is, and the complex amplitude of the negative frequency complex exponential is. In other words, the real cosine signal can be represented as the sum of two complex rotating phasors that are complex conjugates of each other.",
        "184": " Demo: Rotating Phasors Figure~\u203b(b) illustrates how the sum of the two half-amplitude complex conjugate rotating phasors becomes the real cosine signal. In this case, the vector at an angle in the third quadrant is the complex rotating phasor at time. As increases after that time, the angle would increase in the counterclockwise direction. Similarly, the vector in the second quadrant (plotted with a light-orange line) is the complex rotating phasor at time. As increases after that time, the angle of will increase in the clockwise direction. The horizontal vector pointing to the right is the sum of these two complex conjugate rotating phasors. The result is the same as the real vector in the plot on the left, and therefore the real cosine wave traced out as a function of time is the same in both cases. The lower right shows the variation of the real values of for.",
        "185": "This representation of real sinusoidal signals in terms of their positive and negative frequency components is a remarkably useful concept. The negative frequencies, which arise due to the complex exponential representation, turn out to lead to many simplifications in the analysis of signal and systems problems. We will develop this representation further in Chapter~\u203b, where we introduce the idea of the spectrum of a signal.",
        "186": "Show that the following representation can be derived for the real sine signal: where. In this case, the interpretation is that the sine signal is also composed of two complex exponentials with the same positive and negative frequencies, but the complex coefficients multiplying the terms are different from those of the cosine signal. Specifically, the sine signal requires additional phase shifts of applied to the complex amplitude and, respectively.",
        "187": "Phasor Addition There are many situations in which it is necessary to add two or more sinusoidal signals. When all signals have the same frequency, the problem simplifies. This problem arises in electrical circuit analysis, and it will arise again in Chapter~\u203b, where we introduce the concept of discrete-time filtering. Thus it is useful to develop a mechanism for adding several sinusoids having the same frequency, but with different amplitudes and phases. Our goal is to prove that the following statement is true: Equation states that a sum of cosine signals of differing amplitudes and phase shifts, but with the same frequency, can always be reduced to a single cosine signal of the same frequency. A proof of can be accomplished by using trigonometric identities such as We can expand the sum into sines and cosines, collect terms involving and those involving, and then finally use the same identity in the reverse direction. However, this is exceedingly tedious to do as a numerical computation (see Exercise \u203b), and it leads to some very messy expressions if we wish to obtain a general formula. As we will see, a much simpler approach can be based on the complex exponential representation of the cosine signals.",
        "188": "Use to show that the sum reduces to, where and The value of, given in radians, corresponds to. Addition of Complex Numbers When two complex numbers are added, it is necessary to use the Cartesian form. If and, then ; i.e., the real and imaginary parts of the sum are the sum of the real and imaginary parts, respectively. Using the vector interpretation of complex numbers, where both and are viewed as vectors with their tails at the origin, the sum is the result of vector addition, and is constructed as follows:",
        "189": " Draw a copy of with its tail at the head of. Call this displaced vector. Draw the vector from the origin to the head of. This is the sum. This process is depicted in Fig.~\u203b for the case and.",
        "190": " Graphical construction of complex number addition,, shows that the process is the same as vector addition. Phasor Addition Rule The phasor representation of cosine signals can be used to show the following result: where is any integer. That is, the sum of two or more cosine signals each having the same frequency, but having different amplitudes and phase shifts, can be expressed as a single equivalent cosine signal. The resulting amplitude and phase of the term on the right-hand side of can be computed from the individual amplitudes and phases on the left-hand side by doing the following addition of complex numbers: Equation is the essence of the phasor addition rule. Proof of the phasor rule requires the following two pieces of information:",
        "191": " Any sinusoid can be written in the form: For any set of complex numbers the sum of the real parts is equal to the real part of the sum, so we have Proof of the phasor addition rule involves the following algebraic manipulations: Two steps (shown in color) are important in this proof. In the third line, the complex exponential is factored out of the summation because all the sinusoids have the same frequency. In going from the third line to the fourth, the crucial step is replacing the summation term in parentheses with a single complex number,, as defined in. Phasor Addition Rule: Example We now return to the example of Exercise~\u203b, where and the sum was found to be The frequency of both sinusoids is 10 Hz, so the period is sec. The waveforms of the three signals are shown Fig.~\u203b(b) and the phasors used to solve the problem are shown on the left in Fig.~\u203b(a). Notice that the times where the maximum of each cosine signal occurs can be derived from the phase through the formula Adding sinusoids by doing a phasor addition, which is actually a graphical vector sum. (b)~The time of the signal maximum is marked on each plot. } which gives These times are marked with vertical dashed lines in the corresponding waveform plots in Fig.~\u203b(b). The phasor addition of the two signals is computed in four steps.",
        "192": " Represent and by the phasors: Convert both phasors to rectangular form: Add the real parts and the imaginary parts: Convert back to polar form, obtaining Therefore, the final formula for is MATLAB Demo of Phasors The process of phasor addition can be accomplished easily using MATLAB. The answer generated by MATLAB and printed with the special function zprint (provided on the CD-ROM) for this particular phasor addition is given in Table~\u203b.",
        "193": " Phasor Addition Example Z = X + jY Magnitude Phase Ph/pi Ph(deg) Z1 0.5814 1.597 1.7 1.222 0.389 70.00 Z2 -1.785 -0.6498 1.9 -2.793 -0.889 -160.00 Z3 -1.204 0.9476 1.532 2.475 0.788 141.79 Help on zprint gives",
        "194": " ZPRINT print out complex # in rect and polar form usage: zprint(z) z = vector of complex numbers Demo: Z-Drill The MATLAB code that generates Fig.~\u203b can be found on the CD and is also contained in the first lab (also on the CD). It uses the special MATLAB functions (provided on the CD-ROM) zprint, zvect, zcat, ucplot, and zcoords to make the vector plots.",
        "195": " Lab: #2 Introduction to Complex Exponentials Summary of the Phasor Addition Rule In this section, we have shown how a real cosine signal can be represented as the real part of a complex exponential signal (complex rotating phasor), and we have applied this representation to show how to simplify the process of adding several cosine signals of the same frequency. In summary, all we have to do to get the cosine signal representation of the sum is:",
        "196": " Obtain the phasor representation of each of the individual signals. Add the phasors of the individual signals to get. This requires polar-to-Cartesian-to-polar format conversions. Multiply by to get. Take the real part to get. In other words, and must be calculated by doing a vector sum of all the phasors.",
        "197": "Consider the two sinusoids, Obtain the phasor representations of these two signals, add the phasors, plot the two phasors and their sum in the complex plane, and show that the sum of the two signals is In degrees the phase should be. Examine the plots in Fig.~\u203b to see whether you can identify the cosine waves,, and. Two sinusoids and their sum,. ",
        "198": "Physics of the Tuning Fork In Section~\u203b, we described a simple experiment in which a tuning fork was seen to generate a signal whose waveform looked very much like that of a sinusoidal signal. Now that we know a lot more about sinusoidal signals, it is worthwhile to take up this issue again. Is it a coincidence that the tuning-fork signal looks like a sinusoid, or is there a deeper connection between vibrations and sinusoids? In this section, we present a simple analysis of the tuning-fork system that shows that the tuning fork does indeed vibrate sinusoidally when given a displacement from its equilibrium position. The sinusoidal motion of the tuning-fork tines is transferred to the surrounding air particles, thereby producing the acoustic signal that we hear. This simple example illustrates how mathematical models of physical systems that are derived from fundamental physical principles can lead to concise mathematical descriptions of physical phenomena and of signals that result. Equations from Laws of Physics A simplified drawing of the tuning fork is shown in Fig.~\u203b. As we have seen experimentally, when struck against a firm surface, the tines of the tuning fork vibrate and produce a \u201cpure\u201d tone. We are interested in deriving the equations that describe the physical behavior of the tuning fork so that we can understand the basic mechanism by which the sound is produced. Newtons second law,, will lead to a differential equation whose solution is a sine or cosine function, or a complex exponential.",
        "199": " (a) Tuning fork. (b) The coordinate system needed to write equations for the vibration of the tine. When the tuning fork is struck, one of the tines is deformed slightly from its rest position, as depicted in Fig.~\u203b(b). We know from experience that unless the deformation was so large as to break or bend the metal, there would be a tendency for the tine to return to its original rest position. The physical law that governs this movement is Hookes law. Although the tuning fork is made of a very stiff metal, we can think of it as an elastic material when the deformation is tiny. Hookes law states that the restoring force is directly proportional to the amount of deformation. If we set up a coordinate system as in Fig.~\u203b(b), the deformation is along the -axis, and we can write where the parameter is the elastic constant of the material (i.e., its stiffness). The minus sign indicates that this restoring force acts in the negative direction when the displacement of the tine is in the positive direction; i.e., it acts to pull the tine back toward the neutral position.",
        "200": "Now this restoring force due to stiffness produces an acceleration as dictated by Newtons second law, i.e., where is the mass of the tine, and the second derivative with respect to time of position is the acceleration of the mass along the -axis. Since these two forces must balance each other (i.e., the sum of the forces is zero), we get a second-order differential equation that describes the motion of the tine for all values of time This particular differential equation is rather easy to solve, because we can, in fact, guess the solution. From the derivative properties of sine and cosine functions, we are motivated to try as a solution the function where the parameter is a constant that must be determined. The second derivative of is Notice that the second derivative of the cosine function is the same cosine function multiplied by a constant. Therefore, when we substitute into, we get Since this equation must be satisfied for all, it follows that the coefficients of on both sides of the equation must be equal, which leads to the following algebraic equation This equation can be solved for, obtaining Therefore, we conclude that one solution of the differential equation is From our model, describes the motion of the tuning-fork tine. Therefore we conclude that the tines oscillate sinusoidally. This motion is, in turn, transferred to the particles of air in the locality of the tines thereby producing the tiny variations in air pressure that make up an acoustic wave. The formula for the frequency lets us draw two conclusions:",
        "201": " Of two tuning forks having the same mass, the stiffer one will produce a higher frequency. This is because the frequency is proportional to, which is in the numerator of. Of two tuning forks having the same stiffness, the heavier one will produce a lower frequency. This is because the frequency is inversely proportional to the square root of the mass,, which is in the denominator of. Demo: Tuning Fork General Solution to the Differential Equation There are many possible solutions to the tuning-fork differential equation. We can prove that the following function will satisfy the differential equation by substituting back into, and taking derivatives. Once again the frequency must be. Only the frequency is constrained by our simple model; the specific values of the parameters and are not important. From this we can conclude that any scaled or time-shifted sinusoid with the correct frequency will satisfy the differential equation that describes the motion of the tuning forks tine. This implies that an infinite number of different sinusoidal waveforms can be produced in the tuning fork experiment. For any particular experiment, and would be determined by the exact strength and timing of the sharp force that gave the tine its initial displacement. However, the frequency of all these sinusoids will be determined only by the mass and stiffness of the tuning-fork metal.",
        "202": "Demonstrate that a complex exponential signal can also be a solution to the tuning-fork differential equation: By substituting and into both sides of the differential equation, show that the equation is satisfied for all by both of the signals Determine the value of for which the differential equation is satisfied. Listening to Tones The observer is an important part of any physical experiment. This is particularly true when the experiment involves listening to the sound produced. In the tuning-fork experiment, we perceive a tone with a certain pitch (related to the frequency) and loudness (related to the amplitude). The human ear and neural processing system respond to the frequency and amplitude of a sustained sound like that produced by the tuning fork, but the phase is not perceptible. This is because phase is really due to an arbitrary definition of the starting time of the sinusoid; i.e., a sustained tone sounds the same now as it did 5 minutes ago. On the other hand, we could pick up the sound with a microphone and sample or display the signal on an oscilloscope. In this case, it would be possible to make precise measurements of frequency and amplitude, but phase would be measured accurately only with respect to the time base of the sampler or the oscilloscope.",
        "203": "Time Signals: More Than Formulas The purpose of this chapter has been to introduce the concept of a sinusoidal signal and to illustrate how sinusoidal signals can arise in real situations. Signals, as we have defined them, are varying patterns that convey or represent information, usually about the state or behavior of a physical system. We have seen by both theory and observation that a tuning fork generates a signal that can be represented mathematically as a sinusoidal signal. In the context of the tuning fork, the cosine wave conveys and represents information about the state of the tuning fork. Encoded in the sinusoidal waveform is information such as whether the tuning fork is vibrating or at rest, and, if it is vibrating, its frequency and the amplitude of its vibrations. This information can be extracted from the signal by human listeners, or it can be recorded for later processing by either humans or computers.",
        "204": " Demo: Clay Whistle Although the solution to the differential equation of the tuning fork is a cosine function, the resulting mathematical formula is simply a model that results from an idealization of the tuning fork. It is important to recall that the signal is a separate entity from the formula. The actual waveform produced by a tuning fork is probably not exactly sinusoidal. The signal is represented by the mathematical formula, which can be derived from an idealized model based on physical principles. This model is a good approximation of reality, but an approximation nevertheless. Even so, this model is extremely useful since it leads directly to a useful mathematical representation of the signal produced by the tuning fork.",
        "205": "In the case of a complicated signal generated by a musical instrument, the signal cannot be so easily reduced to a mathematical formula. Figure~\u203b shows a short segment of a recording of orchestra music. Just the appearance of the waveform suggests a much more complex situation. Although it oscillates like the cosine wave, it clearly is not periodic (at least, as far as we can see from the given segment). Orchestra music consists of many instruments sounding different notes together. If each instrument produced a pure sinusoidal tone at the frequency of the note that is assigned to it, then the composite orchestra signal would be simply a sum of sinusoids with different frequencies, amplitudes, and phase shifts. While this is far from being a correct model for most instruments, it is actually a highly appropriate way to think about the orchestra signal. In fact, we will see very soon that sums of sinusoids of different frequencies, amplitudes, and phase shifts can result in an infinite variety of waveforms. Indeed, it is true that almost any signal can be represented as a sum of sinusoidal signals. When this concept was first introduced by Jean-Baptiste Joseph Fourier in 1807, it was received with great skepticism by the famous mathematicians of the world. Nowadays, this notion is commonplace (although no less remarkable). The mathematical and computational techniques of Fourier analysis underlie the frequency\u2013domain analysis tools used extensively in electrical engineering and other areas of science and engineering.",
        "206": " A short segment of an orchestra music signal. ",
        "207": "Summary and Links We have introduced sinusoidal signals in this chapter. We have attempted to show that they arise naturally as a result of simple physical processes and that they can be represented by familiar mathematical functions, and also by complex exponentials. The value of the mathematical representation of a signal is twofold. First, the mathematical representation provides a convenient formula to consistently describe the signal. For example, the cosine signal is completely described in terms of just three parameters. Second, by representing both signals and systems through mathematical expressions, we can make precise statements about the interaction between signals and systems.",
        "208": " Lab: #1 Introduction to MATLAB In connection with this chapter, two laboratories are found on the CDROM. Lab #1 involves some introductory exercises on the basic elements of the MATLAB programming environment, and its use for manipulating complex numbers and plotting sinusoids. Appendix \u203b is also available for a quick overview of essential ideas about MATLAB. Labs #2a and #2b deal with sinusoids and phasor addition. In these labs, students must re-create a phasor addition demonstration similar to Fig.~\u203b. Copies of the lab write-ups are also found on the CD-ROM.",
        "209": " Lab: #2 Introduction to Complex Exponentials On the CD-ROM, one can find the following resources:",
        "210": " A tuning-fork movie that shows the experiment of striking the tuning fork and recording its sound. Several recorded sounds from different tuning forks are available, as well as sounds from clay whistles.",
        "211": " Demo: Links to many demos A drill program written in MATLAB for working with the amplitude, phase, and frequency of sinusoidal plots. A set of movies showing rotating phasor(s) and how they generate sinusoids through the real part operator. ",
        "212": "Finally, the CD-ROM also contains a wealth of solved homework problems that may be used for practice and self-study.",
        "213": " Note: Hundreds of Solved Problems ",
        "214": "CHAPTER 3 Spectrum Representation ",
        "215": "This chapter introduces the concept of the spectrum, a compact representation of the frequency content of a signal that is composed of sinusoids. In Chapter \u203b, we learned about the properties of sinusoidal waveforms of the form where } is a phasor, and we showed how phasors can simplify the addition of sinusoids of the same frequency. In this chapter, we will show how more complicated waveforms can be constructed out of sums of sinusoidal signals of different amplitudes, phases, and frequencies. As we will define it, the spectrum is simply the collection of amplitude, phase, and frequency information that allows us to express the signal in the form where is a real constant, and } is the complex amplitude (i.e., phasor) for the complex exponential of frequency. We will see that it is useful to show this information in a graphical representation. This visual form allows us to see interrelationships among the different frequency components and their relative amplitudes quickly and easily.",
        "216": "The Spectrum of a Sum of Sinusoids One of the reasons that sinusoids are so important for our study is that they are the basic building blocks for making more complicated signals. Later in this chapter, we will show some extraordinarily complicated waveforms that can be constructed from rather simple combinations of basic cosine waves. The most general and powerful method for producing new signals from sinusoids is the additive linear combination, where a signal is created by adding together a constant and sinusoids, each with a different frequency, amplitude, and phase. Mathematically, this signal may be represented by the equation where each amplitude, phase, and frequency may be chosen independently. Such a signal may also be represented in terms of the complex amplitude representations of the individual sinusoidal components, i.e., where represents a real constant component, and each complex amplitude represents the magnitude and phase of a rotating phasor whose frequency is.",
        "217": "The inverse Euler formula gives a way to represent in the alternative form As in the case of individual sinusoids, this form follows from the fact that the real part of a complex number is equal to one-half the sum of that number and its complex conjugate. Equation also shows that each sinusoid in the sum decomposes into two rotating phasors, one with positive frequency,, and the other with negative frequency,. ",
        "218": "We define the two-sided spectrum of a signal composed of sinusoids as in to be the set of complex amplitudes and the frequencies that specify the signal in the representation of. To be specific, although it is a somewhat awkward mathematical notation, our definition of the spectrum is just the set of pairs ",
        "219": "Each pair indicates the size and relative phase of the sinusoidal component contributing at frequency. It is common to refer to the spectrum as the frequency-domain representation of the signal. Instead of giving the time waveform itself (i.e., the time-domain representation), the frequency-domain representation simply gives the information required to synthesize it with ",
        "220": "EXAMPLE:\u00a0 Two-Sided Spectrum For example, consider the sum of a constant and two sinusoids: When we apply the inverse Euler formula, we get the following five terms: Note that the constant component of the signal, often called the DC component, can be expressed as a complex exponential signal with zero frequency, i.e.,. Therefore, in the list form suggested in, the spectrum of this signal is the set of five rotating phasors represented by Notation Change The relationship between and the spectrum involves special cases because the factor of multiplies every in the spectrum, except for. The companion formulas such as are cumbersome. Therefore, we introduce as a new symbol for the complex amplitude in the spectrum, and define it as follows: This allows us to say that the spectrum is the set of pairs. The primary motivation for this notational change is to simplify the formulas for the Fourier Series coefficients developed later in Section~\u203b. For example, equation, which is similar to the Fourier Series synthesis formula, can now be written as a single compact summation where we have defined. Graphical Plot of the Spectrum A plot of the spectrum is much more revealing than the list of pairs. Each frequency component can be represented by a vertical line at the appropriate frequency, and the length of the line can be drawn proportional to the magnitude,. This is shown in Fig.~\u203b for the signal in. Each spectral line is labeled with the value of to complete the information needed to define the spectrum. This simple but effective plot makes it easy to see two things: the relative location of the frequencies, and the relative amplitudes of the sinusoidal components. This is why the spectrum plot is widely used as a graphical representation of the signal. As we will see in Chapters \u203b and \u203b, the frequency-domain representation is so useful because it is often very easy to see how systems affect a signal by determining what happens to the signal spectrum as it is transmitted through the system. This is why the spectrum is the key to understanding most complex processing systems such as radios, televisions, CD players, and the like.",
        "221": " Spectrum plot for the signal. Units of frequency are Hz. Negative frequency components should be included for completeness even though they are conjugates of the corresponding positive frequency components. Notice that, for the example in Fig.~\u203b, the complex amplitude of each negative frequency component is the complex conjugate of the complex amplitude at the corresponding positive frequency component. This is a general property of the spectrum whenever is a real signal, because the complex rotating phasors with positive and negative frequency must combine to form a real signal (see Fig.~\u203b(b) and the movie found on the CD-ROM).",
        "222": " Demo: Ch.~\u203b Rotating Phasors A general procedure for computing and plotting the spectrum for an arbitrarily chosen signal requires the study of Fourier analysis. However, by assuming that the signal of interest is the sum of a constant and one or more sinusoids, we can begin to explore the virtues of the spectrum representation. For a signal where we know the sinusoidal waveforms that comprise it, the procedure is straightforward. It is necessary only to express the cosines and sines as complex exponentials (by using the inverse Euler relation) and then to plot the complex amplitude of each of the positive and negative frequency components at the corresponding frequency. In other words, if it is known a priori that a signal is composed of a finite number of sinusoidal components, the process of analyzing that signal to find its spectral components involves writing an equation for the signal in the form of, and picking off the amplitude, phase, and frequency of each of its rotating phasor components.",
        "223": "In many other cases, spectrum analysis is not so simple, but it is nevertheless possible. For example, it is possible to represent any periodic waveform (even discontinuous signals) as a sum of complex exponential signals where the frequencies are all integer multiples of a common frequency, called the fundamental frequency. Likewise, most (nonperiodic) signals can also be represented as a superposition of complex exponential signals. The mathematical tools for doing this analysis are called Fourier series (see Section~\u203b) and Fourier transforms (Chapter \u203b). Before tackling the general case, we will show examples where the sum of just a few sinusoids can be used to produce audio signals that are interesting to hear, and we will relate the sounds to their spectra.",
        "224": "Beat Notes When we multiply two sinusoids having different frequencies, we can create an interesting audio effect called a beat note. The phenomenon, which may sound like a warble, is best heard by picking one of the frequencies to be very small (e.g., 10 Hz), and the other around 1 kHz. Some musical instruments naturally produce beating tones. Another use for multiplying sinusoids is modulation for radio broadcasting. AM radio stations use this method, which is called amplitude modulation. ",
        "225": " Lab: #3, AM and Beat Notes Multiplication of Sinusoids Although the signal is produced by multiplying two sinusoids, our spectrum representation demands that the signal be expressed as an additive linear combination of complex exponential signals. Other combinations of sinusoids would also have to be rewritten in the additive form in order to display their spectrum representation. This is illustrated by the following example.",
        "226": "EXAMPLE:\u00a0 Spectrum of a Product For the special case of a beat signal formed as the product of two sinusoids at 5 Hz and Hz it is necessary to rewrite as a sum before its spectrum can be defined. The following technique for doing this relies on the inverse Euler formula From the second and third lines of this derivation, it is obvious that there are four terms in the additive combination, and the four spectrum components are at frequencies and rad/sec which convert to,,, and Hz. It is worth noting that neither of the original frequencies (5 Hz and Hz) used to define in are in the spectrum. Let. Find an additive combination in the form of for, and then plot the spectrum. Count the number of frequency components in the spectrum. What is the highest frequency contained in ? Use the inverse Euler formula rather than a trigonometric identity. Beat Note Waveform Demo: Beat Notes Beat notes are produced by adding two sinusoids with nearly identical frequencies, (e.g., by playing two neighboring piano keys). The example in and suggests that the product of two sinusoids is equivalent to a sum. Thus we can derive a general relationship between any beat signal, its spectrum, and the product form if we start with an additive combination of two closely spaced sinusoids: The two frequencies can be expressed as and, where we have defined a center frequency and a deviation frequency, which we assume is much smaller than. The spectrum of this beat signal is plotted in Fig.~\u203b.",
        "227": "Using the complex exponential representation of the two cosines, we can rewrite as a product of two cosines, and thereby have a form that is easier to plot in the time domain.The analysis proceeds as follows: EXAMPLE:\u00a0 Time-Domain Plot of a Beat Note For a numerical example, we take and Hz so that A time-domain plot of is given in Fig.~\u203b(b). Multiplicative components of a beat note with Hz and Hz. The time interval between nulls is msec, which is dictated by the frequency difference. Figure~\u203b(a) shows the two sinusoidal components, and, that make up the product in. The plot of the beat note is constructed by first drawing and its negative version to define boundaries inside of which we then draw the higher frequency signal. These boundaries are called the signals envelope. The resulting beat note is plotted in Fig.~\u203b(b), where it can be seen that the effect of multiplying the higher-frequency sinusoid (200 Hz) by the lower-frequency sinusoid (at 20 Hz) is to change the envelope of the peaks of the higher-frequency waveform. If we listen to such an we can hear that the variation causes the signal to fade in and out because the signal envelope is rising and falling, as in Fig.~\u203b(b). This is the phenomenon called \u201cbeating\u201d of tones in music.",
        "228": " Demo: Spectrograms: Simple Sounds EXAMPLE:\u00a0 Decreasing If is decreased to 9 Hz, we see in Fig.~\u203b(a,b) that the envelope of the 200 Hz tone changes much more slowly.",
        "229": " Beat note with Hz and Hz. Nulls are now msec. apart. The time interval between nulls of the envelope is, so the more closely spaced the frequencies of the sinusoids in, the slower the envelope variation. These figures are simplified somewhat by using cosines for both terms in, but other phase relationships would give similar patterns. Finally, remember that the spectrum for in Fig.~\u203b contains frequency components at Hz and Hz, while the spectrum for Fig.~\u203b has frequencies Hz and Hz. Musicians use this beating phenomenon as an aid in tuning two instruments to the same pitch. When two notes are close but not identical in frequency, the beating phenomenon is heard. As one pitch is changed to become closer to the other, the effect disappears, and the two instruments are then \u201cin tune.\u201d Amplitude Modulation Lab: #3 AM and FM Sinusoidal Signals Multiplying sinusoids is also useful in modulation for communication systems. Amplitude modulation is the process of multiplying a low-frequency signal by a high-frequency sinusoid. It is the technique used to broadcast AM radio: In fact \u201cAM\u201d is just the abbreviation for amplitude modulation. The AM signal is a product of the form where it is assumed that the frequency of the cosine term ( Hz) is much higher than any frequencies contained in the spectrum of, which represents the voice or music signal to be transmitted. The cosine wave in is called the carrier signal, and its frequency is called the carrier frequency.",
        "230": "With our basic knowledge of the spectrum at this point, the form of in must be restricted to be a sum of sinusoids, but that is sufficient to understand how the modulation process works.",
        "231": "EXAMPLE:\u00a0 Amplitude Modulation If we let and Hz, then the AM signal is a multiplication similar to the beat signal: A plot of this signal is given in Fig.~\u203b, where it can be seen that the effect of multiplying the higher-frequency sinusoid (200 Hz) by the lower-frequency sinusoid (at 20 Hz) is to \u201cmodulate\u201d (or change) the amplitude envelope of the carrier waveform\u2013-hence the name amplitude modulation for a signal like. The primary difference between this AM signal and the beat signal is that the envelope never goes to zero. This is because the DC component (5) is greater than the amplitude (4) of the 20-Hz component. When the carrier frequency becomes very high compared to the frequencies in as in Fig.~\u203b, it is possible to see the outline of the modulating cosine without drawing the envelope signal explicitly. This characteristic simplifies the implementation of a detector circuit in AM broadcast receivers.",
        "232": " AM signal Hz and Hz. The modulating signal has been superimposed as a light colored line to show its effect. AM signal Hz and Hz. The modulating signal has been superimposed as a light colored line to show its effect. In the frequency domain, the AM signal spectrum is nearly the same as the beat signal, the only difference being a relatively large term at. The spectrum can be derived by first breaking the time\u2013domain signal into two terms and then using our previous knowledge about the beat signal to get the following additive combination for the spectrum: AM signal Hz and Hz. The modulating signal has been superimposed as a light colored line to show its effect. Thus there are six spectral components for at the frequencies Hz and Hz, and also at the carrier frequency Hz (Fig.~\u203b). It is interesting to note that the spectrum for contains two identical subsets, one centered at and the other at. These subsets each contain three spectral lines, and it is easy to show that each subset is just a frequency shifted version of the two-sided spectrum of. In Chapter~\u203b we will show that this is true for a much broader class of signals.",
        "233": "Derive the spectrum of and plot it as a function of frequency. Compare this result to the spectral plot for the AM signal in Fig.~\u203b.",
        "234": "Periodic Waveforms A periodic signal satisfies the condition that for all, which states that the signal repeats its values every secs. The time interval is called the period of ; if it is the smallest such repetition interval, it is called the fundamental period. For example, the signal has a period of sec., but its fundamental period is sec. In this section, we show that periodic signals can be synthesized by adding two or more cosine waves that have harmonically related frequencies; i.e., all frequencies are integer multiples of a frequency. In other words, the signal would be synthesized as the sum of cosine waves where the frequency,, of the th cosine component in is The frequency is called the harmonic of because it is an integer multiple of the basic frequency, which is called the fundamental frequency.",
        "235": "EXAMPLE:\u00a0 Calculating The fundamental frequency is the largest such that. In mathematical terms, this is the greatest common divisor, so we can state For example, if the signal is the sum of sinusoids with frequencies 1.2, 2, and 6 Hz, then Hz, because 1.2 Hz is the harmonic, 2 Hz is the harmonic, and 6 Hz is the harmonic. What is the period of ? Since each cosine in has a period of, the sum must have exactly the same period and. Thus the period of is, the reciprocal of the fundamental frequency. Often, is also the fundamental period.",
        "236": "Using the complex exponential representation of the cosines, we can write as where was defined by in Section~\u203b.",
        "237": "Show that one possible period of the complex exponential signal is. Also show that the fundamental period is. Synthetic Vowel As an example of synthesizing a periodic signal, consider a case where the sum in ",
        "238": "contains nonzero terms for, and where the fundamental frequency is Hz. The numerical values of the complex amplitudes are listed in Table~\u203b.",
        "239": "This signal approximates the waveform produced by a man Complex amplitudes for the periodic signal that approximates the vowel \u201cah\u201d. The coefficients are given for positive indices, but the values for negative are the conjugates,. (Hz) Mag Phase 100 0 0 0 200 6 113 300 0 0 0 400 14 708 500 24 418 600 0 0 0 1500 0 0 0 1600 6 811 1700 2 362 ",
        "240": "speaking the vowel sound \u201cah.\u201d The two-sided spectrum of this signal is plotted in Fig.~\u203b. Note that all the frequencies are integer multiples of 100 Hz, even though there is no spectral component at 100 Hz itself. Also note that the negative frequency components have phase angles that are the negative of the phase angles of the corresponding positive frequency components, because the complex amplitudes for the negative frequencies are the complex conjugates of the complex amplitudes for the corresponding positive frequencies. Note that Fig.~\u203b shows the spectrum as two plots, one for the magnitudes and one for the phases. This is in contrast to Fig.~\u203b, where it was convenient to make just one plot and label the spectral components with their complex amplitudes.",
        "241": " Spectrum of signal defined in Table~\u203b. The magnitude is an even function with respect to ; the phase is odd. The synthetic vowel signal has ten spectral components, but only five sinusoidal terms when the real part is taken as in. It is interesting to examine the contribution of each real sinusoidal component separately. We can do this by successively plotting the waveforms corresponding to only one sinusoid, then two sinusoids, then three, etc. Figure~\u203b (top) shows a plot of the sinusoidal term in Table~\u203b alone.",
        "242": " Sum of all five terms in Table~\u203b. The 200-Hz term is shown in the top panel; additional terms are added one at a time until the entire vowel signal is created (bottom). Note that since the frequency of this component is Hz, the waveform is periodic with period msec. The next panel of Fig.~\u203b shows shows which is a plot of the sum of the and terms.",
        "243": "Now notice that the two frequencies are multiples of 200 Hz, so the period of is still 5 msec. Figure \u203b (middle) shows a plot of the sum of the first three terms,. Now we see that the period of the waveform is increased to msecs. This is because the three frequencies, 200, 400, and 500 Hz are integer multiples of 100 Hz; i.e., the fundamental frequency is now 100 Hz. Figure~\u203b (bottom) shows, the sum of all the terms in Table~\u203b. Note that the period of is msec, which equals, even though there is no component with frequency. The signal is typical of waveforms for vowel sounds in speech. The high frequencies in the signal contribute the fine detail in the waveform. This is evident in Fig.~\u203b as the waveform becomes increasingly complicated and more rapidly varying as higher-frequency components such as the th and th harmonics are added.",
        "244": " Demo: Vowel Synthesis Example of a Non-periodic Signal When we add harmonically related complex exponentials, we get a periodic result. What happens when the frequencies have no simple relation to one another? The sinusoidal synthesis formula is still valid, but now we will make no assumptions about the individual frequencies.",
        "245": "With a simple example, we want to demonstrate that periodicity is tied to harmonic frequencies. We can do this by taking a specific example. Consider the harmonic signal made up from the first, third, and fifth harmonics of a square wave with fundamental frequency Hz: A plot of is shown in Fig.~\u203b using a \u201cstrip chart\u201d format. The plot consists of three lines, each one containing 2 sec of the signal. The first line starts at, the second at, and the third at. This lets us view a long section of the signal, which in this case is clearly periodic, with period equal to 1/10 sec.",
        "246": " Demo: Spectrograms: Simple Sounds: Square Wave Now we create a second signal that is just a slight perturbation from the first. Define to be the sum of three sinusoids: The amplitudes are the same, but the frequencies have been changed slightly. The plot of in Fig.~\u203b shows that is not periodic.",
        "247": " Sum of cosine waves of harmonic frequencies. The fundamental frequency of is 10 Hz. Sum of cosine waves of harmonic frequencies. The fundamental frequency of is 10 Hz. The spectrum plots in Fig.~\u203b will help explain the difference between the signals in Figs.~\u203b and \u203b. In Fig.~\u203b(a), the frequencies are integer multiples of a common frequency, Hz, so the waveform of Fig.~\u203b is periodic with period sec. The waveform of Fig.~\u203b is nonperiodic. We can justify this in the \u201cfrequency domain\u201d by examining Fig.~\u203b(b), which shows that the spectrum of the signal in Figs.~\u203b does not have a fundamental frequency, since the frequencies are not integer multiples of a common fundamental frequency. These spectrum plots show \u201chow much\u201d of each cosine wave is in the sum, and they are very similar. However, the frequencies are slightly different: and. These slight shifts of frequency make a dramatic difference in the time waveform.",
        "248": " Spectrum of (a) the harmonic waveform in Fig.~\u203b which has a period of 0.1 sec, and (b) the nonharmonic waveform in Fig.~\u203b that is not periodic. ",
        "249": "Fourier Series The examples in Sec.~\u203b show that we can synthesize periodic waveforms by using a sum of harmonically related sinusoids. Now, we want to describe a general theory that shows how any periodic signal can be synthesized with a sum of harmonically related sinusoids, although the sum may need an infinite number of terms. This is the mathematical theory of Fourier Series which uses the following representation: where is the fundamental period of the periodic signal. The complex exponential in has a frequency equal to Hz, so all the frequencies are integer multiples of the fundamental frequency Hz.",
        "250": "There are two aspects of the Fourier theory: analysis and synthesis. Starting from and calculating is called Fourier analysis. The reverse process of starting from and generating is called Fourier synthesis. ",
        "251": "The formula in is the general synthesis formula. When the complex amplitudes are conjugate-symmetric, i.e.,, the synthesis formula becomes a sum of sinusoids of the form where, and the amplitude and phase of the term comes from the polar form,. In other words, the condition is sufficient for the synthesized waveform to be a real function of time.",
        "252": "By clever choice of the complex amplitudes in, we can represent a number of interesting periodic waveforms, such as square waves, triangle waves, and so on. The fact that a discontinuous square wave can be represented with an infinite number of sinusoids was one of the amazing claims in Fouriers famous thesis of 1807. It took many years before mathematicians were able to develop a rigorous convergence proof to verify Fouriers claim. Fourier Series: Analysis How do we derive the coefficients for the harmonic sum, i.e., how do we go from to ? The answer is that we use the Fourier series integral to perform Fourier analysis. The complex amplitudes for any periodic signal can be calculated with the Fourier integral where is the fundamental period of. A special case of is that the DC component is obtained by A common interpretation of is that is simply the average value of the signal over one period.",
        "253": "The Fourier integral is convenient if we have a formula that defines over one period. Two examples will be presented later to illustrate this point. On the other hand, if is known only as a recording, then numerical methods such as those discussed in Chapter \u203b will be needed. Fourier Series Derivation In this section, we present a derivation of the Fourier Series integral formula. The derivation relies on a simple property of the complex exponential signal\u2013-the integral of a complex exponential over an integral number of periods is zero. In equation form, where again we have used for any integer (positive or negative).",
        "254": "Now we can generalize the zero-integral property of the complex exponential to involve two signals: } T_0 & } where the * superscript in denotes the complex conjugate. ",
        "255": "Proof: Proving the orthogonality property is straightforward. We begin with There are two cases to consider for the last integral: when the exponent becomes zero, so the integral is Otherwise, when the exponent is nonzero and we can invoke the zero-integral property in to get where. The orthogonality property of complex exponentials simplifies the rest of the Fourier Series derivation. If we assume that is valid, then we can multiply both sides by the complex exponential and integrate over the period. Notice that we are able to isolate one of the complex amplitudes in the final step by applying the orthogonality property.",
        "256": "The crucial step above occurs when the order of the infinite summation and the integration is swapped. This is a delicate manipulation that depends on convergence properties of the infinite series expansion. It was also a topic of research that occupied mathematicians for a good part of the early 19th century. For our purposes, if we assume that is a smooth function and has only a finite number of discontinuities within one period, then the swap is permissible.",
        "257": "The final analysis formula is obtained by dividing both sides of the equation by and writing on one side of the equation. Since could be any index, we can replace with to obtain ",
        "258": " where is the fundamental frequency of the periodic signal. This analysis formula goes hand in hand with the synthesis formula for periodic signals, which is",
        "259": " ",
        "260": "Spectrum of the Fourier Series When we discussed the spectrum in Section~\u203b, we described a graphical procedure for drawing the spectrum when is composed of a sum of complex exponentials. By virtue of the synthesis formula, the Fourier Series coefficients are, in fact, the complex amplitudes that define the spectrum of. In order to illustrate this general connection between the Fourier Series and the spectrum, we use the \u201csine-cubed\u201d signal. First, we derive the coefficients for, and then we sketch its spectrum.",
        "261": "EXAMPLE:\u00a0 Fourier Series without Integration Determine the Fourier Series coefficients of the signal: Solution: There are two ways to get the coefficients: plug into the Fourier integral, or use the inverse Euler formula to expand into a sum of complex exponentials. It is far easier to use the latter approach. Using the inverse Euler formula for, we get the following expansion of the sine-cubed function: We see that contains four frequencies: and rad/s. Since, the fundamental frequency is rad/sec.",
        "262": "The Fourier Series coefficients are indexed in terms of the fundamental frequency, so This example shows that it is not always necessary to evaluate an integral to obtain the coefficients. Now we can draw the spectrum (Fig.~\u203b) because we know that we have four nonzero components located at the four frequencies: rad/sec. We prefer to plot the spectrum versus frequency in hertz in this case, so the spectrum lines are at and 4.5 Hz. The second harmonic is missing and the third harmonic is at 4.5 Hz.",
        "263": " Spectrum of the \u201csine-cubed\u201d signal derived from its Fourier Series coefficients. Only the range from to Hz is shown. The complex amplitude of each spectrum line is equal to the Fourier Series coefficient for that frequency,. Use the Fourier integral to determine all the Fourier Series coefficients of the \u201csine-cubed\u201d signal. In other words, evaluate the integral for all. Hints: find the period first, so that the integration interval is known. In addition, you might find it easier to convert the function to exponential form (via the inverse Euler formula for ) before doing the Fourier integral on each of four different terms. If you then invoke the orthogonality property on each integral, you should get exactly the same answer as. Make a sketch of the spectrum of the signal defined by: ",
        "264": "Fourier Analysis of Periodic Signals",
        "265": "We can synthesize any periodic signal by using a sum of sinusoids, as long as we constrain the frequencies to be harmonically related. To demonstrate Fourier synthesis of waveshapes that do not look at all sinusoidal, we will work out the details for a square wave and a triangle wave in this section. The resulting formulas for their Fourier coefficients are relatively compact. Figure~\u203b shows the relationship between Fourier analysis and Fourier synthesis using representative plots for the square wave case. If you have access to MATLAB it is straightforward to write a Fourier Synthesis Program that takes a list of frequencies and a list of complex amplitudes and then produces a signal as the sum of several complex exponentials according to the finite Fourier synthesis summation formula This MATLAB programming exercise is described in more detail in the music synthesis lab.",
        "266": " Lab: #4 Synthesis of Sinusoidal Signals ",
        "267": " {Major components of Fourier analysis and synthesis showing the relationship between the original periodic signal, its spectrum, and the synthesized signal that approximates the original. }",
        "268": "The Square Wave The simplest example to consider is the periodic square wave, which is defined for one cycle by Figure~\u203b shows a plot of this signal which is called a 50% duty cycle square wave because it is off (equal to zero) during half of its period.",
        "269": " Plot of the square-wave signal whose \u201cduty cycle\u201d is 50. We will derive a formula that depends on for the complex amplitudes. First of all, we substitute the definition of into the integral and obtain The upper limit becomes, because the signal is zero for. Next, we perform the integration and simplify: Since, we can write the following general formula for the Fourier Series coefficients of the square wave. There is one shortcoming with this formula for ; it is not valid when because appears in the denominator. Therefore, we must evaluate separately using The formula for when has a numerator that is either 0 (for even) or 2 (for odd), because alternates between and. Therefore, the final answer for the Fourier series coefficients of the square wave has three cases: Notice that the formula for does not depend on the period,, but this is not usually the case. Also, the magnitude of these coefficients decreases as, so the high frequency terms contribute less when synthesizing the waveform via.",
        "270": "DC Value of a Square Wave The Fourier Series coefficient for has a special interpretation as the average value of the signal. If we repeat the analysis integral for the case where, then The integral is the area under the function for one period. If we think of the area as a sum and realize that dividing by is akin to dividing by the number of elements in the sum, we can interpret (\u203b) as the average value of the signal. In the specific case of the 50 duty-cycle square wave, the average value is because the signal is equal to for half the period and then for the other half. This checks with the earlier calculation that.",
        "271": "In the synthesis formula, the coefficient is an additive constant, so a change in its value will move the plot of the signal up or down vertically. The terminology \u201cDC\u201d comes from electric circuits, where a constant value of current is called direct current, or DC. It is common to call the DC coefficient, or DC term, in a Fourier expansion. Finally, one should note that the frequency of DC is.",
        "272": "Spectrum for a Square Wave Figure~\u203b shows the spectrum for the 50 duty cycle square wave analyzed in when the fundamental frequency is 25 Hz. Spectrum of a square wave derived from its Fourier Series coefficients. Only the range from to is shown. With a fundamental frequency of 25 Hz, this corresponds to frequencies ranging from Hz to Hz. Since for nonzero and even, the only frequencies present in the spectrum are the odd harmonics at,,, and so on. The complex amplitudes of the odd harmonics are the Fourier Series coefficients,, and these are used as the labels on the spectrum lines in Fig.~\u203b. Also the figure shows that the magnitude of these coefficients drops off as.",
        "273": " Demo: Spectrograms: Simple Sounds Synthesis of a Square Wave Using a simple MATLAB M-file, a synthesis was done via with a fundamental frequency of Hz, and. The fundamental period is secs. In Fig.~\u203b, the plots are shown for three different cases where the number of terms in the sum is, 7, and 17. Notice how the period of the synthesized waveform is always the same, because it is determined by the fundamental frequency.",
        "274": " Summing harmonic components via : (top panel); (middle) and (bottom). The DC level of is included in each synthesis. The synthesis formula usually can be simplified to a cosine form. For the particular case of the square wave coefficients, when we take the DC plus first and third harmonic terms, we get the sum of two cosines plus the constant (DC) level given in equation",
        "275": " As more harmonic terms are added, a square-wave signal waveshape would be approximated better with this sum of cosines. However, notice what happens in Fig.~\u203b as increases; the sum of cosines appears to converge to the constant values and, but the convergence is not uniformly good\u2013-the \u201cears\u201d at the discontinuous steps never go away completely. This behavior, which occurs at any discontinuity of a waveform, is called the Gibbs phenomenon, and it is one of the interesting subtleties of Fourier theory that is extensively studied in advanced treatments.",
        "276": " Lab: #4 Synthesis of Sinusoidal Signals Demo: Fourier Series Triangle Wave Another interesting case that is still relatively simple is that of a triangle wave shown in Fig.~\u203b.",
        "277": " Periodic triangle wave. The mathematical formula for the triangle wave consists of two segments. We have to give the definition of the waveform over exactly one period, so we do that for the time interval :",
        "278": " } 2(T_0-t)/T_0 & } where sec in Fig.~\u203b. Unlike the square wave, the triangle wave is a continuous signal.",
        "279": "Now we attack the Fourier integral for this case to derive a formula for the coefficients of the triangle wave. We might suspect from our earlier experience that the DC coefficient has to be found separately, so we do that first. Plugging into the definition with, we obtain If we recognize that the integral over one period is, in fact, the area under the triangle, we get For the general case where, we must break the Fourier Series analysis integral into two sections because the signal consists of two pieces: After integration by parts and many tedious algebraic steps, the integral for can be written as Since the numerator in equals either 0 or, and we can write the following cases for : Once again, this particular formula is independent of, the fundamental period of the triangle wave.",
        "280": "Starting from, derive the formula for the Fourier Series coefficients of the triangle wave. Use integration by parts to manipulate the integrands which contain terms of the form. Make a plot of the spectrum for the triangle wave (similar to Fig.~\u203b for the square wave). Use the complex amplitudes from and assume that Hz. Synthesis of a Triangle Wave The ideal triangle wave in Fig.~\u203b is a continuous signal, unlike the square wave which is discontinuous. Therefore, it is easier to approximate the triangle wave with a finite Fourier sum. Two cases are shown in Fig.~\u203b, for and 11. The fundamental frequency is equal to Hz. In the case the approximation is nearly indistinguishable from the triangularly-shaped waveform. Adding harmonics for will not improve the synthesis very much. Even the case is reasonably good, despite using only DC and two sinuosoidal terms. We can see the reason for this by plotting the spectrum (as in Exercise~\u203b), which will show that the high frequency components decrease in size much faster those of the square wave.",
        "281": " Summing harmonic components for the triangle wave via : first and third harmonics (top panel); up to and including the eleventh harmonic (bottom). For the approximation of the triangle wave, derive the mathematical formula for the sinusoids; similar to what was done in for the square wave. Convergence of Fourier Synthesis We can think of the finite Fourier sum as making an approximation to the true signal; i.e., In fact, we might hope that with enough complex exponentials we could make the approximation perfect. This leads us to define an error signal,, as the difference between the true signal and the synthesis with terms, i.e.,. We can quantify the error by measuring a feature of the error, such as its maximum magnitude. This would be called the worst-case error. Now we can compare the and approximations of the triangle wave by comparing their worst-case errors. If Fig.~\u203b is zoomed, these errors can be measured and the result is 0.0497 for and 0.0168 for. Because the triangle wave is continuous, the maximum error decreases to zero as (i.e., there is no Gibbs phenomenon). This is not the case for the discontinuous square wave where the maximum error is always half the size of the jump in the waveform right at the discontinuity point with an overshoot of about 9 of the size of the discontinuity on either side. This is illustrated in Fig.~\u203b for and. Fig.~\u203b was generated by using the Fourier coefficients from in a MATLAB script to generate a plot of the worst-case error.",
        "282": " Error magnitude when approximating a square wave with a sum of harmonic components via : (top panel) and (bottom). ",
        "283": "Parsevals Theorem One reason that the Fourier Series is so useful is that there is an equivalence between the sum of the squares of the Fourier coefficients and the signal power. The average power of the periodic signal is the total energy of one period of divided by the duration of the period. The average power is a convenient measure of the size (or strength) of the signal.",
        "284": "EXAMPLE Average Power in a Sinusoid. For the case where, the average power integral",
        "285": " can be evaluated directly. Although it is possible to use a trigonometric identity, we will use Eulers formula followed by the complex number identity. The Fourier Series has only two non-zero coefficients, and, so an alternate way of evaluating the average power is: Amazingly, this result generalizes to the case where the Fourier Series has more than two coefficients. The somewhat tedious proof uses the orthogonality property of complex exponential signals. The general result, known as Parsevals Theorem is",
        "286": " ",
        "287": "Use Parsevals Theorem to complete the proof the famous formula: by finding the numerical value of the integer. Hint: use the square wave and its Fourier Series coefficients (except for DC).",
        "288": "The Nature of the Fourier Series Approximation One consequence of Parsevals Theorem is that we can measure the approximation error when the Fourier Series expansion is truncated to the range. If we start with a periodic signal and assume that we have its Fourier Series coefficients,, then Parsevals Theorem (\u203b) says that each coefficient contributes to the total power of the signal. Therefore, the power in the difference between the true signal and the term approximation is but this presents a difficulty when has a point of discontinuity, as in the square wave example. A better way to define the limiting process is to use what is called the squared error, or error power. In this definition, the error is a cumulative difference between the lefthand and righthand sides of (\u203b), and it becomes small when the total error power over one period goes to zero. As shown in Fig.~\u203b, Parsevals Theorem guarantees that the error power will get small whenever the signal has finite average power. The integrand in (\u203b) should be called the approximation error when a finite number of Fourier Series terms are used to represent. If we define to be the signal formed by a finite sum of complex exponentials: then the error signal is.",
        "289": "The best example of convergence under these two interpretations is given by the square wave which has two points of discontinuity per period. Figure~\u203b allows us to compare and for several values of. As increases, the signal has higher frequency oscillations, but it also gets closer to over most of the time interval. On the other hand, when we focus our attention on the regions near the discontinuous edges, we notice that the size of the last oscillation is not decreasing. This \u201covershoot\u201d is called the Gibbs Phenomenon, after J. Gibbs who first proved that the size of the overshoot does not decrease with, and in fact is always equal to about 9% of the size of the discontinuity in the square wave. Also we notice that is always equal to at the edges, but the definition of is ambiguous at those points (it could be either or ). These two observations are the specific reasons why pointwise convergence is not obtained for the Fourier Series. However, we can notice in Fig.~\u203b that the overshoot is getting narrower even as its maximum amplitude stays the same. Thus the power in the overshoot is decreasing \u2014 in other words, there is convergence according to the squared error measure, which is confirmed in Fig.~\u203b.",
        "290": "Time\u2013Frequency Spectrum We have seen that a wide range of interesting waveforms can be synthesized by the equation These waveforms range from constants, to cosine signals, to general periodic signals, to complicated-looking signals that are not periodic. One assumption we have made so far is that the amplitudes, phases, and frequencies in do not change with time. However, most real-world signals exhibit frequency changes over time. Music is the best example. For very short time intervals, the music may have a \u201cconstant\u201d spectrum, but over the long term, the frequency content of the music changes dramatically. Indeed, the changing frequency spectrum is the very essence of music. Human speech is another good example. Vowel sounds, if held for a long time, exhibit a \u201cconstant\u201d nature because the vocal tract resonates with its characteristic frequency components. However, as we speak different words, the frequency content is continually changing. In any event, most interesting signals can be modelled as a sum of sinusoids if we let the frequencies, amplitudes, and phases vary with time. Therefore, we need a way to describe such time\u2013frequency variations. This leads us to the concept of a time\u2013frequency spectrum or spectrogram. ",
        "291": "The mathematical concept of a time\u2013frequency spectrum is a sophisticated idea, but the intuitive notion of such a spectrum is supported by common, everyday examples. The best example to cite is musical notation (Fig.~\u203b). A musical score specifies how a piece is to be played by giving the notes to be played, the time duration of each note, and the starting time of each. The notation itself is not completely obvious, but the horizontal \u201caxis\u201d in Fig.~\u203b is time, while the vertical axis is frequency. The time duration for each note varies depending on whether it is a whole note, half note, quarter note, eighth, sixteenth, etc. In Fig.~\u203b most of the notes are sixteenth notes, indicating that the piece should be played briskly. If we assign a time duration to a sixteenth note, then all sixteenth notes should have the same duration. An eighth note would have twice the duration of a sixteenth note, and a quarter note would be four times longer than a sixteenth note, and so on.",
        "292": " Sheet music notation is a time\u2013frequency diagram. The vertical axis has a much more complicated notation to define frequency. If you look carefully at Fig.~\u203b, you will see that the black dots that mark the notes lie either on one of the horizontal lines or in the space between two lines. Each of these denotes a white key on the piano keyboard depicted in Fig~\u203b, and each key produces a different frequency tone. The black keys on the piano are denoted by \u201csharps\u201d or flats\u201d. Figure \u203b has a few notes sharped. The musical score is divided into a treble section (the top five lines) and a bass section (the bottom five lines). The vertical reference point for the notes is \u201cmiddle C,\u201d which lies on an invisible horizontal line between the treble and bass sections (key number 40 in Fig.~\u203b). Thus the bottom horizontal line in the treble section represents the white key (E) that is two above middle C; that is, key number 44 in Fig.~\u203b.",
        "293": " Piano keys can be numbered from 1 to 88. Three octaves are shown. Middle C is key 40. A\u2013440 is key 49. Once the mapping from the musical score to the piano keys has been made, we can write a mathematical formula for the frequency of each note. A piano, which has 88 keys, is divided into octaves containing twelve keys each. The meaning of the word octave is a doubling of the frequency. Within an octave, the neighboring keys maintain a constant frequency ratio. Since there are twelve keys per octave, the ratio is With this ratio, we can compute the frequencies of all keys if we have one reference. The convention is that the A key above middle C, called A\u2013440, has frequency 440 Hz. Since A\u2013440 is key number 49 and middle C is key number 40, the frequency of middle C is It is not our objective to explain how to read sheet music, although two of the lab projects on the CD-ROM investigate methods for synthesizing waveforms to create songs and musical sounds. What is interesting about musical notation is that it uses a two-dimensional display to indicate frequency content that changes with time. If we adopt a similar notation, we can specify how to synthesize sinusoids with time-varying frequency content. Our notation is illustrated in Fig.~\u203b.",
        "294": " Lab: #4 Synthesis of Sinusoidal Signals Lab: #5 FM Synthesis for Musical Instruments Stepped Frequency The simplest example of time-varying frequency content is to make a waveform whose frequency stays constant for a short duration and then steps to a higher (or lower) frequency. An example from music would be to play a scale that would be a succession of notes progressing over one octave. For example, the C-major scale consists of playing the notes D, E, F, G, A, B, C one after another, starting at middle C. This scale is played completely on the white keys. The frequencies of these notes are:",
        "295": " Middle-C D E F G A B C 262 Hz 294 330 349 392 440 494 523 Ideal time-frequency diagram for playing the C-major scale. The horizontal dotted lines correspond to the five lines in the treble staff of sheet music (Fig.\u203b). A graphical presentation of the C-major scale is shown in Fig.~\u203b. It should be interpreted as follows: Synthesize the frequency 262 Hz for 200 msec, then the frequency 294 Hz during the next 200 msec, and so on. The total waveform duration will be 1.6 sec. In music notation, the notes would be written as in Fig.~\u203b(top), where each note is a quarter note. ",
        "296": " Musical notation for the C-major scale, and the corresponding spectrogram computed using MATLABs specgram function. Spectrogram Analysis The frequency content of a signal can be considered from two points of view: analysis or synthesis. For example, the ideal time\u2013frequency diagram in Figure~\u203b specifies a rule for synthesizing the C-major scale. Analysis is a more challenging problem, as we saw in Section~\u203b, where the Fourier series analysis integral was given.",
        "297": "Analysis for time-varying frequencies is usually considered a subject reserved for advanced graduate courses. One reason is that we cannot write a simple mathematical formula like the Fourier series integral to do the analysis. On the other hand, excellent numerical routines are now available for time\u2013frequency analysis. Specifically, we can compute a spectrogram, which is a two-dimensional function of time and frequency that displays the time variation of the spectral content of a signal.",
        "298": " Demo: Spectrograms: Real Sounds: Piano In MATLAB, the function specgram will compute the spectrogram, and its default values work well for most signals. Therefore, it is reasonable to see what sort of output can be produced by the specgram function. Figure~\u203b shows the results of applying specgram to the stepped-frequency sinusoids that make up the C-major scale. The calculation is performed by doing a frequency analysis on short segments of the signal and plotting the results at the specific time at which the analysis is done. By repeating this process with slight displacements in time, a two-dimensional array is created whose magnitude can be displayed as a grayscale image whose horizontal axis is time and whose vertical axis is frequency. The time axis must be interpreted as the \u201ctime of analysis\u201d because the frequency calculation is not instantaneous; rather, it is based on a finite segment of the signal\u2013-in this case, 25.6 msec.",
        "299": " It is quite easy to identify the frequency content due to each note, but there are also some interfering artifacts that make the spectrogram in Fig.~\u203b less than ideal. In Chapter \u203b, we will undertake a discussion of frequency analysis in order to explain how the spectrogram is calculated and how one should choose the analysis parameters to get a good result. Even though the spectrogram is a highly advanced idea in signal analysis, its application is relatively easy and intuitive, especially for music signals which are described symbolically by a notation that is very much like a spectrogram.",
        "300": "Frequency Modulation: Chirp Signals Section \u203b revealed the possibility that interesting sounds can be created when the frequency varies as a function of time. In this section, we use a different mathematical formula to create signals whose frequency is time-varying. We will also pursue this idea in Lab~#5.",
        "301": " Lab: #5 FM Synthesis for Musical Instruments Chirp, or Linearly Swept Frequency A \u201cchirp\u201d signal is a swept-frequency signal whose frequency changes linearly from some low value to a high one. For example, in the audible region, we might begin at 220 Hz and go up to 2320 Hz. One method for producing such a signal would be to concatenate a large number of short constant-frequency sinusoids, whose frequencies step from low to high. This approach has one notable disadvantage: The boundary between the short sinusoids will be discontinuous unless we are careful to adjust the initial phase of each small sinusoid. Figure~\u203b shows a time waveform where the frequency is being stepped. Notice the jumps at, 2, 3, 4, 5 secs, which, in this case, are caused by using for each small sinusoidal segment.",
        "302": "A better approach is to modify the formula for the sinusoid so that we get a time-varying frequency. Such a formula can be derived from the complex-exponential point of view. If we regard a constant-frequency sinusoid as the real part of a complex (rotating) phasor then the angle function of this signal is the exponent which obviously changes linearly with time. The time derivative of the angle function is, which equals the constant frequency.",
        "303": " Stepped-frequency sinusoid, with six frequencies changing from 30 Hz to 80 Hz in 10 Hz increments. The frequency changes once per millisecond. Therefore, we adopt the following general notation for the class of signals with time-varying angle function: where denotes the angle function versus time. For example, we can create a signal with quadratic angle function by defining Now we can define the instantaneous frequency for these signals as the slope of the angle function (i.e., its derivative) where the units of are rad/sec, or, if we divide by we obtain Hz. If the angle function of is quadratic, then its frequency changes linearly with time; that is, The frequency variation produced by the time-varying angle function is called frequency modulation, and signals of this class are called FM signals. Finally, since the linear variation of the frequency can produce an audible sound similar to a siren or a chirp, the linear FM signals are also called chirp signals, or simply chirps.",
        "304": " Demo: Spectrograms: Chirp Sounds The process can be reversed because states that the instantaneous frequency is the derivative of the angle function. Thus, if a certain linear frequency sweep is desired, the actual angle function needed in is obtained from the integral of.",
        "305": "EXAMPLE:\u00a0 Synthesize a Chirp Formula Suppose we want to synthesize a frequency sweep from Hz to Hz over a 3-second time interval, i.e., the beginning and ending times are and sec. First of all, it is necessary to create a formula for the instantaneous frequency Then we must integrate to get the angle function: where the phase shift,, is an arbitrary constant. The chirp signal is. A Closer Look at Instantaneous Frequency It may be difficult to see why the derivative of the angle function would be the instantaneous frequency. The following experiment provides a clue.",
        "306": " Define a \u201cchirp\u201d signal as in Example~\u203b with the following parameters: In other words, determine and in to define so that it sweeps the specified frequency range. Now make a plot of the signal synthesized in (a). In Fig.~\u203b, this plot is the middle panel.",
        "307": " Comparing a chirp signal (middle) to constant-frequency sinusoids (top and bottom). Notice where the local frequency of the chirp is equal to one of the sinusoids. It is difficult to verify whether or not this chirp signal will have the correct frequency content. However, the rest of this experiment will demonstrate that the derivative of the angle function is the \u201ccorrect\u201d definition of instantaneous frequency. First of all, plot a 300-Hz sinusoid, which is shown in the upper panel of Fig.~\u203b. Finally, generate and plot a 500-Hz sinusoid, as in the bottom panel of Fig.~\u203b. Now compare the three signals in Fig.~\u203b with respect to the frequency content of the chirp. Concentrate on the frequency of the chirp in the time range sec. Notice that the 300-Hz sinusoid matches the chirp in this time region. Evaluate the theoretical in this region. It is possible to find another region (near s) where the chirp frequency is equal (locally) to 500 Hz. We have seen that for signals of the form, the instantaneous frequency of the signal is the derivative of the angle function. If is constant, the frequency is zero. If is linear, is a sinusoid at some fixed frequency. If is quadratic, is a chirp signal whose frequency changes linearly versus time. More complicated variations of can produce a wide variety of signals. One application of FM signals is in music synthesis. This application is illustrated with demos and a lab on the CD-ROM.",
        "308": " Demo: Spectrograms: Chirp Sounds ",
        "309": "Summary and Links This chapter introduced the concept of the spectrum, in which we represent a signal by its sinusoidal components. The spectrum is a graphical presentation of the complex amplitude for each frequency component in the signal. We showed how complicated signals can be formed from relatively simple spectra, and we presented the essential concepts of the Fourier Series so that we could form the spectrum of arbitrary periodic signals. Finally, we ended with a discussion of how the spectrum can vary with time.",
        "310": "At this point, so many different demonstrations and projects can be done that we must limit our list somewhat. Among the laboratory projects on the CD-ROM are three devoted to different aspects of the spectrum. The first (Lab #3) contains exercises on the Fourier series representation of the square wave and a sawtooth wave. The second (Lab #4) requires students to develop a music synthesis program to play a piece such as Bachs \u201cJesu, Joy of Mans Desiring.\u201d This synthesis must be done with sinusoids, but can be refined with extras such as a tapered amplitude envelope. Finally, Lab #5 deals with beat notes, chirp signals, and spectrograms. The second part of this lab involves a music synthesis method based on frequency modulation. The FM synthesis algorithm can produce realistic sounds for instruments such as a clarinet or a drum. In addition, there is one more lab that involves some practical systems that work with sinusoidal signals, such as a Touch-Tone phone. This lab, however, requires some knowledge of filtering so it is reserved for Chapter 7. Write-ups of the labs can be found on the CD-ROM.",
        "311": " Demo: Links to many demos The CD-ROM also contains many demonstrations of sounds and their spectrograms:",
        "312": " Spectrograms of simple sounds such as sine waves, square waves, and other harmonics. Spectrograms of realistic sounds, including a piano recording, a synthetic scale, and a synthesized music passage done by one of the students who took an early version of this course. Spectrograms of chirp signals that show how the rate of change of the frequency affects the sound you hear. An explanation of the FM synthesis method for emulating musical instruments. Several example sounds are included for listening. Rotating Phasors Vowel Synthesis Note: Hundreds of Solved Problems Finally, the reader is reminded of the large number of solved homework problems that are available for review and practice on the CD-ROM.",
        "313": "CHAPTER 4 Sampling and Aliasing This chapter is concerned with the conversion of signals between the analog (continuous-time) and digital (discrete-time) domains. The primary objective of our presentation is an understanding of the Sampling Theorem, which states that when the sampling rate is greater than twice the highest frequency contained in the spectrum of the analog signal, the original signal can be reconstructed exactly from the samples.",
        "314": "The process of converting from digital back to analog is called reconstruction. A common example is given by audio CDs. Music on a CD is sampled at 44,100 times per second and stored in a digital form from which a CD player reconstructs the continuous (analog) waveform that we listen to. The reconstruction process is basically one of interpolation because we must \u201cfill in\u201d the missing signal values between the sample times by constructing a smooth curve through the discrete-time sample values. Although this process may be studied as time-domain interpolation, we will see that a spectrum or frequency-domain view is very helpful in understanding the important issues in sampling.",
        "315": "Sampling Sinusoidal waveforms of the form are examples of continuous-time signals. It is also common to refer to such signals as analog signals because both the signal amplitude and the time variable are assumed to be real (not discrete) numbers. Continuous-time signals are represented mathematically by functions of time,, where is a continuous variable. In earlier chapters, we have \u201cplotted\u201d analog waveforms using MATLAB, but actually we did not plot the continuous-time waveform. Instead, we really plotted the waveform only at isolated (discrete) points in time and then connected those points with straight lines. Indeed, digital computers cannot deal with continuous-time signals directly; instead, they must represent and manipulate them numerically (as with MATLAB), or sometimes symbolically (as with Mathematica or Maple). The key point is that any computer representation is discrete. (Recall the discussion in Section \u203b on p.~.)",
        "316": "A discrete-time signal is represented mathematically by an indexed sequence of numbers. When stored in a digital computer, the signal values are held in memory locations, so they would be indexed by memory address. We denote the values of the discrete-time signal as, where is the integer index indicating the order of the values in the sequence. The square brackets enclosing the argument allow us to differentiate between the continuous-time signal and a corresponding discrete-time signal. We can obtain discrete-time signals in either of the following ways:",
        "317": " We can sample a continuous-time signal at equally spaced time instants, ; that is, where represents any continuously varying signal, e.g., speech or audio. The individual values of are called samples of the continuous-time signal. The fixed time interval between samples,, can also be expressed as a fixed sampling rate,, in samples per second: Therefore, an alternative way to write the sequence in is.",
        "318": "Sampling can be viewed as a transformation or operation that acts on a continuous-time signal to produce an output which is a corresponding discrete-time signal. In engineering, it is common to call such a transformation a system, and represent it graphically with a block diagram that shows the input and output signals along with a name that describes the system operation. The sampling operation is an example of a {system} whose input is a continuous-time signal and whose output is a discrete-time signal as shown in Fig.~\u203b. The system block diagram of Fig.~\u203b represents the mathematical equation in and is called an ideal continuous-to-discrete (C-to-D) converter. Its idealized mathematical form is useful for analysis, but an actual hardware system for doing sampling is an analog-to-digital (A-to-D) converter, which approximates the perfect sampling of the C-to-D converter. A-to-D converters differ from ideal C-to-D converters because of real-world problems such as amplitude quantization to 12 or 16 bits, jitter in the sampling times, and other factors that are difficult to analyze. Since these factors can be made negligible with careful design, we can safely confine our presentation to the ideal C-to-D system.",
        "319": " Block diagram representation of the ideal continuous-to-discrete (C-to-D) converter. The parameter specifies uniform sampling of the input signal every seconds. We can also compute the values of a discrete-time signal directly from a formula. A simple example is which determines the sequence of values corresponding to the indices. Although there might be no explicit underlying continuous-time signal that is being sampled, we will nevertheless often refer to the individual values of the sequence as samples. Discrete-time signals described by formulas will be very common in our study of discrete-time signals and systems.",
        "320": "When we plot discrete-time signals, we will use the format shown in Fig.~\u203b, which shows eight values (samples) of the sequence. Such plots show clearly that the signal has values only for integer indices; in between, the discrete-time signal is undefined.",
        "321": " Plotting format for discrete-time signals; sometimes called a \u201clolly pop\u201d or \u201ctinker-toy\u201d plot. In MATLAB, the function stem will produce this plot, so many students also refer to this as a \u201cstem plot.\u201d Sampling Sinusoidal Signals Sinusoidal signals are examples of continuous-time signals that exist in the \u201creal world\u201d outside the computer, and for which we can also write a simple mathematical formula. Because more general continuous-time signals can be represented as sums of sinusoids, and because the effects of sampling are easily understood for sinusoids, we will use them as the basis for our study of sampling.",
        "322": "If we sample a signal of the form of, we obtain where we have defined to be The signal in is a discrete-time cosine signal, and is its discrete-time frequency. We use a \u201chat\u201d over to denote that this is a new frequency variable. It is a normalized version of the continuous-time radian frequency with respect to the sampling frequency. Since has units of rad/sec, the units of are radians; i.e., is a dimensionless quantity. This is entirely consistent with the fact that the index in is dimensionless. Once the samples are taken from, the time scale information has been lost. The discrete-time signal is just a sequence of numbers, and these numbers carry no information about the sampling period,, used in obtaining them. An immediate implication of this observation is that an infinite number of continuous-time sinusoidal signals can be transformed into the same discrete-time sinusoid by sampling. All we need to do is change the sampling period inversely proportional to the input frequency of the continuous-time sinusoid. For example, if rad/sec and sec, then rad. On the other hand, if rad/sec and sec, is still equal to rad.",
        "323": "The top panel of Fig.~\u203b shows, a continuous-time sinusoid with frequency Hz. The middle panel of Fig.~\u203b shows the samples taken with sampling period msec. The sequence is given by the formula, so the discrete-time radian frequency is. (Since is dimensionless, it is redundant to specify its units as rad.) The sampling rate in this example is samples/sec. The sample values are plotted as discrete points as in the middle panel of Fig.~\u203b. The points are not connected by a continuous curve because we do not have any direct information about the value of the function between the sample values. In this case, there are 20 sample values per period of the signal, because the sampling frequency (2000 samples/sec) is 20 times higher than the frequency of the continuous signal (100 Hz). From this discrete-time plot, it appears that the sample values alone are sufficient to visually reconstruct a continuous-time cosine wave, but without knowledge of the sampling rate, we cannot tell what the frequency should be.",
        "324": " A continuous-time 100-Hz sinusoid (top) and two discrete-time sinusoids formed by sampling at samples/sec (middle) and at samples/sec (bottom). Another example of sampling is shown in the bottom panel of Fig.~\u203b. In this case, the 100 Hz sinusoid is sampled at a lower rate ( samples/sec) resulting in the sequence of samples. In this case, the discrete-time radian frequency is. The time between samples is msec, so there are only 5 samples per period of the continuous-time signal. We see that without the original waveform superimposed, it would be difficult to discern the precise waveshape of the original continuous-time sinusoid.",
        "325": "If the sampling rate is samples/sec. and the continuous-time signal is, what value of will give a sequence of samples identical to the discrete-time signal shown in the bottom panel of Fig.~\u203b? At first glance, Exercise~\u203b appears to ask a rather simple question because the formula in can be solved for in terms of and. Then it is easy to obtain the answer for Exercise~\u203b as rad/sec. But is this the only possible answer? In order to understand this question and find its answer, we need to look more closely at the sampling operation and a concept that we will call aliasing.",
        "326": "The Concept of Aliasing A simple definition of the word aliasing would involve something like \u201ctwo names for the same person, or thing.\u201d Let us now turn to the question of how aliasing arises in a mathematical treatment of discrete-time signals, specifically for discrete-time sinusoids. The sinusoid is the mathematical formula for the plot shown in the bottom panel of Fig.~\u203b, so it is one \u201cname\u201d that identifies that signal. Now consider another sinusoid,, apparently with a different frequency. To see how a plot of would look, we invoke the simple trigonometric identity to obtain because is an integer number of periods of the cosine function. In other words, this phenomenon that we are calling aliasing is solely due to the fact that trigonometric functions are periodic with period.",
        "327": " Demo: Sampling Theory Tutorial Figure \u203b shows that the stem plots of these two signals, and, are identical.",
        "328": " Illustration of aliasing: two continuous-time signals drawn through the same samples. The samples belong to two different cosine signals with different frequencies, but the cosine functions have the same values at. Figure~\u203b also shows continuous plots of the signals and, which when sampled with would give and. It is clear from Fig.~\u203b that the values of these continuous cosine signals are equal at integer values,. Since for all integers, we see that is another name for the same plot, the same discrete-time signal. It is an alias. ",
        "329": "The frequency of is, while the frequency of is. When speaking about the frequencies, we say that is an alias of. There are many more frequency aliases as the following problem suggests.",
        "330": "Show that is an alias of. In addition, find two more frequencies that are aliases of rad. In the previous exercise, it should be easy to see that adding any integer multiple of to will give an alias, so the following general formula holds for the frequency aliases: Since is the smallest of all the aliases, it is sometimes called the principal alias. ",
        "331": "However, we are not finished yet. There are other aliases. Another trigonometric identity states that, so we can generate another alias for as follows: The frequency of is. A general form for all the alias frequencies of this type would be For reasons that will become clear later on, these aliases of a negative frequency are called folded aliases. ",
        "332": "If we examine aliasing for the general discrete-time sinusoid, an extra complication arises for the folded case as illustrated by the following analysis: Notice that the algebraic sign of the phase angles of the folded aliases must be opposite to the sign of the phase angle of the principal alias.",
        "333": "Show that the signal is an alias of the signal. It might be instructive to make MATLAB plots of these two signals to verify that the phase must change sign to have identical plots. In summary, we can write the following general formulas for all aliases of a sinusoid with frequency : because the following signals are equal for all : If we were to make a stem plot of all of these signals (with specific numbers for,, and ), we would not be able to tell them apart, as was shown in the example of Fig.~\u203b.",
        "334": "Spectrum of a Discrete-Time Signal We have seen that it is sometimes very useful to represent a continuous-time sinusoidal signal by a spectrum plot. How would we plot the spectrum of a discrete-time signal? Aliasing makes this problematic because a given discrete-time sinusoidal sequence could correspond to an infinite number of different frequencies. Our approach will be to take this into account by making a plot that explicitly shows that there are many different sinusoids that have the same samples. Figure \u203b shows that we do this by drawing the spectrum representation of the principal alias along with several more of the other aliases.",
        "335": " Spectrum of a discrete-time sinusoid. The principal alias components are at. In Fig.~\u203b, the spectrum plot includes a representation of the principal alias, and two of the aliases, and. Recall from Eulers formula, on p.~, that the spectrum of each discrete-time alias signal consists of a positive frequency component and a corresponding component at negative frequency. Thus, is represented by the components at and is represented by the black components at, and so forth. Another way of thinking about constructing the plot in Fig.~\u203b is that the spectrum of the principal alias signal consisting of components at was moved up by and down by, i.e., the spectrum was shifted by integer multiples of.",
        "336": "In the case of the spectrum representation of a continuous-time signal, the assumption was that all of the spectrum components were added together to synthesize the continuous-time signal. This is not so for the discrete-time case. We show spectrum representations of several of the aliases simply to emphasize the fact that many different frequencies could produce the same time-domain sequence. In the case of a sum of discrete-time sinusoids, such as we would make a plot like Fig.~\u203b for each of the cosine signals and superimpose them on a single plot. To synthesize the time-domain signal corresponding to a given spectrum representation, we simply need to select one signal from each of the distinct alias sets. For example, includes only the principal aliases since and are both less than.",
        "337": "Make a spectrum plot for the signal of similar to Fig.~\u203b showing the principal alias and two other alias frequency sets. How would your plot change if the signal was The Sampling Theorem The plots shown in Fig.~\u203b naturally raise the question of how frequently we must sample in order to retain enough information to reconstruct the original continuous-time signal from its samples. The amazingly simple answer is given by the Shannon sampling theorem, one of the theoretical pillars of modern digital communications, digital control, and digital signal processing.",
        "338": " Shannon Sampling Theorem A continuous-time signal with frequencies no higher than can be reconstructed exactly from its samples, if the samples are taken at a rate that is greater than. Notice that the sampling theorem involves two issues. First, it talks about reconstructing the signal from its samples, although it never specifies the algorithm for doing the reconstruction. Second, it gives a minimum sampling rate that is dependent on the frequency content of, the continuous-time signal.",
        "339": "The minimum sampling rate of is called the Nyquist rate. We can see examples of the sampling theorem in many commercial products. For example, audio CDs use a sampling rate of 44.1 kHz for storing music signals in a digital format. This number is slightly more than two times 20 kHz, which is the generally accepted upper limit for human hearing and perception of musical sounds. In other applications, the Nyquist rate is significant because we are usually motivated to use the lowest possible sampling rate in order to minimize system cost in terms of storage, processing speed per sample, and so on.",
        "340": "The Shannon theorem states that reconstruction of a sinusoid is possible if we have at least two samples per period. What happens when we do not sample fast enough? The simple answer is that aliasing occurs. The next sections will delve into this issue by using a spectrum view of the C-to-D conversion process.",
        "341": "Ideal Reconstruction The sampling theorem suggests that a process exists for reconstructing a continuous-time signal from its samples. This reconstruction process would undo the C-to-D conversion so it is called D-to-C conversion. The ideal discrete-to-continuous (D-to-C) converter is depicted in Fig.~\u203b.",
        "342": " Block diagram of the ideal discrete-to-continuous (D-to-C) converter when the sampling rate is. The output signal must satisfy. Since the sampling process of the ideal C-to-D converter is defined by the substitution as in, we would expect the same relationship to govern the ideal D-to-C converter, i.e., but this substitution is only true when is a sum of sinusoids. In that special case where consists of one or more discrete-time sinusoids, such as, we can use the substitution in to produce at the D-to-C output. This is convenient because we obtain a mathematical formula for, but if we only have a sequence of numbers for that was obtained by sampling, and we do not know the formula for or there is no simple formula for the signal, things are not so simple. An actual D-to-A converter involves more than the substitution, because it must also \u201cfill in\u201d the signal values between the sampling times,. In Section~\u203b we will see how interpolation can be used to build an A-to-D converter that approximates the behavior of the ideal C-to-D converter. Later on in Chapter \u203b, we will use Fourier transform theory to show how to build better A-to-D converters by incorporating a lowpass (electronic) filter.",
        "343": "If the ideal C-to-D converter works correctly for a sampled cosine signal, then we can describe its operation as frequency scaling. For above, the discrete-time frequency is, and the continuous-time frequency of is. Thus the relationship appears to be. In fact, we can confirm the same result if we solve the frequency scaling equation for in terms of, Thus each frequency component in a discrete-time signal could be mapped to a frequency component of the continuous-time output signal. But there is one more issue: the discrete-time signal has aliases\u2013-an infinite number of them given by. Which discrete-time frequency will be used in ? The selection rule is arbitrary, but the ideal C-to-D converter always selects the lowest possible frequency components (the principal aliases). These frequencies are guaranteed to lie in the range, so when converting from to analog frequency, the output frequency always lies between and.",
        "344": " Demo: Continuous-Discrete Sampling Sampling and reconstruction system. In summary, the Shannon sampling theorem guarantees that if contains no frequencies higher than, and if, then the output signal of the ideal D-to-C converter is equal to the signal, the input to the ideal C-to-D converter in Fig.~\u203b.",
        "345": "Spectrum View of Sampling and Reconstruction We will now present a frequency spectrum explanation of the sampling process. Because the output of the C-to-D converter is a discrete-time signal with an infinite number of aliases, we will use the spectrum diagram that includes all of the aliases as discussed in Section~\u203b. Then the folding and aliasing of frequencies in Fig.~\u203b can be tracked via the movement of spectrum lines in the frequency domain.",
        "346": " Demo: Aliasing and Folding Spectrum of a Discrete-Time Signal Obtained by Sampling If a discrete-time sinusoid is obtained by sampling, its frequency is given by. If we include all the aliases as predicted by, it is necessary to plot an infinite number of spectrum lines to have a thorough representation of the spectrum. Of course, we can only plot a few of them and perhaps indicate that more exist outside our plotting range. Suppose that we start with a continuous-time sinusoid,, whose spectrum consists of two spectrum lines at with complex amplitudes of.",
        "347": "Plot the spectrum of the continuous-time signal. The sampled discrete-time signal also has two spectrum lines at, but it also must contain all the aliases at the following discrete-time frequencies: This illustrates the important fact that when a discrete-time sinusoid is derived by sampling, the alias frequencies all are based on the normalized value,, of the frequency of the continuous-time signal. The alias frequencies are obtained by adding multiples of radians to the normalized value frequency.",
        "348": "The next sections show a set of examples of sampling a continuous-time 100 Hz sinusoid of the form. The sampling frequency is varied to show what happens at different sampling rates. The examples in Figs.~(\u203b\u2013\u203b) show the discrete-time spectrum for different cases where is above or below the Nyquist rate. All were constructed with a MATLAB M-file that plots the location of spectrum components involved in the sampling process. Over-Sampling In most applications, we try to obey the constraint of the sampling theorem by sampling at a rate higher than twice the highest frequency so that we will avoid the problems of aliasing and folding. This is called over-sampling. For example, when we sample the 100 Hz sinusoid, at a sampling rate of samples/sec, we are sampling two and a half times faster than the minimum required by the sampling theorem. The time- and frequency-domain plots are shown in Fig.~\u203b. With reference to the C-to-D and D-to-C blocks in Fig.~\u203b, the top panel of Fig.~\u203b shows the spectrum of the input signal, while the bottom panel shows the spectrum of, which is the output of the C-to-D converter. The middle panel shows the time-domain plot of, and the reconstructed output.",
        "349": " Sampling a 100 Hz sinusoid at samples/sec. The spectrum plot (bottom) shows the aliased spectrum components as well as the positive and negative frequency components of the original sinusoid at rad. The time-domain plot (middle) shows the samples as gray dots, the original signal as a continuous orange line, and the reconstructed signal as a dashed black line. For the continuous-time frequency-domain spectrum representation of the 100 Hz sinusoid (top panel of Fig.~\u203b), the frequency axis is measured in hertz. Note that the negative-frequency complex exponential component of the cosine is denoted with a * at the top of its spectrum line. The middle panel shows,, and together, but keeps the horizontal axis as time in milliseconds. The bottom plot contains the spectrum of the discrete-time signal versus the normalized frequency. According to the frequency scaling equation, the input analog frequency of 100 Hz maps to, so we plot spectrum lines at. Then we also draw all the aliases at The D-to-C converter transforms the discrete-time spectrum to the continuous-time output spectrum. But there is one complication: the D-to-C converter must select just one pair of spectrum lines from all the possibilities given by. The selection rule is arbitrary, but in order to be consistent with real D-to-A converter operation, we must assert that the ideal D-to-C converter always selects the lowest possible frequency for each set of aliases. These are what we have called the principal alias frequencies, and in Fig.~\u203b these are the frequency components that fall inside the dashed box shown in the bottom panel, i.e.,. They satisfy the relationship,, and when they are converted from to analog frequency, the result is Hz. Since the frequency components inside the dashed box satisfy, the spectral lines for the output will always lie between and.",
        "350": "In summary, for the oversampling case where the original frequency is less than, the original waveform will be reconstructed exactly. In the present example, Hz and, so the Nyquist condition of the sampling theorem is satisfied, and the output equals the input as shown in the middle panel of Fig.~\u203b. Aliasing Due to Under-Sampling When, the signal is under-sampled. For example, if Hz and Hz, we can show that aliasing distortion occurs. In the top panel of Fig.~\u203b, the spectrum of the analog input signal is shown, along with a dashed line indicating the sampling rate at Hz.",
        "351": " Sampling a 100 Hz sinusoid at samples/sec. The time-domain plot (middle) shows the samples as gray dots, the original sinusoid as a continuous orange line, and the reconstructed signal as a dashed black line. In this case, is a 20 Hz sinusoid that passes through the same sample points. The spectrum of the discrete-time signal (in the bottom panel) contains lines at, as predicted by the frequency scaling equation. In order to complete the discrete-time spectrum we must also draw all the aliases at In the middle panel of Fig.~\u203b, the 100 Hz sinusoid (solid orange line) is sampled too infrequently to be recognized as the original 100 Hz sinusoid. When we examine the D-to-C process for this case, we use the lowest frequency spectrum lines from the discrete-time spectrum (bottom panel). These are at, so we calculate the output spectrum lines at Hz. Another way to state this result is to observe that the same samples would have been obtained from a 20 Hz sinusoid. The reconstructed signal is that 20 Hz sinusoid shown as the black dashed line in Fig.~\u203b (middle). Notice that the alias frequency of 20 Hz can be found by subtracting from 100 Hz. Understanding this point is the key to aliasing.",
        "352": "The aliasing of sinusoidal components can have some dramatic effects. Figure~\u203b shows the case where the sampling rate and the frequency of the sinusoid are the same. Clearly, what happens is that the samples are always taken at the same place on the waveform, so we get the equivalent of sampling a constant (DC), which is the same as a sinusoid with zero frequency. We can justify this result in the frequency domain by noting that the discrete-time signal must contain spectrum lines at, and also at the aliases separated by. Thus, one of the aliases lands at, and that is the one reconstructed by the D-to-C converter.",
        "353": " Sampling a 100 Hz sinusoid at samples/sec. The time-domain plot (middle) shows the samples as gray dots, the original sinusoid as a continuous orange line, and the reconstructed signal as a dashed black line. The discrete-time spectrum (bottom) contains only one line at. Folding Due to Under-Sampling Figure~\u203b shows the case where under-sampling leads to folding; here the sampling rate is samples/sec. Once again, the top panel shows the spectrum of the continuous-time signal with spectrum lines at Hz. Sampling a 100 Hz sinusoid at samples/sec. The time-domain plot (middle) shows the samples as gray dots, the original sinusoid as a continuous orange line, and the reconstructed signal as a dashed black line. The discrete-time spectrum (bottom) contains only one line at. The discrete-time spectrum (bottom panel) is constructed by mapping Hz to the two spectrum lines at, and then including all the aliases to get lines at In this case, an interesting thing happens. The two frequency components between are, but the one at is an alias of. This is an example of folding. The analog frequency of the reconstructed output will be Hz. An additional fact about folding is that the sign of the phase of the signal will be changed. If the original 100-Hz sinusoid had a phase of, then the phase of the component at would be, and it follows that the phase of the aliased component at would also be. After reconstruction, the phase of would be. It is possible to observe this \u201cphase-reversal\u201d in the middle panel of Fig.~\u203b. The input signal (solid orange line) is going down at, while the reconstructed output (black dashed line) is going up. This means that when we sample a 100 Hz sinusoid at a sampling rate of 125 samples/sec, we get the same samples that we would have gotten by sampling a 25 Hz sinusoid, but with opposite phase. Maximum Reconstructed Frequency The preceding examples have one thing in common: the output frequency is always less than. For a sampled sinusoid, the ideal D-to-C converter picks the alias frequency closest to and maps it to the output analog frequency via. Since the principal alias is guaranteed to lie between and, the output frequency will always lie between and.",
        "354": " Lab: #3, Chirp Synthesis from Ch.~\u203b One striking demonstration of this fact can be made by implementing the system in Fig.~\u203b and using a linear FM chirp signal as the input, and then listening to the reconstructed output signal. Suppose that the instantaneous frequency of the input chirp increases according to the formula Hz, i.e.,. After sampling, we obtain Once is reconstructed from, what would you hear? It turns out that the output frequency goes up and down as shown in Fig.~\u203b.",
        "355": " Folding of a sinusoid sampled at samples/sec. The apparent frequency is the lowest frequency of a sinusoid that has exactly the same samples as the input sinusoid. This happens because we know that the output cannot have a frequency higher than, even though the input frequency is continually increasing. If we concentrate on a specific input frequency, we can map it to its normalized discrete frequency, determine all of the aliases, and then figure out the reconstructed frequency from the alias closest to. There is no easy formula, so we just consider a few separate cases. First of all, when the input frequency goes from 0 to, will increase from 0 to and the aliases will not need to be considered. The output frequency will be equal to the input frequency. Now consider the input frequency increasing from to. The corresponding frequency for increases from to, and its negative frequency companion goes from to. The principal alias of the negative frequency component goes from to. The reconstructed output signal will, therefore, have a frequency going from to.",
        "356": "Figure~\u203b shows a plot of the output frequency versus the input frequency. As goes from to, the output frequency decreases from to 0. The terminology folded frequency comes from the fact that the input and output frequencies are mirror images with respect to, and would lie on top of one another if the graph were folded about the line in Fig.~\u203b.",
        "357": "Strobe Demonstration One effective means for demonstrating aliasing is to use a strobe light to illuminate a spinning object. In fact, this process is used routinely to set the timing of automobile engines and gives a practical example where aliasing is a desirable effect. In our case, we use a disk attached to the shaft of an electric motor that rotates at a constant angular speed (see Fig.~\u203b). On this white disk is painted a spot that is easily seen whenever the strobe light flashes. In our particular case, the rotation speed of the motor is approximately 750 rpm, and the flash rate of the strobe light is variable over a wide range. ",
        "358": " Demo: Strobe Movie The disk attached to the shaft of a motor rotates clockwise at a constant speed. Suppose that the flashing rate is very high, say nine times the rotation rate, i.e., rpm. In this case, the disk will not rotate very far between flashes. In fact, for the case, it will move only per flash, as in Fig.~\u203b. Since the movement will be clockwise, the angular change from one flash to the next is.",
        "359": " Six successive positions of the spot for a very high flashing rate. Light spots indicate previous locations of the dark spot. The disk is rotating clockwise, and the spot appears to move in the same direction. Angular change is per flash. If the flash rate of the strobe is set equal to 750 flashes/min, i.e., the rotation rate of the disk, then the spot will appear to stand still. This is aliasing because the spot makes exactly one revolution between flashes and therefore is always at the same position when illuminated by the strobe. It is exactly the same situation illustrated in Fig.~\u203b where the frequency of the sinusoid aliased to zero. This is not the only flash rate for which the spot will appear to stand still. In fact, a slower flash rate that permits two or three or any integer number of complete revolutions between flashes will create the same effect. In our case of a 750-rpm motor, flash rates of 375, 250,, 150, and 125 flashes/min will also work.",
        "360": " Demo: Synthetic Strobe Movies By using flashing rates that are close to these numbers, we can make the spot move slowly, and we can also control its direction of motion (clockwise or counterclockwise). For example, if we set the strobe for a flashing rate that is just slightly higher than the rotation rate, we will observe another aliasing effect. Suppose that the flashing rate is 806 flashes/min; then the disk rotates slightly less than one full revolution between flashes. We can calculate the movement if we recognize that one complete revolution takes min. Therefore, Once again, the minus sign indicates rotation in a clockwise direction, but since the angular change is almost, we would observe a small positive angular change instead, and the spot would appear to move in the counterclockwise direction, as shown in Fig.~\u203b. The fact that one can distinguish between clockwise rotation and counterclockwise rotation is equivalent to saying that positive and negative frequencies have separate physical meanings.",
        "361": " Six successive positions of the spot for a flashing rate that aliases the spot motion. Light spots indicate previous locations of the dark spot. The disk is rotating clockwise, but the spot appears to move counterclockwise. Angular change is per flash. In order to analyze the strobe experiment mathematically, we need a notation for the motion of the spot as a function of time. The spot moves in an \u2013 coordinate system, so a succinct notation is given by a complex number whose real part is and whose imaginary part is. The position of the spot is Furthermore, since the motion of the spot is on a circle of radius, the correct formula for is a complex exponential with constant frequency. The minus sign in the exponent indicates clockwise rotation, and the initial phase specifies the location of the spot at (see Fig.~\u203b). The frequency of the motor rotation is constant, as is the radius. It will be convenient to set in what follows so that the formulas will be less cluttered.",
        "362": " The position of the spot on the disk can be expressed as a rotating phasor versus time. The frequency is the rotation rate of the motor in rpm. The effect of the strobe light is to sample at a fixed rate given by the flashing rate. Thus the position of the spot at the th flash can be expressed as the discrete-time signal Substituting into the complex exponential we get If the constraint of the sampling theorem is met (i.e., }, then there will be no aliasing in the experiment. In fact, the angular change from one sample time to the next will be between and, so the spot will appear to rotate clockwise.",
        "363": "The more interesting cases occur when the flashing rate drops below. Then the disk may make one or more revolutions between flashes, which introduces the aliasing phenomenon. Using the sampled position formula, we can solve the following type of problem: Find all possible flashing rates so that the spot will move counterclockwise at a rate of per flash, which is equivalent to one revolution every 14.4 flashes. Assume a constant motor speed of rpm. One twist to this problem is that the two rotation rates are specified in different units.",
        "364": "A systematic approach is possible if we use the property, whenever is an integer. The desired rotation of the spot can be expressed as where the factor equals converted to radians. The initial phase is set equal to in, so that. Then we can equate and, but we throw in the factor, which is just multiplying by one Now with the following analysis we can generate an equation that can be solved for the flashing rates: So, finally we can solve for the flashing rate where is any integer. Since we want positive values for the flashing rate, and since there is a minus sign associated with the clockwise rotation rate of the motor, we choose to generate the different solutions. For example, when the motor rpm is 750, the following flashing rates (in flashes/min) will give the desired spot movement:",
        "365": " 805.97 388.49 255.92 190.81 ",
        "366": "To test your knowledge of this concept, try solving for all possible flashing rates so that the spot will move clockwise at a rate of one revolution every 9 flashes. Assume the same motor rotation speed of 750 rpm clockwise. What is the maximum flashing rate for this case? Spectrum Interpretation The strobe demo involves the sampling of complex exponentials, so we can present the results in the form of a spectrum diagram rather than using equations, as in the foregoing section. The rotating disk has an analog spectrum given by a single frequency component at cycles/min (see Fig.~\u203b).",
        "367": " Analog spectrum representing the disk spinning clockwise (negative frequency) at rpm. The units for are cycles/min. When the strobe light is applied at a rate of flashes per minute, the resulting spectrum of the discrete-time signal will contain an infinite number of frequency lines at the frequencies for. Equation tells us that there are two steps needed to create the spectrum (Fig.~\u203b) of the discrete-time signal representing the sampled motion:",
        "368": " Compute a normalized motor frequency by dividing by. Then plot a spectrum line at. The minus sign represents the clockwise rotation. Repeat the normalized spectral line for the motor by shifting it by all integer multiples of. This accounts for the aliases of the discrete-time exponential,. Digital spectrum representing the strobed disk spinning clockwise at rpm, but sampled at flashes per minute. The horizontal axis is normalized frequency:. The normalized motor frequency also appears at the aliases, where is an integer. How do we use Fig.~\u203b to explain what we see in the strobe-light experiment? Fig.~\u203b contains an infinite number of spectrum lines, so it offers an infinite number of possibilities for what we will observe. Our visual system, however, must pick one and it is reasonable that we would pick the slowest one. In effect, our human visual system is acting as a D-to-C converter. Thus, in the discrete-time spectrum, only the frequency component closest to counts in D-to-C reconstruction, so the strobed signal appears to be rotating at that lowest normalized frequency. However, one last conversion must be made to give the perceived analog rotation rate in rpm. The discrete-time frequency must be converted back to analog frequency. In Fig.~\u203b, the alias for is the one closest to zero frequency, so the corresponding analog frequency is This may seem like a roundabout way to say that the observed rotation rate of the spot differs from by an integer multiple of the sampling rate, but it does provide a systematic method to draw the graphical picture of the relative frequency locations. Finally, the spectrum picture makes it easy to identify the lowest discrete-time frequency as the one that is \u201creconstructed.\u201d We will say more about why this is true in Section~\u203b.",
        "369": "The case where the sampling rate is variable and is fixed is a bit harder to solve, but this is the actual situation for our strobed disk experiment. Nonetheless, a graphical approach is still possible because the desired spot frequency defines a line in the discrete-time spectrum, say. This line is the one closest to the origin, so we must add an integer multiple to to match the normalized motor rotation frequency. This equation can be solved for the flashing rate, but the final answer depends on the integer, which predicts, by the way, that there are many answers for, as we already saw in. The result is ",
        "370": "Discrete-to-Continuous Conversion The purpose of the ideal discrete-to-continuous (D-to-C) converter is to interpolate a smooth continuous-time function through the input samples. Thus, in the special case when, and if, then according to the sampling theorem, the converter should produce For sampled sinusoidal signals only, the ideal D-to-C converter in effect replaces by. On the other hand, if, then we know that aliasing or folding distortion has occurred, and the ideal D-to-C converter will reconstruct a cosine wave with frequency equal to the alias frequency that is less than. Interpolation with Pulses How does the D-to-C converter work? In this section, we explain how the D-to-C converter does interpolation, and then describe a practical system that is nearly the same as the ideal D-to-C converter. These actual hardware systems, called digital-to-analog (D-to-A) converters, approximate the behavior of the ideal D-to-C system.",
        "371": "A general formula that describes a broad class of D-to-C converters is given by the equation where is the characteristic pulse shape of the converter. Equation states that the output signal is produced by adding together many pulses, each shifted in time. In other words, at each sample time, a pulse is emitted with an amplitude proportional to the sample value corresponding to that time instant. Note that all the pulses have a common waveshape specified by. If the pulse has duration greater than or equal to, then the gaps between samples will be filled by adding overlapped pulses.",
        "372": "Obviously, the important issue is the choice of the pulse waveform. Unfortunately, we do not yet have the mathematical tools required to derive the optimal pulse shape required for exact reconstruction of a waveform from its samples as is predicted to be possible in the Shannon sampling theorem. ",
        "373": " Demo: Reconstruction via D-to-A This optimal pulse shape can be constructed during a mathematical proof of the sampling theorem. Nonetheless, we will demonstrate the plausibility of by considering some simple (suboptimal) examples. Figure~\u203b shows four possible pulse waveforms for D-to-C conversion when ~Hz.",
        "374": " Four different pulses for D-to-C conversion. The sampling period is, i.e., Hz. Note that the duration of the pulses is one, two, or four times, or infinite for the ideal pulse. Zero-Order Hold Interpolation The simplest pulse shape that we might propose is a symmetric square pulse of the form This pulse is plotted in the upper left panel of Fig.~\u203b.",
        "375": "From Fig.~\u203b, we see that the total width of the square pulse is ms and that its amplitude is 1. Therefore, each term in the sum will create a flat region of amplitude centered on. This is shown in the top part of Fig.~\u203b, which shows the original 83 Hz cosine wave (solid gray line), its samples taken at a sampling rate of 200 samples/sec, and the individual shifted and scaled pulses, (dashed line). The sum of all the shifted and scaled pulses will be a \u201cstairstep\u201d waveform, as shown in the bottom panel of Fig.~\u203b, where the gray curve is the original cosine wave, and the heavy orange curve shows the reconstructed waveform using the square pulse.",
        "376": " D-to-C conversion using a square pulse. Flat regions are characteristic of the zero-order hold. Demo: Reconstruction via D-to-C The space between samples has indeed been filled with a continuous-time waveform; however, it is clear from the lower part of Fig.~\u203b that the reconstructed waveform for the square pulse is a poor approximation of the original cosine wave. Thus, using a square pulse in is D-to-C conversion, but not ideal D-to-C conversion. Even so, this is a useful model, since many physically realizable digital-to-analog (D-to-A) converters produce outputs that look exactly like this! Linear Interpolation The triangular pulse plotted in the upper right panel of Fig.~\u203b is defined as a pulse consisting of the first-order polynomial (straight line) segments Figure~\u203b (top) again shows the original 83-Hz cosine wave (solid gray curve) and its samples, together with the scaled pulses (dashed orange) for the triangular pulse shape. In this case, the output of the D-to-C converter at any time is the sum of the scaled pulses that overlap at that time instant. Since the duration of the pulse is, and they are shifted by multiples of, no more than two pulses can overlap at any given time. The resulting output is shown as the solid orange curve in the bottom panel of Fig.~\u203b. Note that the result is that the samples are connected by straight lines. Also note that the values at are exactly correct. This is because the triangular pulses are zero at, and only one scaled pulse (with value at ) contributes to the value at. In this case, we say that the continuous-time waveform has been obtained by linear interpolation between the samples. This is a smoother and better approximation of the original waveform (thin gray curve), but there is still significant reconstruction error for this signal.",
        "377": " D-to-C conversion using a triangular pulse, or linear interpolation. Cubic Spline Interpolation A third pulse shape is shown in the lower left panel of Fig.~\u203b. This pulse consists of four cubic spline (third-order polynomial) segments. Note that it has a duration that is twice that of the triangular pulse and four times that of the square pulse. Also, note that this pulse has zeros at the key locations: and its derivative is smooth at the sample locations. The reconstruction with the cubic-spline pulses is shown in Fig.~\u203b. The top panel of Fig.~\u203b shows the original 83-Hz waveform (solid gray), its samples ( samples/sec), and the shifted and scaled pulses (dashed orange). Now note that for values of between two adjacent sample times, four pulses overlap and must be added together in the sum. This means that the reconstructed signal at a particular time instant, which is the sum of these overlapping pulses, depends on the two samples preceding and the two samples following that time instant. The lower panel of Fig.~\u203b shows the original waveform (light gray), the samples, and the output of the D-to-C converter with \u201ccubic-spline pulse\u201d interpolation (solid orange curve). Now we see that the approximation is getting smoother and better, but it is still far from perfect. We will see that this is because the sampling is only 2.4 times the highest frequency.",
        "378": " D-to-C conversion using a cubic-spline pulse. Over-Sampling Aids Interpolation From the previous three examples, it seems that one way to make a smooth reconstruction of a waveform such as a sinusoid is to use a pulse that is smooth and has a long duration. Then the interpolation formula will involve several neighboring samples in the computation of each output value. However, the sampling rate is another important factor. If the original waveform does not vary much over the duration of, then we will also obtain a good reconstruction. One way to ensure this is by over-sampling, i.e., using a sampling rate that is much greater than the frequency of the cosine wave. This is illustrated in Figs.~\u203b, \u203b, and \u203b, where the frequency of the cosine wave is still Hz, but the sampling frequency has been raised to samples/sec. Now the reconstruction pulses are the same shape as in Fig.~\u203b, but they are much shorter, since msec instead of 5~msec. The signal changes much less over the duration of a single pulse, so the waveform appears \u201csmoother\u201d and is much easier to reconstruct accurately using only a few samples. Note that even for the case of the square pulse (Fig.~\u203b) the reconstruction is better, but still discontinuous; the triangular pulse (Fig.~\u203b) gives an excellent approximation; and the cubic-spline pulse gives a reconstruction that is indistinguishable from the original signal on the plotting scale of Fig.~\u203b.",
        "379": " Lab: Reconstruction Movies D-to-C conversion using a square pulse. The original 83-Hz sinusoid was over-sampled at samples/sec. D-to-C conversion using a square pulse. The original 83-Hz sinusoid was over-sampled at samples/sec. Figures~\u203b, \u203b, and \u203b show that over-sampling can make it easier to reconstruct a waveform from its samples. Indeed, this is why audio CD players have over-sampled D-to-A converters! In the CD case, or over-sampling is used to increase the sampling rate before sending the samples to the D-to-A converter. This makes it possible to use a simpler (and therefore less expensive) D-to-A converter to reconstruct an accurate output from a CD player. Ideal Bandlimited Interpolation So far in this section, we have demonstrated the basic principles of discrete-to-continuous conversion. We have shown that this process can be approximated very well by a sum of pulses. One question remains: What is the pulse shape that gives \u201cideal D-to-C conversion\u201d? In Chapter~\u203b, we will show that it is given by the following equation: The infinite length of this \u201cpulse\u201d implies that to reconstruct a signal at time exactly from its samples requires all the samples, not just those around that time. The lower right part of Fig.~\u203b shows this pulse over the interval. It decays outside this interval, but never does reach and stay at zero. Since the pulse has zeros at multiples of, this type of reconstruction is still an interpolation process, called bandlimited interpolation. Using this pulse to reconstruct from samples of a cosine wave will always produce a cosine wave exactly. If the sampling rate satisfies the conditions of the sampling theorem, the reconstructed cosine wave will be identical to the original signal that was sampled. If aliasing occurred in sampling, the ideal D-to-C converter reconstructs a cosine wave with the alias frequency that is less than.",
        "380": " D-to-C conversion using a cubic-spline pulse at samples/sec. ",
        "381": "The Sampling Theorem This chapter has discussed the issues that arise in sampling continuous-time signals. Using the example of the continuous-time cosine signal, we have illustrated the phenomenon of aliasing, and we have shown how the original continuous-time cosine signal can be reconstructed by interpolation. All of the discussion of this chapter has been aimed at the goal of establishing confidence in the Shannon sampling theorem, which, because of its central importance to our study of digital signal processing, is repeated here. ",
        "382": " Shannon Sampling Theorem A continuous-time signal with frequencies no higher than can be reconstructed exactly from its samples, if the samples are taken at a rate that is greater than. A block diagram representation of the sampling theorem is shown in Fig.~\u203b in terms of the ideal C-to-D and D-to-C converters that we have defined in this chapter. The sampling theorem states that for the sampling and reconstruction system shown in Fig.~\u203b, if the input is composed of sinusoidal signals limited to the set of frequencies in the range, then the reconstructed signal is equal to the original signal that was sampled; i.e.,.",
        "383": " Sampling and reconstruction system. Graphical User Interface (GUI) for con2dis which illustrates aliasing with simultaneous diagrams in both the time and frequency domains. The user can change the frequencies moving the spectrum lines with a mouse or by entering values in the edit boxes. Signals composed of sinusoids such that all frequencies are limited to a \u201cband of frequencies\u201d of the form are called bandlimited signals.. Such signals could be represented as where each of the individual signals is of the form and it is assumed that the frequencies are ordered so that and. As we have seen in Chapter \u203b, such an additive combination of cosine signals can produce an infinite variety of both periodic and nonperiodic signal waveforms. If we sample the signal represented by and, we obtain where with. That is, if we sample a sum of continuous-time cosines, we obtain a sum of sampled cosines each of which would be subject to aliasing if the sampling rate is not high enough.",
        "384": "The final step in the process of sampling followed by reconstruction in Fig.~\u203b is discrete-to-continuous conversion by interpolation with where for perfect reconstruction, would be given by. This expression for the reconstructed output is a linear operation on the samples. This means that the total output will consist of the sum of the outputs due to each of the different components. We can see this by substituting into as in Now since each individual sinusoid is assumed to satisfy the conditions of the sampling theorem, it follows that the D-to-C converter will reconstruct each component perfectly, and therefore we conclude that Thus, we have shown that the Shannon sampling theorem applies to any signal that can be represented as a bandlimited sum of sinusoids, and since it can be shown that most real-world signals can be represented as bandlimited signals, it follows that the sampling theorem is a very general guide to sampling all kinds of signals. In Chapter~\u203b, we will demonstrate this more convincingly using the Fourier transform.",
        "385": "Summary and Links This chapter introduced the concept of sampling and the companion operation of reconstruction. With sampling, the possibility of aliasing always exists, and we have created the strobe demo to illustrate that concept in a direct and intuitive way.",
        "386": " Demo: Links to many demos None of the labs can be associated directly with this chapter. Some aspects of sampling have already been used in the music synthesis labs that are associated with Chapter~\u203b, because the sounds must be produced by making their samples in a computer before playing them out through a D-to-A converter.",
        "387": "The CD-ROM also contains many demonstrations of sampling and aliasing: Strobe movie filmed using the natural strobing of a video camera at 30 frames per second. Synthetic strobe demos produced as MATLAB movies. Reconstruction movies that show the interpolation process for different pulse shapes and different sampling rates. A sampling and aliasing MATLAB GUI called con2dis, shown in Fig.~\u203b. Aliasing and folding movies that show how the spectrum changes for different sampling rates. Again, the CD-ROM contains a large number of solved homework problems available for review and practice.",
        "388": " Note: Hundreds of Solved Problems ",
        "389": "CHAPTER 5 FIR Filters Up to this point, we have focused our attention on signals and their mathematical representations. In this chapter, we begin to emphasize systems or filters. Strictly speaking, a filter is a system that is designed to remove some component or modify some characteristic of a signal, but often the two terms are used interchangeably. In this chapter, we introduce the class of FIR (finite impulse response) systems, or, as we will often refer to them, FIR filters. These filters are systems for which each output sample is the sum of a finite number of weighted samples of the input sequence. We will define the basic input\u2013output structure of the FIR filter as a time-domain computation based on a feed-forward difference equation. The unit impulse response of the filter will be defined and shown to characterize the filter. The general concepts of linearity and time invariance will also be presented. These properties characterize a wide class of filters that are exceedingly important in both the continuous-time and the discrete-time cases.",
        "390": "Our purpose in this chapter is to introduce the basic ideas of discrete-time systems and to provide a starting point for further study. The analysis of both discrete-time and continuous-time systems is a rich subject of study that is necessarily based on mathematical representations and manipulations. The systems that we introduce in this chapter are the simplest to analyze. The remainder of the text is concerned with extending the ideas of this chapter to other classes of systems and with developing tools for the analysis of other systems.",
        "391": "Discrete-Time Systems A discrete-time system is a computational process for transforming one sequence, called the input signal, into another sequence called the output signal. As we have already mentioned, systems are often depicted by block diagrams such as the one in Fig.~\u203b. In Chapter~\u203b, we used similar block diagrams to represent the operations of sampling and reconstruction. In the case of sampling, the input signal is a continuous-time signal and the output is a discrete-time signal, while for reconstruction the opposite is true. Now we want to begin to study discrete-time systems where the input and output are discrete-time signals. Such systems are very interesting because they can be implemented with digital computation and because they can be designed so as to modify signals in many useful ways.",
        "392": " Block-diagram representation of a discrete-time system. In general, we represent the operation of a system by the notation which suggests that the output sequence is related to the input sequence by a process that can be described mathematically by an operator. Since a discrete-time signal is a sequence of numbers, such operators can be described by giving a formula for computing the values of the output sequence from the values of the input sequence. For example, the relation defines a system for which the output sequence values are the square of the corresponding input sequence values. A more complicated example would be the following system definition: In this case, the output depends on three consecutive input values. Since infinite possibilities exist for defining discrete-time systems, it is necessary to limit the range of possibilities by placing some restrictions on the properties of the systems that we study. Therefore, we will begin our study of discrete-time systems in this chapter by introducing a very important class of discrete-time systems called FIR filters. Specifically, we will discuss the representation, implementation, and analysis of discrete-time FIR systems, and illustrate how such systems can be used to modify signals.",
        "393": "The Running-Average Filter A simple but useful transformation of a discrete-time signal is to compute a moving average or running average of two or more consecutive numbers of the sequence, thereby forming a new sequence of the average values. The FIR filter is a generalization of the idea of a running average. Averaging is commonly used whenever data fluctuate and must be smoothed prior to interpretation. For example, stock-market prices fluctuate noticeably from day to day, or hour to hour. Therefore, one might take an average of the stock price over several days before looking for any trend. Another everyday example concerns credit-card balances where interest is charged on the average daily balance.",
        "394": "In order to motivate the general definition of the class of FIR systems, let us consider the simple running average as an example of a system that processes an input sequence to produce an output sequence. To be specific, consider a 3-point averaging method; i.e., each value of the output sequence is the sum of three consecutive input sequence values divided by three. If we apply this algorithm to the triangularly shaped sequence shown in Fig.~\u203b, we can compute a new sequence called, which is the output of the averaging operator. The sequence in Fig.~\u203b is an example of a finite-length signal. The support of such a sequence is the set of values over which the sequence is nonzero; in this case, the support of the sequence is the finite interval. A 3-point average of the values gives the answer. This result defines one of the output values. The next output value is obtained by averaging which, yields a value of 14/3. Before going any further, we should decide on the output indexing. For example, the values 4 and 14/3 could be assigned to and, but this is only one of many possibilities. With this indexing, the equations for computing the output from the input are Finite-length input signal,. which generalizes to the following input\u2013output equation: The equation given in is called a difference equation. It is a complete description of the FIR system because we can use to compute the entire output signal for all index values. For the input of Fig.~\u203b, the result is the signal tabulated as follows:",
        "395": " 0 1 2 3 4 5 0 0 0 2 4 6 4 2 0 4 0 Note that the values in orange type in the row are the numbers involved in the computation of. Also note that outside of the finite interval ; i.e., the output also has finite support. The output sequence is also plotted in Fig.~\u203b. Observe that the output sequence is longer (has more nonzero values) than the input sequence, and that the output appears to be a somewhat rounded-off version of the input; i.e. it is smoother than the input sequence. This behavior is characteristic of the running-average FIR filter.",
        "396": " Output of running average filter,. The choice of the output indexing is arbitrary, but it does matter when speaking about properties of the filter. For example, the filter defined in has the property that its output starts (becomes nonzero) before the input starts. This would certainly be undesirable if the input signal values came directly from an A-to-D converter, as is common in audio signal-processing applications. In this case, would stand for time, and we can interpret in as the computation of the present value of the output based on three input values. Since these inputs are indexed as,, and, two of them are \u201cin the future.\u201d In general, values from either the past or the future or both may be used in the computation, as shown in Fig.~\u203b. In all cases of a 3-point running average, a sliding window of three samples determines which three samples are used in the computation of.",
        "397": " The running-average filter calculation at the present time uses values within a sliding window. Gray shading indicates the past ); orange shading, the future. Here, the sliding window encompasses values from both the future and the past. A filter that uses only the present and past values of the input is called a causal filter, implying that the cause does not precede the corresponding effect. Therefore, a filter that uses future values of the input is called noncausal. Noncausal systems cannot be implemented in a real-time application because the input is not yet available when the output has to be computed. In other cases, where stored data blocks are manipulated inside a computer, the issue of causality is not crucial.",
        "398": "An alternative output indexing scheme can produce a 3-point averaging filter that is causal. In this case, the output value is the average of inputs at (the present), (one sample previous), and (two samples previous). The difference equation for this filter is The form given in is a causal running averager, or it may well be called a backward average. Using the difference equation, we can make a table of all output values over the range. (Notice that now the orange-colored values of are used in this case to compute instead of.) The resulting signal has the same values as before, but its support is now the index interval. Observe that the output of the causal filter is simply a shifted version of the output of the previous noncausal filter. This filter is causal because the output depends on only the present and two previous (i.e., past) values of the input. Therefore, the output does not change from zero before the input changes from zero.",
        "399": " 0 1 2 3 4 5 0 0 0 2 4 6 4 2 0 0 0 4 0 Determine the output of a centralized averager for the input in Fig.~\u203b. Is this filter causal or noncausal? What is the support of the output for this input? How would the plot of the output compare to Fig.~\u203b?",
        "400": "The General FIR Filter Note that is a special case of the general difference equation That is, when and for, reduces to the causal running average of. If the coefficients are not all the same, then we might say that defines a weighted running average of samples. It is clear from that the computation of involves the samples for ; i.e.,,,, etc. Since the filter in does not involve future values of the input, the system is causal, and, therefore, the output cannot start before the input becomes nonzero. Figure \u203b shows that the causal FIR filter uses and the past points to compute the output. Figure \u203b also shows that if the input has finite support ( ), there will be an interval of samples at the beginning, where the computation will involve fewer than nonzero samples as the sliding window of the filter engages with the input, and an interval of samples at the end where the sliding window of the filter disengages from the input sequence. It also can be seen from Fig.~\u203b that the output sequence can be as much as samples longer than the input sequence.",
        "401": " Operation of the order causal FIR filter showing various positions of the sliding window of points under which the weighted average is calculated. When the input signal is also finite length ( points), the sliding window will run onto and off of the input data, so the resulting output signal will also have finite length. EXAMPLE:\u00a0 FIR Filter Coefficients The FIR filter is completely defined once the set of filter coefficients is known. For example, if the of a causal filter are then we have a length-4 filter with, and expands into a 4-point difference equation: The parameter is the order of the FIR filter. The number of filter coefficients is also called the filter length. The length is one greater than the order; i.e.,. This terminology will make more sense after we have introduced the -transform in Chapter \u203b.",
        "402": "Compute the output for the length-4 filter whose coefficients are. Use the input signal given in Fig.~\u203b. Verify that the answers tabulated here are correct, then fill in the missing values.",
        "403": " 0 1 2 3 4 5 6 7 8 0 2 4 6 4 2 0 0 0 0 0 0 6 10 18 8 2 0 0 An Illustration of FIR Filtering To illustrate some of the things that we have learned so far, and to show how FIR filters can modify sequences, consider a signal This signal is shown as the upper plot in Fig.~\u203b. We often have real signals of this form; i.e., a component that is the signal of interest (in this case, it may be the slowly varying exponential component ) plus another component that is not of interest. Indeed, the second component is often considered to be noise that interferes with observation of the desired signal. In this case, we will consider the sinusoidal component to be noise that we wish to remove. The solid, exponentially growing curve shown in each of the plots in Fig.~\u203b simply connects the sample values of the desired signal by straight lines for reference in the other two plots.",
        "404": " Illustration of running-average filtering. Now suppose that is the input to a causal 3-point running averager, i.e., In this case, and all the coefficients are equal to 1/3. The output of this filter is shown in the middle plot in Fig.~\u203b. We can notice several things about these plots.",
        "405": " Observe that the input sequence is zero prior to, and from it follows that the output must be zero for. The output becomes nonzero at, and the shaded interval of length samples at the beginning of the nonzero part of the output sequence is the interval where the 3-point averager \u201cruns onto\u201d the input sequence. For, the input samples within the 3-point averaging window are all nonzero. There is another shaded interval of length samples at the end (after sample 40), where the filter window \u201cruns off of\u201d the input sequence. Observe that the size of the sinusoidal component has been reduced, but that the component is not eliminated by the filter. The solid line showing the values of the exponential component has been shifted to the right by sample to account for the shift introduced by the causal filter. Clearly, the 3-point running averager has removed some of the fluctuations in the input signal, but we have not recovered the desired component. Intuitively, we might think that averaging over a longer interval might produce better results. The lower plot in Fig.~\u203b shows the output of a 7-point running averager as defined by In this case, since and all the coefficients are equal to 1/7, we observe the following:",
        "406": " The shaded regions at the beginning and end of the output are now samples long. Now the size of the sinusoidal component is greatly reduced relative to the input sinusoid, and the exponential component is very close to the exponential component of the input (after a shift of samples). What can we conclude from this example? First, it appears that FIR filtering can modify signals in ways that may be useful. Second, the length of the averaging interval seems to have a big effect on the resulting output. Third, the running-average filters appear to introduce a shift equal to samples. All of these observations can be shown to apply to more general FIR filters defined by. However, before we can fully appreciate the details of this example, we must explore the properties of FIR filters in greater detail. We will gain full appreciation of this example only upon the completion of Chapter \u203b.",
        "407": "The Unit Impulse Response In this section, we will introduce three new ideas: the unit impulse sequence, the unit impulse response, and the convolution sum. We will show that the impulse response provides a complete characterization of the filter, because the convolution sum gives a formula for computing the output from the input when the unit impulse response is known.",
        "408": " Unit Impulse Sequence The unit impulse is perhaps the simplest sequence because it has only one nonzero value, which occurs at. The mathematical notation is that of the Kronecker delta function ",
        "409": " It is tabulated in the second row of this table: 0 1 2 3 4 5 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 A shifted impulse such as is nonzero when its argument is zero, i.e.,, or equivalently. The third row of the table gives the values of the shifted impulse, and Fig.~\u203b shows a plot of that sequence.",
        "410": " Shifted impulse sequence,. The shifted impulse is a concept that is very useful in representing signals and systems. Consider, for example, the signal",
        "411": " To interpret, we must observe that the appropriate definition of multiplying a sequence by a number is to multiply each value of the sequence by that number; likewise, adding two or more sequences is defined as adding the sequence values at corresponding positions (times). The following table shows the individual sequences in and their sum:",
        "412": " 0 1 2 3 4 5 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Clearly, is a compact representation of the signal in Fig.~\u203b. Indeed, any sequence can be represented in this way. The equation is true if ranges over all the nonzero values of the sequence. Equation states the obvious: The sequence is formed by using scaled shifted impulses to place samples of the right size at the right positions.",
        "413": " Unit Impulse Response Sequence When the input to the FIR filter is a unit impulse sequence,, the output is, by definition, the unit impulse response, which we will denote by. This is depicted in the block diagram of Fig.~\u203b. Substituting in gives the output : Block diagram showing definition of impulse response. As we have observed, the sum evaluates to a single term for each value of because each is nonzero only when or. In the tabulated form, the impulse response is",
        "414": " 0 1 2 3 0 1 0 0 0 0 0 0 0 0 0 0 In other words, the impulse response of the FIR filter is simply the sequence of difference equation coefficients. Since for and for, the length of the impulse response sequence is finite. This is why the system is called a finite impulse response, (FIR) system. Figure~\u203b illustrates a plot of the impulse response for the case of the causal 3-point running-average filter. Impulse response of 3-point running average filter,. ",
        "415": "Determine and plot the impulse response of the FIR system The Unit-Delay System One important system is the operator that performs a delay or shift by an amount When, the system is called a unit delay. The output of the unit delay is particularly easy to visualize. In a plot, the values of are moved to the right by one time interval. For example, takes on the value of, takes on the value of, is, and so on.",
        "416": "The delay system is actually the simplest of FIR filters; it has only one nonzero coefficient. For example, a system that produces a delay of 2 has filter coefficients. The order of this FIR filter is, and its difference equation is finite-length input signal, for input of Fig.~\u203b",
        "417": "Figure~\u203b shows the output of the delay system with a delay of 2 for the input of Fig.~\u203b on p.~. The impulse response of the delay system is obtained by substituting for in. For the delay-by-2 case, This impulse response is the signal plotted previously in Fig.~\u203b.",
        "418": "Convolution and FIR Filters A general expression for the FIR filters output can be derived in terms of the impulse response. Since the filter coefficients in are identical to the impulse response values, we can replace in by to obtain When the relation between the input and the output of the FIR filter is expressed in terms of the input and the impulse response, as in, it is called a finite convolution sum, and we say that the output is obtained by convolving the sequences and.",
        "419": "Computing the Output of a Convolution The method of tabulating values for the output of an FIR filter works for short signals, but lacks the generality needed in more complicated problems. However, there is a simple interpretation of that leads to a better algorithm for doing convolution. This algorithm can be implemented using the tableau in Fig.~\u203b that tracks the relative position of the signal values. The example in Fig.~\u203b shows how to convolve with. First of all, we write out the signals and on separate rows. Then we use a method similar to \u201csynthetic polynomial multiplication\u201d to form the output as the sum of shifted rows. Each shifted row is produced by multiplying the row by one of the values and shifting the result to the right so that it lines up with the position. The final answer is obtained by summing down the columns. ",
        "420": " Numerical convolution of finite-length signals via synthetic polynomial multiplication 0 1 2 3 4 5 6 7 0 2 4 6 4 2 0 0 0 0 0 0 6 12 18 12 6 0 0 0 0 0 0 0 0 0 0 0 0 4 8 12 8 4 0 0 0 0 0 0 2 4 6 4 2 0 0 6 10 18 16 18 12 8 2 0 The justification of this algorithm for evaluating the convolution sum comes from writing out the sum in as A term such as is the signal with its values shifted two places to the right. The multiplier scales the shifted signal to produce the contribution, which is the orange row in the table.",
        "421": "Use the \u201csynthetic multiplication\u201d convolution algorithm to compute the output for the length 4 filter whose coefficients are. Use the input signal given in Fig.~\u203b on p.~. Later in this chapter, we will prove that convolution is the fundamental input\u2013output algorithm for a large class of very useful filters that includes FIR filters as a special case. We will show that a general form of convolution that also applies to infinite-length signals is This convolution sum has infinite limits, but reduces to when for and.",
        "422": " Convolution in MATLAB In MATLAB, FIR systems can be implemented by using the conv( ) function. For example, the following MATLAB statements xx = sin(0.07*pi*(0:50)); hh = ones(11,1)/11; yy = conv(hh, xx); will evaluate the convolution of the 11-point sequence hh with the 51-point sinusoidal sequence xx. The particular choice for the MATLAB vector hh is actually the impulse response of an 11-point running average system: That is, all 11 filter coefficients are the same and equal to 1/11. In MATLAB, we can compute only the convolution of finite-length signals. Determine the length of the output sequence computed by the MATLAB convolution above. We have already hinted that this operation called convolution is equivalent to polynomial multiplication. In Section \u203b, we will prove that this correspondence is true. At this point, we note that in MATLAB there is no function for multiplying polynomials. Instead, we must know that convolution is equivalent to polynomial multiplication. Then we can represent the polynomials by sequences of their coefficients and use the conv function to convolve them, thereby doing polynomial multiplication.",
        "423": " Demo: Discrete Convolution GUI: DCONVDEMO Use MATLAB to compute the following product of polynomials: ",
        "424": "Implementation of FIR Filters Recall that the general definition of an FIR filter is We can see that to use to compute the output of the FIR filter, we need the following: (1) a means for multiplying delayed-input signal values by the filter coefficients; (2) a means for adding the scaled sequence values; and (3) a means for obtaining delayed versions of the input sequence. We will find it useful to represent the operations of as a block diagram. Such representations will lead to new insights about the properties of the system and about alternative ways to implement the system.",
        "425": " Lab: Lab #6 Building Blocks The basic building-block systems we need are the multiplier, the adder, and the unit-delay operator as depicted in Fig.~\u203b.",
        "426": " Building blocks for making any LTI discrete-time system: (a)~multiplier, ; (b)~adder, ; and (c)~unit-delay,. ",
        "427": " Multiplier The first elementary system performs multiplication of a signal by a constant (see Fig.~\u203b(a)). The output signal is given by the rule where the coefficient is a constant. This system can be the hardware multiplier unit in a computer. For a DSP microprocessor, the speed of this multiplier is one of the fundamental limits on the throughput of the digital filtering process. In applications such as image convolution, billions of multiplications per second may have to be performed to implement a good filter, so quite a bit of engineering work has been directed at designing fast multipliers for DSP applications. Furthermore, since many filters require the same sequence of multiplications over and over, pipelining the multiplier also results in a dramatic speed-up of the filtering process.",
        "428": "Notice, by the way, that the simple multiplier is also an FIR filter, with, and in. The impulse response of the multiplier system is simply.",
        "429": " Adder The second elementary system in Fig.~\u203b(b) performs the addition of two signals. This is a different sort of system because it has two inputs and one output. In hardware, the adder is simply the hardware adder unit in the computer. Since many DSP operations require a multiplication followed immediately by an addition, it is common in DSP microprocessors to build a special multiply-accumulate unit, often called a \u201cMADD\u201d or \u201cMAC\u201d unit.",
        "430": "Notice that the adder is a pointwise combination of the values of the two input sequences. It is not an FIR filter, because it has more than one input; however, it is a crucial building block of FIR filters. With many inputs, the adder could be drawn as a multi-input adder, but, in digital hardware, the additions are typically done two inputs at a time.",
        "431": " Unit Delay The third elementary system performs a delay by one unit of time. It is represented by a block diagram, as in Fig.~\u203b(c). In the case of discrete-time filters, the time dimension is indexed by integers, so this delay is by one \u201ccount\u201d of the system clock. The hardware implementation of the unit delay is actually performed by acquiring a sample value, storing it in memory for one clock cycle, and then releasing it to the output. The delays by more than one time unit that are needed to implement can be implemented (from the block-diagram point of view) by cascading several unit delays in a row. Therefore, an -unit delay requires memory cells configured as a shift register, which can be implemented as a circular buffer in computer memory.",
        "432": "Block Diagrams In order to create a graphical representation that is useful for hardware structures, we use block-diagram notation, which defines the interconnection of the three basic building blocks to make more complex structures. In such a directed graph, the nodes (i.e., junction points) are either summing nodes, splitting nodes, or input\u2013output nodes. The connections between nodes are either delay branches or multiplier branches.",
        "433": " Block-diagram structure for the third-order FIR filter. ",
        "434": "Figure~\u203b shows the general block diagram for a third-order FIR digital filter. This structure shows why the FIR filter is also called a feed-forward difference equation, since all paths lead forward from the input to the output. There are no closed-loop paths in the block diagram. In Chapter \u203b we will discuss filters with feedback, where both input and past output values are involved in the computation of the output.",
        "435": "Strictly speaking, the structure of Fig.~\u203b is a block-diagram representation of the equation which clearly expands into an equation that can be represented as in. The input signal is delayed by the cascaded unit delays, each delayed signal is multiplied by a filter coefficient, and the products are accumulated to form the sum. Thus, it is easy to see that there is a one-to-one correspondence between the block diagram and the difference equation of the FIR filter, because both are defined by the filter coefficients. A useful skill is to start with one representation and then produce the other. The structure in Fig.~\u203b displays a regularity that makes it simple to define longer filters; the number of cascaded delay elements is increased to, and then the filter coefficients are substituted into the diagram. This standard structure is called the direct form. Going from the block diagram back to the difference equation is just as easy, as long as we stick to direct form. Here is a simple exercise to make the correspondence.",
        "436": "Determine the difference equation for the block diagram of Fig.~\u203b. Third-order FIR filter in direct form. ",
        "437": " Other Block Diagrams Many block diagrams will implement the same FIR filter, in the sense that the external behavior from input to output will be the same. Direct form is just one possibility. Other block diagrams would represent a different internal computation, or a different order of computation. In some cases, the internal multipliers might use different coefficients. After we have studied the -transform in Chapters~\u203b and~\u203b, we will have the tools to produce many different implementations.",
        "438": "When faced with an arbitrary block diagram, the following four-step procedure may be used to derive the difference equation from the block diagram.",
        "439": " Give a unique signal name to the input of each unit-delay block. Notice that the output of a unit delay can be written in terms of its input. At each summing node of the structure, write a signal equation. Use the signal names introduced in steps~1 and~2. At this point, you will have several equations involving,, and the internal signal names. These can be reduced to one equation involving only and by eliminating variables, as is done with simultaneous equations. Let us try this procedure on the simple but useful example shown in Fig.~\u203b. First of all, we observe that the internal signal variables have been defined in Fig.~\u203b as the inputs to the three delays. Then the delay outputs are, from top to bottom,,, and. We also notice that is a scaled version of the input. In addition, we write the three equations at the output of the three summing nodes: Now we have four equations in five \u201cunknowns.\u201d To show that this set of equations is equivalent to the direct form, we can eliminate the by combining the equations in a pairwise fashion. Thus, we have derived the same difference equation as before, so this new structure must be a different way of computing the same thing. In fact, it is widely used and is called the transposed form for the FIR filter.",
        "440": " Transposed form block diagram structure for the third-order FIR filter. ",
        "441": "Explain why Fig.~\u203b is called the transposed form of the direct form shown in Fig.~\u203b. Internal Hardware Details A block diagram shows dependencies among the different signal variables. Therefore, different block diagrams that implement the same input\u2013output operation may have dramatically different characteristics as far as their internal behavior goes. Several issues come to mind:",
        "442": " The order of computation is specified by the block diagram. In high-speed applications where parallelism or pipelining must be exploited, dependencies in the block diagram represent constraints on the computation. Partitioning a filter for a VLSI chip would be done (at the highest level) in terms of the block diagram. Likewise, algorithms that must be mapped onto special DSP architectures can be managed by using the block diagram with special compilers that translate the block diagram into optimized code for the DSP chip. Finite word-length effects are important if the filter is constructed using fixed-point arithmetic. In this case, round-off error and overflow are important real-world problems that depend on the internal order of computation. Now that we know something about convolution and the implementation of FIR filters, it is time to consider discrete-time systems in a more general way. In the next section, we will show that FIR filters are a special case of the general class of linear time-invariant systems. Much of what we have learned about FIR filters will apply to this more general class of systems.",
        "443": "Linear Time-Invariant (LTI) Systems In this section, we discuss two general properties of systems. These properties, linearity and time invariance, lead to simplifications of mathematical analysis and greater insight and understanding of system behavior. To facilitate the discussion of these properties, it is useful to recall the block-diagram representation of a general discrete-time system shown in Fig.~\u203b on p.~. This block diagram depicts a transformation of the input signal into an output signal. It will also be useful to introduce the notation to represent this transformation. In a specific case, the system is defined by giving a formula or algorithm for computing all values of the output sequence from the values of the input sequence. A specific example is the square-law system defined by the rule: Another example is, which defines the general delay system; still another is, which defines the general FIR filter and includes the delay system as a special case. We will see that these FIR filters are both linear and time-invariant, while the square-law system is not linear.",
        "444": "Time Invariance A discrete-time system is said to be time-invariant if, when an input is delayed (shifted) by, the output is delayed by the same amount. Using the notation introduced above, we can express this condition as where. The condition must be true for any choice of, the integer that determines the amount of shift.",
        "445": "A block-diagram view of the time-invariance property is given in Fig.~\u203b. In the upper branch, the input is shifted prior to the system; in the lower branch, the output is shifted. Thus, a system can be tested for time-invariance by checking whether or not in Fig.~\u203b.",
        "446": " Testing time-invariance property by checking the interchange of operations. ",
        "447": "Consider the example of the square-law system defined by. If we use the delayed input as the input to the square-law system, we obtain If is the input to the square-law system, then and so the square-law system is time-invariant.",
        "448": "A second simple example is the time-flip system, defined by the equation This system simply reverses the order of (\u201cflips\u201d) the input sequence about the origin. If we delay the input and then reverse the order about the origin, we obtain However, if we first flip the input sequence and then delay it, we obtain a different sequence from ; i.e., since Thus, the time-flip system is not time-invariant.",
        "449": "Test the system defined by the equation to determine whether it is a time-invariant system. Linearity Linear systems have the property that if and, then This mathematical condition must be true for any choice of the constants and. Equation states that if the input consists of a sum of scaled sequences, then the corresponding output is a sum of scaled outputs corresponding to the individual input sequences. A block diagram view of the linearity property is given in Fig.~\u203b, which shows that a system can be tested for the linearity property by checking whether or not.",
        "450": " Testing linearity by checking the interchange of operations. ",
        "451": "The linearity condition in is equivalent to the principle of superposition: If the input is the sum (superposition) of two or more scaled sequences, we can find the output due to each sequence acting alone and then add (superimpose) the separate scaled outputs. Sometimes it is useful to separate into two conditions. Setting we get the condition and using only one scaled input gives Both and must be true in order for to be true.",
        "452": "Reconsider the example of the square-law system defined by. The output in Fig.~\u203b for this system is while Thus,, and the square-law system has been shown not to be linear. Systems that are not linear are called nonlinear systems. ",
        "453": "Show that the time-flip system is a linear system.",
        "454": "The FIR Case FIR Systems described by satisfy both the linearity and time invariance conditions. A mathematical proof of time-invariance can be constructed using the procedure depicted in Fig.~\u203b. If we define the signal to be, then the difference equation relating to in the upper branch of Fig.~\u203b is For comparison, we construct in the lower branch These two expressions are identical, so the output is equal to, and we have proven that the FIR filter is time-invariant.",
        "455": "The linearity condition is simpler to prove. Just substitute into the difference equation and collect terms: Thus, the FIR filter obeys the principle of superposition; therefore, it is a linear system.",
        "456": " A system that satisfies both properties is called a linear time-invariant system, or simply LTI. It should be emphasized that the LTI condition is a general condition. The FIR filter is an example of an LTI system. Not all LTI systems are described by, but all systems described by are LTI systems.",
        "457": "Convolution and LTI Systems Consider an LTI discrete-time system as depicted in Fig.~\u203b. The impulse response of the LTI system is simply the output when the input is the unit impulse sequence. In this section, we will show that the impulse response is a complete characterization for any LTI system, and that convolution is the general formula that allows us to compute the output from the input for any LTI system. In our initial discussion of convolution, we considered only finite-length input sequences and FIR filters. Now, we will give a completely general presentation.",
        "458": "Derivation of the Convolution Sum We begin by recalling from the discussion in Section~\u203b that any signal can be represented as a sum of scaled and shifted impulse signals. Each nonzero sample of the signal multiplies an impulse signal that is shifted to the index of that sample. Specifically, we can write as follows: In the most general case, the range of summation in could be from to. In we have left the range indefinite, realizing that the sum would include all nonzero samples of the input sequence.",
        "459": "A sum of scaled sequences such as is commonly referred to as a \u201clinear combination\u201d or superposition of scaled sequences. Thus, is a representation of the sequence as a linear combination of scaled, shifted impulses. Since LTI systems respond in simple and predictable ways to sums of signals and to shifted signals, this representation is particularly useful for our purpose of deriving a general formula for the output of an LTI system.",
        "460": "Figure~\u203b reminds us that the response to the input is, by definition, the impulse response. Time invariance gives us additional information; the response due to is. In fact, we can write a whole family of input\u2013output pairs as follows: & & h[n] & & h[n-1] & & h[n-2] & & h[n-(-1)] = h[n+1] & & h[n- any integer } Now we are in a position to use linearity, because expresses a general input signal as a linear combination of shifted impulse signals. We can write out a few of the cases: x[0] & x[0] h[n] x[1] & x[1] h[n-1] x[2] & x[2] h[n-2] x[ & x[ h[n- any integer } Then we use superposition to put it all together as The derivation of did not assume that either or was of finite duration, so, in general, we may need infinite limits on the sum. With this modification we obtain solid blue]{ Convolution Sum Formula} } ",
        "461": "y[n] = x[ h[n- } This expression represents the convolution operation in the most general sense, so we have proved that all LTI systems can be represented by a convolution sum. The infinite limits take care of all possibilities, including the special cases where either or both of the sequences are finite in length.",
        "462": "EXAMPLE:\u00a0 FIR convolution For example, if is nonzero only in the interval, then reduces to because the argument must lie in the range, so the range for in is restricted to. Fig.~\u203b shows a MATLAB GUI that is available to generate examples of convolution with a variety of simple signals.",
        "463": " Demo: dconvdemo GUI Graphical User Interface (GUI) for dconvdemo which illustrates the sliding window nature of FIR filtering (i.e., convolution). The user can pick the input signal and the impulse response, and then slide the filter over the input signal to observe how the output is constructed. By making the substitution in, show that can also be expressed in the same form as, Some Properties of LTI Systems The properties of convolution are the properties of LTI systems. Thus, it is of interest to explore these properties and relate them to properties of LTI systems.",
        "464": " Convolution as an Operator An interesting aspect of convolution is its algebraic character as an operation between two signals. The convolution of and is an operation that will be denoted by, i.e., We say that the sequence is convolved with the sequence to produce the output.",
        "465": "The notation is useful because it allows us to think about convolution problems in an operational way. As a simple example, recall that the ideal delay system has impulse response. We know that the output of the ideal delay system is. Therefore, it follows that solid blue]{ with an impulse}} ",
        "466": "x[n] = x[n-n_0] } This is a very important and useful result because states that \u201cto convolve any sequence with an impulse located at, all we need to do is translate the origin of to.\u201d",
        "467": " Commutative Property of Convolution It is relatively easy to prove that convolution is a commutative operation between two sequences. On the third line in this set of equations, the limits on the sum can be swapped without consequence, because a set of numbers can be added in any order. Thus, we have proved that convolution is a commutative operation, i.e., Associative Property of Convolution The associative property which is perhaps less obvious than the commutative property states that, when we are convolving three signals, we can convolve two of them and then convolve that result with the third signal. In the third line, we used the change of variables. This proves that convolution is an associative operation. The implication of this property for cascaded systems is explored in Section~\u203b.",
        "468": "Cascaded LTI Systems In a cascade connection of two systems, the output of the first system is the input to the second system, and the overall output of the cascade system is taken to be the output of the second system. Figure~\u203b shows two LTI systems (LTI~1 and LTI~2) in cascade.",
        "469": " A cascade of two LTI systems. The overall impulse response is the convolution of the two individual impulse responses. LTI systems have the remarkable property that two LTI systems in cascade can be implemented in either order. This property is a direct consequence of the commutative and associative properties of convolution, as demonstrated by the following three equivalent expressions that can be obtained by applying the commutative and associative properties of convolution: Equation is a mathematical statement of the fact that the second system processes the output of the first, which is. Equation shows that the output is the convolution of the input with a new impulse response. This corresponds to Fig.~\u203b(b) with. Equation uses the commutative property of convolution to show that. Applying the associative property leads to, which is the cascade connection in Fig.~\u203b(a). Notice that reordering the LTI systems in cascade gives the same final output, i.e., it is correct to label the outputs of all three systems in Figs.~\u203b and \u203b with the same symbol,, even though the intermediate signals, and, are different.",
        "470": "Another way to show that the order of cascaded LTI systems does not affect the overall system response is to prove that the impulse response of the two cascade systems is the same. In Fig.~\u203b, the impulse input and the corresponding outputs are shown below the arrows. When the input to the first system is an impulse, the output of LTI~1 is its impulse response,, which becomes the input to LTI~2. The output of LTI~2 is, therefore, just the convolution of its input with its impulse response. Therefore, the overall impulse response of Fig.~\u203b is. In the same way, we can easily show that the overall impulse response of the other cascade system in Fig.~\u203b(a) is. Since convolution is commutative, the two cascade systems have the same impulse response which is also the impulse response of the equivalent system. Again, since the overall impulse response is the same for each of the three systems in Figs.~\u203b and \u203b, the output is the same for all three systems for the same input.",
        "471": "EXAMPLE:\u00a0 Impulse Response of Cascade To illustrate the utility of the results that we have obtained for cascaded LTI systems, consider the cascade of two systems defined by The results of this section show that the overall cascade system has impulse response Therefore, to find the overall impulse response we must convolve with. This can be done by using the polynomial multiplication algorithm of Section \u203b. In this case, the computation is as follows:",
        "472": " 0 1 2 3 4 5 6 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 2 3 2 1 0 Therefore, the equivalent impulse response is where is the sequence.",
        "473": "This result means that a system with this impulse response can be implemented either by the single difference equation where is the above sequence, or by the pair of difference equations Example \u203b illustrates an important point. There is a significant difference between and. It can be seen that the implementation in requires a total of only five additions to compute each value of the output sequence, while requires five additions and an additional four multiplications by coefficients that are not equal to unity. On a larger scale (longer filters), such differences in the amount and type of computation can be very important in practical applications of FIR filters. Thus, the existence of alternative equivalent implementations of the same filter is significant.",
        "474": "Example of FIR Filtering We conclude this chapter with an example of the use of FIR filtering on a real signal. An example of sampled data that can be viewed as a signal is the Dow-Jones Industrial Average. The DJIA is a sequence of numbers obtained by averaging the closing prices of a selected number of representative stocks. It has been computed since 1897, and is used in many ways by investors and economists. The entire sequence dating back to 1897 makes up a signal that has positive values and is exponentially growing. In order to obtain an example where fine detail is visible, we have selected the segment from 1950 to 1970 of the weekly closing price average to be our signal. This signal is the one showing high variability in Fig.~\u203b, where each weekly value is plotted as a distinct point. The smoother curve in Fig.~\u203b is the output of a 51-point causal running averager; i.e., where is the impulse response of the causal 51-point running averager.",
        "475": " DJIA weekly closings filtered by a 51-point running-average FIR filter. Input (orange) and output (black). Notice that, as we have observed in Section \u203b, there is a region at the beginning and end (50 samples, in this case) where the filter is engaging and disengaging the input signal. Also notice that much of the fine-scale variation has been removed by the filter. Finally, notice that the output is shifted relative to the input. In Chapter \u203b, we will develop techniques that will allow us to show that the shift introduced by this filter is exactly samples.",
        "476": "In this example, it is important to be able to compare the input and output without the shift. It would be better use the noncausal centralized running averager where is the impulse response of the centralized running averager. The output can be obtained by shifting (advancing) the output of the causal running averager by 25 samples to the left. In terms of our previous discussion of cascaded FIR systems, we could think of the centralized system as a cascade of the causal system with a system whose impulse response is, i.e., Thus, we find that, i.e., the impulse response of the centralized running averager is a shifted version of the impulse response of the causal running averager. By shifting the impulse response, we remove the delay introduced by the causal system.",
        "477": "Determine the impulse response of the 51-point causal running averager and determine the impulse response for the 51-point centralized running averager. The cascade representation of the centralized running averager is depicted in Fig.~\u203b. Another way to describe the system in Fig.~\u203b is that the second system compensates for the delay of the first, or we might describe the centralized running averager as a delay-compensated running-average filter.",
        "478": " Cascade interpretation of centralized running averager in terms of causal running averager with delay compensation. Figure \u203b shows the input and output for the delay-compensated running-average filter. It now can be seen that corresponding features of the input and output are well aligned.",
        "479": " Input (orange) and output (black) for delay-compensated 51-point running averager applied to DJIA weekly closings. As the example in this section illustrates, FIR filters can be used to remove rapid fluctuations in signals. Furthermore, the example shows that it is worthwhile to develop the fundamental mathematical properties of such systems because these properties can be useful in helping us to understand the way such systems work. In Chapter~\u203b, we will further develop our understanding of FIR systems.",
        "480": "Summary and Links This chapter introduced the concept of FIR filtering. Among the laboratory projects on the CD-ROM, there is one (Lab~#7) that deals with discrete convolution and the effects of sampling. Lab~#7 also requires the use of two MATLAB GUIs, one for sampling, CON2DIS, the other for discrete convolution, DCONVDEMO.",
        "481": " Demo: CON2DIS: Sampling and Aliasing Demo: DCONVDEMO: Convolution The CD-ROM also contains three demonstrations of FIR filtering; the DCONVDEMO GUI, and also individual demonstrations of the properties of linearity and time invariance illustrated by using simple FIR filters to process shifted and scaled sinusoids.",
        "482": "Finally, the reader is once again reminded of the large number of solved homework problems that are available for review and practice on the CD-ROM.",
        "483": " Note: Hundreds of Solved Problems ",
        "484": "CHAPTER 6 Frequency Response of FIR Filters Chapter \u203b introduced the class of FIR discrete-time systems. We showed that the weighted running average of a finite number of input sequence values defines a discrete-time system, and we showed that such systems are linear and time-invariant. We also showed that the impulse response of an FIR system completely defines the system. In this chapter, we introduce the concept of the frequency response of a linear time-invariant FIR filter and show that the frequency response and impulse response are uniquely related. It is remarkable that for linear time-invariant systems, when the input is a complex sinusoid, the corresponding output signal is another complex sinusoid of exactly the same frequency, but with different magnitude and phase. The frequency-response function over all frequencies summarizes the response of an LTI system by giving the magnitude and phase change experienced by all possible sinusoids. Furthermore, since linear time-invariant systems obey the principle of superposition, the frequency-response function is a complete characterization of the behavior of the system for any input that can be represented as a sum of sinusoids. Since almost any discrete-time signal can be represented by a superposition of sinusoids, the frequency response is sufficient therefore to represent the system for almost any signal.",
        "485": "Sinusoidal Response of FIR Systems Linear time-invariant systems behave in a particularly simple way when the input is a discrete-time complex exponential. To see this, consider the {FIR system} and assume that the input is a complex exponential signal with normalized radian frequency Recall that this discrete-time signal could have been obtained by sampling the continuous-time signal If, then and are related by, where is the sampling period. For such inputs, the corresponding output is where Because we have represented the frequency of the complex exponential signal as the general symbol, we have obtained an expression that is a function of. In other words, describes the response of the LTI system to a complex exponential signal of any frequency. The quantity defined by is therefore called the frequency-response function for the system. (Generally, we shorten this to frequency response.)",
        "486": "However, there is an issue of notation for the frequency response that is dictated by consistency with the -transform (to be introduced in Ch.~\u203b). In the righthand side contains powers of the complex exponential. This is true of many expressions for the frequency response. Therefore, we elect to use the notation instead of to emphasize the ubiquity of. The function still depends on the variable. Furthermore, since the impulse response sequence of an FIR filter is the same as the sequence of filter coefficients, we can express the frequency response in terms of either the filter coefficients or the impulse response ; i.e., ",
        "487": " Several important points can be made about and. First of all, the precise interpretation of is as follows: When the input is a discrete-time complex exponential signal, the output of an LTI FIR filter is also a discrete-time complex exponential signal with a different complex amplitude, but the same frequency. The frequency response multiplies the signal, thereby changing the complex amplitude. While it is tempting to express this fact by the mathematical statement, it is strongly recommended that this never be done because it is too easy to forget that the mathematical statement is true only for complex exponential signals of frequency. The statement is meaningless for any signal other than signals of precisely the form. It is very important to understand this point.",
        "488": "A second important point is that the frequency response is complex-valued so it can be expressed either as or as. The effect of the LTI system on the magnitude and phase of the input complex exponential signal is determined completely by the frequency response function. Specifically, if the input is, then using the polar form of gives the result The magnitude and angle form of the frequency response is the most convenient form, since multiplication is most conveniently accomplished in polar form. The angle of the frequency response simply adds to the phase of the input, thereby producing additional phase shift in the complex exponential signal. Since the magnitude of the frequency response multiplies the magnitude of the complex exponential signal, this part of the frequency response controls the size of the output. Thus, is also referred to as the gain of the system. ",
        "489": "EXAMPLE:\u00a0 Frequency Response Formula Consider an LTI system for which the difference equation coefficients are. Substituting into gives To obtain formulas for the magnitude and phase of the frequency response of this FIR filter, we can manipulate the equation as follows: Since for frequencies, the magnitude is and the phase is. EXAMPLE:\u00a0 Complex Exponential Input Consider the complex input. If this signal is the input to the system of Example~\u203b, then and. Therefore, the output of the system for the given input is Thus, for this system and the given input, the output is equal to the input multiplied by 3, and the phase shift corresponds to a delay of one sample. When the sequence of coefficients is symmetrical (,, etc.), the frequency response can be manipulated as in Example~\u203b. Following the style of that example, show that the frequency response of an FIR filter with coefficients can be expressed as ",
        "490": "Superposition and the Frequency Response The principle of superposition makes it very easy to find the output of a linear time-invariant system if the input is a sum of complex exponential signals. This is why the frequency response is so important in the analysis and design of LTI systems.",
        "491": "As an example, suppose that the input to an LTI system is a cosine wave with a specific normalized frequency plus a DC level, If we represent the signal in terms of complex exponentials, the signal is composed of three complex exponential signals, with frequencies and. By superposition, we can determine the output due to each term separately and then add them to obtain the output corresponding to. Because the components of the input signal are all complex exponential signals, it is easy to find their respective outputs if we know the frequency response of the system; we just multiply each component by evaluated at the corresponding frequency, i.e., Note that we have used the fact that a constant signal is a complex exponential with. If we express as, then the algebraic steps conjugate-symmetry property, which is always true when the filter coefficients are real. (See Section~\u203b.)} in show that y[n] can finally be expressed as a cosine signal.",
        "492": " Notice that the magnitude and phase change of the cosine input signal are taken from the positive frequency part of, but also notice that it was crucial to express as a sum of complex exponentials and then use the frequency response to find the output due to each component separately.",
        "493": "EXAMPLE:\u00a0 Cosine Input For the FIR filter with coefficients, find the output when the input is The frequency response of the system was determined in Example \u203b to be ",
        "494": "Note that ; i.e., has conjugate symmetry. Solution of this problem requires just one evaluation of at the frequency : Therefore, the magnitude is and the phase is, so the output is Notice that the magnitude of the frequency response multiplies the amplitude of the cosine signal, and the phase angle of the frequency response adds to the phase of the cosine signal. This problem could also be solved with the DLTI MATLAB GUI. Demo: DLTIDEMO If the input signal consists of many complex exponential signals, the frequency response can be applied to find the output due to each component separately, and the results added to determine the total output. This is the principle of superposition at work. If we can find a representation for a signal in terms of complex exponentials, the frequency response gives a simple and highly intuitive means for determining what an LTI system does to that input signal. For example, if the input to an LTI system is a real signal and can be represented as then it follows that if, the corresponding output is in.",
        "495": " That is, each individual complex exponential component is modified by the frequency response evaluated at the frequency of that component.",
        "496": "EXAMPLE:\u00a0 Three Sinusoidal Inputs For the FIR filter with coefficients, find the output when the input is The frequency response of the system was determined in Example \u203b, and is the same as the frequency response in Example~\u203b. The input in this example differs from that of Example~\u203b by the addition of a constant (DC) term and an additional cosine signal of frequency. The solution by superposition therefore requires that we evaluate at frequencies,, and, giving Therefore, the output is Notice that, in this case, the DC component is multiplied by 4, the component at frequency is multiplied by 3, but the component at frequency is multiplied by 0.1522. Because the frequency-response magnitude (gain) is so small at frequency, the component at this frequency is essentially filtered out of the input signal. The examples of this section illustrate an approach to solving problems that is often called the frequency-domain approach. As these examples show, we do not need to deal with the time-domain description (i.e., the difference equation or impulse response) of the system when the input is a complex exponential signal. We can work exclusively with the frequency-domain description (i.e., the frequency-response function), if we think about how the spectrum of the signal is modified by the system rather than considering what happens to the individual samples of the input signal. We will have ample opportunity to visit both the time-domain and the frequency-domain in the remainder of this chapter.",
        "497": " Demo: Introduction to FIR Filtering ",
        "498": "Steady-State and Transient Response In Section~\u203b, we showed that if the input is where, then the corresponding output of an LTI FIR system is where In, the condition that be a complex exponential signal existing over is important. Without this condition, we will not obtain the simple result of. However, this condition appears to be somewhat impractical. In any practical implementation, we surely would not have actual input signals that exist back to ! Fortunately, we can relax the condition that the complex exponential be defined over the doubly infinite interval and still take advantage of the convenience of. To see this, consider the following \u201csuddenly applied\u201d complex exponential signal that starts at and is nonzero only for : Note that multiplication by the unit-step signal is a convenient way to impose the suddenly applied condition. The output of an LTI FIR system for this input is By considering different values of and the fact that for, it follows that the sum in can be expressed as That is, when the complex exponential signal is suddenly applied, the output can be considered to be defined over three distinct regions. In the first region,, the input is zero, and therefore the corresponding output is zero, too. The second region is a transition region whose length is samples (i.e., the order of the FIR system). In this region, the complex multiplier of depends upon. This region is often called the transient part of the output. In the third region,, the output is identical to the output that would be obtained if the input were defined over the doubly infinite interval. That is, This part of the output is generally called the steady-state part. While we have specified that the steady-state part exists for all, it should be clear that holds only as long as the input remains equal to. If, at some time, the input changes frequency or goes to zero, another transient region will occur.",
        "499": "EXAMPLE:\u00a0 Steady-State Output A simple example will illustrate the above discussion. Consider the system of Exercise~\u203b, whose filter coefficients are the sequence. The frequency response of this system is If the input is the suddenly applied cosine signal we can represent it as the sum of two suddenly applied complex exponential signals. Therefore, the frequency response can be used as discussed in Section~\u203b to determine the corresponding steady-state output. Since at is and, the steady-state output is The frequency response has allowed us to find a simple expression for the output everywhere in the steady-state region. If we desire the values of the output in the transient region, we could compute them using the difference equation for the system.",
        "500": "The input and output signals for this example are shown in Fig.~\u203b. Since for this system, the transient region is (indicated by the shaded region), and the steady-state region is. Also note that, as predicted by the steady-state analysis above, the signal in the steady-state region is simply a scaled and shifted (by 2 samples) version of the input. (a) Input and (b) corresponding output for FIR filter with coefficients. The transient region is the shaded area in (b). (Note the different vertical scales.) ",
        "501": "Properties of the Frequency Response The frequency response function is a complex-valued function of the normalized frequency variable. This function has interesting properties that often can be used to simplify analysis.",
        "502": "Relation to Impulse Response and Difference Equation can be calculated directly from the filter coefficients. If is compared to, we see that, given the difference equation, it is simple to write down an expression for by noting that each term in corresponds to a term or in, and vice versa. Likewise, can be determined directly from the impulse response since the impulse response of the FIR system consists of the sequence of filter coefficients; i.e., for. To emphasize this point, we can write the correspondence",
        "503": " ",
        "504": "The process of going from the difference equation or impulse response to the frequency response is straightforward for the FIR filter. It is also simple to go from the frequency response to the difference equation or to the impulse response if we express in terms of powers of. These points are illustrated by the following examples.",
        "505": "EXAMPLE:\u00a0 }] Consider the FIR filter defined by the impulse response By inspection, the sequence of filter coefficients is, so the difference equation corresponding to this impulse response is and the frequency response of this system is ",
        "506": "EXAMPLE:\u00a0 Difference Equation from Suppose that the frequency response is given by the equation Since, we can write which corresponds to the following FIR difference equation: The impulse response, likewise, is easy to determine directly from, when expressed in terms of powers of. Use the inverse Euler formula for sines to find the impulse response and difference equation for.",
        "507": "Periodicity of An important property of a discrete-time LTI system is that its frequency response is always a periodic function with period. This can be seen by considering a frequency where is any frequency. Substituting into gives since when is an integer. It is not surprising that should have this property, since, as we have seen in Chapter \u203b, a change in the input frequency by is not detectable; i.e., In other words, two complex exponential signals with frequencies differing by cannot be distinguished from their samples alone, so there is no reason to expect a discrete-time system to behave differently for two such frequencies. For this reason, it is always sufficient to specify the frequency response only over an interval of one period, e.g.,.",
        "508": "Conjugate Symmetry The frequency response is complex, but usually has a symmetry in its magnitude and phase that allows us to concentrate on just half of the period when plotting. This is the property of conjugate symmetry which is true whenever the filter coefficients are real so that (equivalently, ). We can prove this property for the FIR case as follows: ",
        "509": "The conjugate-symmetry property implies that the magnitude function is an even function of and the phase is an odd function, i.e., Similarly, the real part is an even function of and the imaginary part is an odd function, i.e., As a result, plots of the frequency response are often shown only over half a period,, because the negative frequency region can be constructed by symmetry. These symmetries are illustrated by the plots in Section \u203b.",
        "510": "Prove that the magnitude is an even function of and the phase is an odd function of for a conjugate-symmetric frequency response.",
        "511": "Graphical Representation of the Frequency Response Two important points should be emphasized about the frequency response of an LTI system. The first is that for a given system, the frequency response usually varies with frequency, so that sinusoids of different frequencies are treated differently by the system. The second important point is that by appropriate choice of the coefficients,, a wide variety of frequency response shapes can be realized. In order to visualize the variation of the frequency response with frequency, it is useful to plot versus. We will see that the plot tells us at a glance what the system does to complex exponential signals and sinusoids of different frequencies. Several examples are provided in this section to illustrate the value of plotting the frequency response.",
        "512": "Delay System The delay system is a simple FIR filter given by the difference equation It has only one nonzero filter coefficient,, so its frequency response is For this filter, a plot of the frequency response is easy to visualize; the magnitude response is one for all frequencies and the phase is given by the equation of a straight line with a slope equal to, as in Fig.~\u203b. As a result, we can associate the property of linear phase with time delay in all filters. Since time delay affects only the time origin of the signal in a predictable way, we often think of linear phase as an ideal phase response.",
        "513": " Phase response of pure delay system,. First Difference System As another simple example, consider the first-difference system The frequency response of this LTI system is ",
        "514": "The different parts of the complex representations are The real and imaginary parts for this example are plotted in Fig.~\u203b; the magnitude and phase are plotted in Fig.~\u203b. All functions are plotted for, even though we would normally need to plot the frequency response of a discrete-time system only for, or (because of conjugate symmetry). These extended plots verify that is periodic with period, and they verify the conjugate symmetry properties discussed in Section \u203b.",
        "515": " (a) Real and (b) imaginary parts for over three periods showing periodicity and conjugate symmetry of. }",
        "516": "The utility of the magnitude and phase plots of can be seen even for this simple example. In Fig.~\u203b,, so we easily see that the system completely removes components with (i.e., DC). Furthermore, we can also see that the system emphasizes the higher frequencies (near ) relative to the lower frequencies, so it would be called a highpass filter. This is another typical way of thinking about systems in the frequency domain.",
        "517": " (a) Magnitude and (b) phase for over three periods showing periodicity and conjugate symmetry of. }",
        "518": "The real and imaginary parts and the magnitude and phase can always be determined as demonstrated above by standard manipulations of complex numbers. However, there is a simpler approach for getting the magnitude and phase when the sequence of coefficients is either symmetric or antisymmetric about a central point. The following algebraic manipulation of is possible because the coefficients satisfy the symmetry condition: The trick, which we have already used in Example \u203b on p.~, is to factor out an exponential whose phase is half of the filter order times, and then use the inverse Euler formula to combine corresponding positive- and negative-frequency complex exponentials; i.e., The form derived for is almost a valid polar form, but since is negative for, we must write and absorb the algebraic sign\u203b, which exhibits several linear segments. Notice also that the phase plot has discontinuities at and. The size of these discontinuities is, since they correspond to a sign change in.",
        "519": "EXAMPLE:\u00a0 First-Difference Removes DC Suppose that the input to a first-difference system is. Since the output is related to the input by the equation, it follows that: From this result, we see that the first-difference system removes the constant value and leaves two cosine signals of the same frequency, which could be combined by phasor addition. However, the solution using the frequency-response function is simpler. Since the first-difference system has frequency response the output of this system for the given input is Therefore, since and the output will be A Simple Lowpass Filter In Examples \u203b, \u203b, and \u203b, the system had frequency response Since the factor for all, it follows that and These functions are plotted in Fig.~\u203b for. Figure~\u203b shows at a glance that the system has a delay of 1 sample and that it tends to favor the low frequencies (close to ) with high gain, while it tends to suppress high frequencies (close to ). In this case, there is a gradual decrease in gain from to, so that the midrange frequencies receive more gain than the high frequencies, but less than the low frequencies. Filters with magnitude responses that suppress the high frequencies of the input are called lowpass filters.",
        "520": " Magnitude (a) and phase (b) of system with frequency response. Gray dots indicate points where the frequency response is evaluated to calculate the sinusoidal response in Example~\u203b. }",
        "521": "EXAMPLE:\u00a0 Lowpass Filter If we repeat Example~\u203b, we can show how a plot of makes it easy to find a filters output for sinusoidal inputs. In Example~\u203b, the input was and the filter coefficients were. Fig.~\u203b shows the frequency response of this system, which is a lowpass filter. In order to get the output signal, we must evaluate at frequencies,, and giving These values are the points indicated with gray dots on the graphs of Fig.~\u203b. As in Example~\u203b, the output is The plot of the frequency response shows that all frequencies around are greatly attenuated by the system. Also, the linear phase plot with slope of indicates that all frequencies experience a time delay of 1 sample.",
        "522": "The output of the simple lowpass filter is the time waveform shown in Fig.~\u203b(b). Note that the DC component is indicated in both parts of the figure as a gray horizontal line. The output appears to be the sum of a constant level of 16 plus a cosine that has amplitude 9 and seems to be periodic with period 6. Closer inspection reveals that this is not exactly true because there is a third output component at frequency which is just barely visible in Fig.~\u203b(b). Its size is about 5% of the size of the component with frequency. Input and output of system with frequency response. (a) Segment of the input signal given by, and (b) the corresponding segment of the output. ",
        "523": "Cascaded LTI Systems In Section \u203b, we showed that if two LTI systems are connected in cascade (output of the first is input to the second), then the overall impulse response is the convolution of the two individual impulse responses, and therefore the cascade system is equivalent to a single system whose impulse response is the convolution of the two individual impulse responses. In this section, we will show that the frequency response of a cascade connection of two LTI systems is simply the product of the individual frequency responses.",
        "524": " Demo: Cascading FIR Filters Figure~\u203b(a) shows two LTI systems in cascade. To find the frequency response of the overall system (from input to output ), we let Then, the output of the first LTI system is and the output of the second system is From a similar analysis of Fig.~\u203b(b), it follows that Since from the commutative property of complex multiplication, it follows that ; i.e., the two cascade systems are equivalent for the same complex exponential input, and both of them are equivalent to a single LTI system with frequency response The output of any system with frequency response given by will be the same as either or. This is depicted in Fig.~\u203b(c).",
        "525": " Frequency response of three equivalent cascaded LTI systems. All three systems have the same frequency response so that for the same input. Recall from Section \u203b on p.~ that the overall impulse response is. We can summarize this by the correspondence",
        "526": " That is, convolution of impulse responses corresponds to multiplication of the frequency responses of cascaded systems. The correspondence shown in is useful because it provides another way of representing and manipulating LTI systems. This is illustrated by the following example.",
        "527": "EXAMPLE:\u00a0 Cascade Suppose that the first system in a cascade of two systems is defined by the set of coefficients and the second system is defined by the coefficients. The frequency responses of the individual systems are and The overall frequency response is Thus, the overall equivalent impulse response is This example illustrates that convolution of two impulse responses is equivalent to multiplying their corresponding frequency responses. Notice that, for FIR systems, the frequency response is just a polynomial in the variable. Thus, multiplying two frequency responses requires polynomial multiplication. This result provides the theoretical basis for the \u201csynthetic\u201d polynomial multiplication algorithm discussed in Section \u203b on p.~.",
        "528": "Suppose that two systems are cascaded. The first system is defined by the set of coefficients, and the second system is defined by the coefficients. Determine the frequency response and the impulse response of the overall cascade system.",
        "529": "Running-Average Filtering A simple linear time-invariant system is defined by the equation This system is called an -point running averager, because the output at time is computed as the average of and the previous samples of the input. The system defined by can be implemented in MATLAB for by the statements: bb = ones(11,1)/11; yy = conv(bb, xx); where xx is a vector containing the samples of the input. The vector bb contains the 11 filter coefficients, which are all the same size, in this case.",
        "530": "Using on p.~, the frequency response of the -point running averager is We can derive a simple formula for the magnitude and phase of the averager by making use of the formula for the sum of the first terms of a geometric series, First of all, we identify as, and then do the following steps: The numerator and denominator are simplified by using the inverse Euler formula for sines. We will find it convenient to express in the form where The function is often called the Dirichlet function, and the subscript indicates that it comes from an -point averager. In MATLAB, it can be evaluated with the diric function.",
        "531": "Plotting the Frequency Response The frequency response of an 11-point running-average filter is given by the equation where, in this case, is the Dirichlet function defined by with, i.e., As Eq.~ makes clear, the frequency-response function,, can be expressed as the product of the real amplitude function and the complex exponential factor. The latter has a magnitude of 1 and a phase angle. Figure \u203b(a) shows a plot of the amplitude function, ; the phase function is in the bottom part of the figure. We use the terminology amplitude rather than magnitude, because can be negative. We can obtain a plot of the magnitude by taking the absolute value of. We shall consider the amplitude representation first, because it is simpler to examine the properties of the amplitude and phase functions. Figure \u203b shows only one period, i.e.,. The frequency response is, of course, periodic with period, so the plots in Fig.~\u203b would simply repeat with that period.",
        "532": " (a) Amplitude and (b) phase functions for frequency response of 11-point running-average filter. The amplitude is a Dirichlet function, but it is not the magnitude since it has negative values. In the case of the 11-point running averager, the phase factor is easy to plot, since it is a straight line with slope of. The amplitude factor is somewhat more involved. First note that ; i.e., is an even function of because it is the ratio of two odd functions. Since is even and periodic with period, we need only to consider its values in the interval. All others can be inferred from symmetry and periodicity. The numerator is, which, of course, oscillates between and and is zero whenever, where is an integer; solving for, is zero at frequencies where is a nonzero integer. The denominator of is, which is zero at and increases to a maximum of eleven at. Therefore, is large around, where the denominator is small, and it oscillates with decreasing amplitude as increases to. The behavior for is of particular interest because, at this frequency, is indeterminate, i.e., By lH rule, however, it is easily shown that. Thus, the function has the following properties: is an even function of that is periodic with period. has a maximum value of one at. decays as increases, reaching its smallest nonzero amplitude at. has zeros at nonzero integer multiples of. (In general, the zeros of are at nonzero multiples of.) Together, the amplitude and phase plots of Fig.~\u203b completely define the frequency response of the 11-point running-average filter. Normally, however, the frequency response is represented in the form This would require plotting and as functions of. It is easy to see from that. The top part of Fig.~\u203b shows for the 11-point running-average filter. On the other hand, the phase response,, is more complicated to plot than the linear function shown in Fig.~\u203b(b). There are two reasons for this:",
        "533": " The algebraic sign of must be represented in the phase function, since discards the sign of. It is generally easiest to plot the principal value of the phase function. Magnitude and phase of frequency response of 11-point running-average filter. Compare to Fig.~\u203b. ",
        "534": "The sign of can be incorporated into the phase by noting that whenever. The principal value of the angle of a complex number is defined to be the angle between and radians. Using the result where is any integer, we see that we can add or subtract integer multiples of from the angle of a complex number without changing the value of the complex number. We can always find a multiple of, which, when added to or subtracted from, will produce a result in the range.",
        "535": "This is called reducing modulo. The principal value is generally what is computed when an inverse tangent function is evaluated in MATLAB or other computer languages. In Fig.~\u203b, we were able to plot an angle whose values were outside the principal value range simply because we had an equation for the angle. A plot like Figure~\u203b could be produced using the following MATLAB code: omega = -pi:(pi/500):pi; bb = ones(1,11)/11; HH = freqz(bb,1,omega); subplot(2,1,1), plot(omega,abs(HH)) subplot(2,1,2), plot(omega,angle(HH)) The MATLAB function angle uses the arctangent to return the principal value of the angle determined by the real and imaginary parts of the elements of the vector HH.",
        "536": "In Fig.~\u203b, the phase curve is seen to have discontinuities that occur at the zeros of. These discontinuities are due to the combination of multiples of radians added to the phase due to the negative sign of in the intervals,, and and multiples of that are added implicitly in the computation of the principal value. The equation for the phase curve plotted in Fig.~\u203b is as follows for frequencies : The values for can be filled in using the fact that.",
        "537": "Test yourself to see whether you understand why the principal value of is as shown in Fig.~\u203b for the 11-point moving averager. Cascade of Magnitude and Phase It can be seen from that is the product of two functions, i.e., where and which is the Dirichlet function defined earlier. The component contributes only to the phase of, and we see that this phase contribution is a linear function of. Earlier, we saw that a linear phase such as corresponds to a time delay of samples. The linear-phase contribution (with slope of ) is clearly in evidence in Fig.~\u203b(b). The frequency response of the second system is real. It contributes to the magnitude of, and when it is negative, it also contributes to the phase of causing the discontinuities at multiples of.",
        "538": "The product representation suggests the block diagram of Fig.~\u203b, which shows that the running averager can be thought of as a cascade combination of a delay followed by a \u201clowpass filter\u201d that accentuates low frequencies relative to high frequencies. The overall moving average system can only be implemented by. However, the block diagram is a useful convenience for thinking about the system. The system cannot be implemented by this cascade because can never be implemented by itself. When is a integer, is a delay in Fig.~\u203b. The case when is not an integer requires special interpretation, which will be provided in Section \u203b. For the present discussion, we will assume that is an odd integer, so that is also an integer.",
        "539": " Representation of -point running averager as the cascade of a delay and a real frequency response. Experiment: Smoothing an Image As a simple experiment to show the filtering effect of the running-average system, consider the image at the top of Fig.~\u203b. The image is a two-dimensional discrete signal that can be represented as a two-dimensional array of samples. In an image, each sample value is called a pixel, which is shorthand for picture element. A single horizontal scan line (at ) was extracted from the image yielding the one-dimensional signal, plotted at the bottom of Fig.~\u203b. The position in the image from which was extracted is shown by the orange line in the image. The values in the image signal are all positive integers in the range. These numbers can be represented by 8-bit binary numbers. If you compare the one-dimensional plot to the gray levels in the region around the line, you will see that dark regions in the image have large values (near 255), and bright regions have low values (near zero). This is actually a \u201cnegative\u201d image, but that is appropriate since it is a scan of a handwritten homework solution.",
        "540": " \u201cHomework\u201d image and one horizontal scan of the image at row 40. An 11-point running averager was applied to, and the input and output were plotted on the same graph (Fig.~\u203b). Notice that the output appears to be a smoother version of with a slight shift to the right. This shift is the 5-sample delay that we expect for an 11-point running averager. The smoothness is a result of the relative attenuation of the higher frequencies in the signal that correspond to the edges of the handwritten characters in the image. To verify the effect of the delay of the system, Fig.~\u203b shows a plot of and. Now we see that the output appears to be aligned with the input.",
        "541": " Input and output of 11-point running averager. The solid orange line is the output; the thin gray line is the input. Input and output of 11-point running averager. The solid orange line is the output; the thin gray line is the input. The 11-point averager can be applied first over all the rows and then over all the columns of the image to get a visual assessment of the lowpass filtering operation. Each row is filtered using the one-dimensional averager, then each column of that filtered image is processed. The result is shown in Fig.~\u203b, where is it obvious that the lowpass filter has blurred the image. As we have seen, the filter attenuates the high-frequency components of the image. Thus, we can conclude that sharp edges in an image must be associated with high frequencies.",
        "542": " Result of filtering both the rows and the columns of the \u201chomework\u201d image with an 11-point running averager. The processed image had to be rescaled so that its values would occupy the entire gray scale range. As another example of the effect of filtering, the image signal was distorted by adding a cosine signal to create a new input: This corrupted signal was filtered with the 11-point running averager. The delayed input and the corresponding output are shown in Fig.~\u203b. By comparing Figs.~\u203b and \u203b, it is clear that the output is the same for and. The reason is clear: is one of the frequencies that is completely removed by the averaging filter, because. Since the system is LTI and obeys superposition, the output due to must be the same as the output due to alone. If the cosine is added to each row of an image, it appears visible as vertical stripes (Fig.~\u203b(a)). When each row is processed with an 11-point averaging filter, the cosine will be removed, but the image will be blurred horizontally (Fig.~\u203b(b)). Result of filtering both the rows and the columns of the \u201chomework\u201d image with an 11-point running averager. The processed image had to be rescaled so that its values would occupy the entire gray scale range. (a)~\u201cHomework plus cosine\u201d image. The periodic nature of the cosine across each row causes a vertical striping. (b)~After filtering the rows of the \u201chomework plus cosine\u201d image with an 11-point running averager, the processed image is blurred, but has no traces of the cosine stripes. (Both input and output were rescaled for 8-bit image display.) ",
        "543": " Demo: Cascading FIR Filters ",
        "544": "Filtering Sampled Continuous-Time Signals Discrete-time filters can be used to filter continuous-time signals that have been sampled. In this section, we study the relationship between the frequency response of the discrete-time filter and the effective frequency response applied to the continuous-time signal. When the input to a discrete-time system is a sequence derived by sampling a continuous-time signal, we can use our knowledge of sampling and reconstruction to interpret the effect of the filter on the original continuous-time signal.",
        "545": "Consider the system depicted in Fig.~\u203b, and assume that the input is the complex sinusoid with. After sampling, the input sequence to the discrete-time filter is The relationship between the discrete-time frequency and the continuous-time frequency is If the frequency of the continuous-time signal satisfies the condition of the sampling theorem, i.e.,, then there will be no aliasing, and the normalized discrete-time frequency is such that.",
        "546": " System for doing discrete-time filtering of continuous-time signals. The frequency response of the discrete-time system gives us a quick way to calculate the output in Fig~\u203b. If we now make the substitution, then we can write in terms of the analog frequency as Finally, since no aliasing occurred in the original sampling, the ideal D-to-C converter will reconstruct the original frequency, giving Remember that this formula for is good only for frequencies such that, and recall that the ideal D-to-C converter reconstructs all digital frequency components in the band as analog frequencies in the band. As long as there is no aliasing, the frequency band of the input signal matches the frequency band of the output. Thus, the overall system of Fig.~\u203b behaves as though it is an LTI continuous-time system whose frequency response is.",
        "547": "It is very important to understand this analysis of the system of Fig.~\u203b. We have just shown that the system of Fig.~\u203b can be used to implement LTI filtering operations on continuous-time signals. Furthermore, it is clear from this analysis that the block diagram of Fig.~\u203b represents an infinite number of systems. This is true in two ways. First, for any given discrete-time system, we can change the sampling period and obtain a new system. Alternatively, if we fix the sampling period, we can change the discrete-time system to vary the overall response. In a specific case, all we have to do is select the sampling rate to avoid aliasing, and then design a discrete-time LTI filter whose frequency response has the desired frequency-selective properties.",
        "548": "In general, when we use the system of Fig.~\u203b to filter continuous-time signals, we would want to choose the sampling frequency to be as low as possible. Why? Example: Lowpass Averager As an example, we use the 11-point moving averager as the filter in Fig.~\u203b. The frequency response of this discrete-time system is The magnitude of this frequency response is shown in the top part of Fig.~\u203b.",
        "549": "When this system is used as the discrete-time system in Fig.~\u203b with sampling frequency, we want to answer two questions: What is the equivalent analog frequency response, and how would the signal be processed by the system?",
        "550": "The frequency-response question is easy. The equivalent analog-frequency response is where is in Hz. A plot of the equivalent continuous-time frequency response versus is shown in the bottom part of Fig.~\u203b. Note that the frequency response of the overall system stops abruptly at Hz, since the ideal D-to-C converter does not reconstruct frequencies above.",
        "551": " Frequency response of 11-point moving averager (a) and equivalent analog frequency response (b) when used to filter analog signals. The sampling frequency is Hz, so the maximum analog frequency that can be processed is 500 Hz. The second question is also easy if we track the two frequencies of the input signal through the three systems of Fig.~\u203b. The input contains two frequencies at and. Since, there is no aliasing, so the same frequency components appear in the output signal. The magnitude and phase changes are found by evaluating the equivalent frequency response at 25 and 250 Hz. These values can be checked against the plots in Fig.~\u203b. Thus the final output is ",
        "552": "The lowpass nature of the filter has greatly attenuated the 250-Hz component, while the 25-Hz component suffered only slight attenuation because it lies in the passband of the filter near 0~Hz.",
        "553": "Assuming the same input signal and the same discrete-time system, work the example of this section again, but use a sampling rate of Hz. Interpretation of Delay We have seen that a frequency response of the form implies a time delay of samples. For, an integer, the interpretation of this is straightforward. If the input to the system is, the corresponding output is. However, if is not an integer, the interpretation is less obvious. An example of where this can occur is the -point running-average system whose frequency response is where is the real function Thus, the -point running averager includes a delay of samples. If is an odd integer, this delay causes the output to be shifted samples with respect to the input. However, if is an even integer, then is not an integer. The analyis of this section provides a useful interpretation of this delay factor.",
        "554": "Suppose that the input to the ideal C-to-D converter is and that there is no aliasing, so that the sampled input to the -point running averager is where. Now, the output of the -point running-average filter is Finally, if (i.e., no aliasing occurred in the sampling operation), then the ideal D-to-C converter will reconstruct the complex exponential signal Thus, regardless of whether or not is an integer, the delay factor corresponds to a delay of seconds with respect to continuous-time signals sampled with sampling period.",
        "555": "EXAMPLE:\u00a0 Time-Delay of FIR Filter To illustrate the effect of non-integer delay with the running average filter, consider the cosine signal, which could have resulted in Fig~\u203b from sampling the signal with sampling rate Hz. Figure~\u203b(a) shows and. If is the input to a 5-point running average filter, the steady-state part of the output is For this filter output, the output of the D-to-C converter in Fig.~\u203b(b) would be The delay is 2 samples. On the other hand, if the same signal is the input to a 4-point running-average system, the steady-state part of the output (Fig.~\u203b(c)) is Now the delay is 3/2 samples, so we cannot write as an integer shift with respect to the input sequence. In this case, the \u201c3/2 samples\u201d delay introduced by the filter can be interpreted in terms of the corresponding output of the D-to-C converter in Fig.~\u203b(c), which in this case would be Figure \u203b shows the input and the corresponding outputs and and and. In all cases, the solid curve is the continuous-time cosine signal that would be reconstructed by the ideal D-to-C converter for the given discrete-time signal. The following specific points are made by this example: Both outputs are smaller than the input. This is because for both cases. The gray vertical lines in the lower two panels show the peaks of the output cosine signals that correspond to the peak of the input at. Note that the delay is for the 5-point averager and for the 4-point averager. The effect of the fractional delay is to implement an interpolation of the cosine signal at points halfway between the original samples. Input signal (a), output of 5-point running averager (b), and output of 4-point running averager (c). The solid gray curve is the corresponding continuous-time signal: (a), (b), and (c). ",
        "556": "Summary and Links This chapter introduced the concept of the frequency response for the class of FIR filters. The frequency response applies to any linear time-invariant system, as we will see in upcoming chapters. The MATLAB GUI shown in Fig.~\u203b embodies the primary result of this chapter. It shows that one evaluation of the frequency response is sufficient to predict how a sinusoid will be processed by a LTI system. When the filter changes, the frequency response changes so sinusoidal frequency components are treated differently.",
        "557": " Demo: DLTIDEMO: Sinusoid-in sinusoid-out This chapter extends the discussion of Chapter~\u203b, which introduced the basics of FIR filtering. The labs, in particular, require the student to be familiar with both chapters. Lab #8 is an experiment with the frequency response of FIR filters.",
        "558": " Lab: #8 Frequency Response Two other labs involving FIR filtering are listed under Chapter 7 on the CD-ROM. Lab #9 uses results from Lab #8 to build a system to decode Touch-Tone dialing signals. Finally, Lab #10 uses filters to build a system that determines which note is played on a piano. Lab: #9 Encoding and Decoding Touch-Tones Lab: #10 Octave Band Filtering The CD-ROM also contains the following demonstrations of lowpass and highpass filtering:",
        "559": " Filtering photographic images to show that lowpass filtering is blurring, and that highpass filtering enhances images. Cascade processing of images to show that a highpass filtering can undo the blurring effects of a lowpass filter.",
        "560": " Demo: Cascading FIR Filters Filtering of sound signals to illustrate bass and treble emphasis.",
        "561": " Demo: Introduction to FIR Filtering Finally, the reader is reminded of the large number of solved homework problems available for review and practice on the CD-ROM.",
        "562": " Note: Hundreds of Solved Problems Graphical User Interface (GUI) for DLTIdemo which illustrates the sinusoid-in gives sinusoid-out concept. The user can change the LTI system as well as the input sinusoidal frequency, amplitude and phase. ",
        "563": "CHAPTER 7 The Discrete-Time Fourier Transform In Chapter \u203b we showed that interesting continuous-time waveforms can be synthesized by summing sinusoids, or complex exponential signals, having different frequencies and complex amplitudes. We also introduced the concept of the spectrum of a signal as the collection of information about the frequencies and corresponding complex amplitudes, of the complex exponential signals, and found it convenient to display the spectrum as a plot of spectrum lines versus frequency, each labeled with amplitude and phase. This spectrum plot is a frequency-domain representation that tells us at a glance \u201chow much of each frequency is present in the signal.\u201d In Chapter \u203b we extended the spectrum concept from continuous-time signals to discrete-time signals obtained by sampling. In the discrete-time case, the line spectrum is plotted as a function of normalized frequency. In Chapter \u203b we developed the frequency response which is the frequency-domain representation of an FIR filter. Since an FIR filter can also be characterized in the time domain by its impulse response signal, it is not hard to imagine that the frequency response is the frequency-domain representation, or spectrum, of the sequence. In this chapter, we take the next step by developing the discrete-Time Fourier transform. The DTFT, as we shall usually call it, is a frequency-domain representation for a wide range of both finite- and infinite-length discrete-time signals. The DTFT will be denoted,, which shows that the frequency dependence is specifically through the complex exponential function. The operation of taking the Fourier transform of a signal will become a common tool for analyzing signals and systems in the frequency domain. The application of the discrete-time Fourier transform is usually called Fourier analysis, or spectrum analysis or \u201cgoing into the Fourier domain or frequency domain.\u201d Thus the words spectrum, Fourier, and frequency-domain representation become equivalent, even though each one retains its own distinct character.",
        "564": "DTFT: Discrete-Time Fourier Transform The concept of frequency response discussed in Chapter~\u203b emerged from analysis showing that if an input to an LTI discrete-time system is of the form, then the corresponding output has the form, where is called the frequency response of the LTI system. This fact, coupled with the principle of superposition for LTI systems leads to the fundamental result that the frequency response function is all we need to know about the system in order to determine the output due to any linear (additive) combination of signals of the form or. For discrete-time filters such as the causal FIR filters discussed in Chapter~\u203b, the frequency response function is obtained from the summation formula In a mathematical sense, the impulse response is transformed into the frequency response by the operation of evaluating for each value of over the domain. The operation of transformation (adding up the terms in for each value ) replaces a function of a discrete-time index (a sequence) by a periodic function of the continuous frequency variable. By this transformation, the time-domain representation is replaced by the frequency-domain representation. For this notion to be complete and useful, we need to know that the result of the transformation is unique, and we need the ability to go back from the frequency-domain representation to the time-domain representation. That is, we need an inverse transform that recovers the original from. In Chapter~\u203b, we showed that the sequence can be reconstructed from a frequency response represented in terms of powers of as in by simply picking off the coefficients of the polynomial; i.e., is the coefficient of. While this process can be effective if is small, there is a much more powerful approach to inverting the transformation that holds even for infinite-length sequences.",
        "565": "In this section, we will show that the frequency response is identical to the result of applying the more general concept of the discrete-time Fourier transform to the impulse response of the LTI system. We will give an integral form for the inverse discrete-time Fourier transform that can be used even when does not have a finite polynomial representation such as. Furthermore, we will show that the discrete-time Fourier transform can be used to represent a wide range of sequences, including sequences of infinite length, and that these sequences can be impulse responses, inputs to LTI systems, outputs of LTI systems, or indeed, any sequence that satisfies certain conditions to be discussed in this chapter. The Discrete-Time Fourier Transform The discrete-time Fourier transform or {DTFT} of a sequence is defined as The DTFT that results from the definition is a function of frequency. Going from the signal to its DTFT is referred to as \u201ctaking the forward transform\u201d, and going from the DTFT back to the signal is referred to as \u201ctaking the inverse transform.\u201d The limits on the sum in are shown as infinite so that the DTFT will be defined for infinitely long signals as well as finite-length signals. However, a comparison of to shows that if the sequence were a finite-length impulse response, then the DTFT of that sequence would be the same as the frequency response of the FIR system. More generally, if is the impulse response of an LTI system, then the DTFT of is the frequency response of that system. Examples of infinite-duration impulse response filters will be given in Chapter \u203b.",
        "566": "Show that the DTFT function defined in is always periodic in with period, i.e., DTFT of a Shifted Impulse Sequence Our first task is to develop examples of the DTFT for some common signals. The simplest case is the time-shifted unit-impulse sequence. Its forward DTFT is by definition Since the impulse sequence is nonzero only at it follows that the sum has only one nonzero term, so To emphasize the importance of this and other DTFT relationships, we use the the notation to denote the forward and inverse transforms in one statement: }} If both and have DTFTs, then we can use the algebraic property that multiplication distributes over addition to write That is, the frequency-domain representations are combined in exactly the same way as the signals are combined.",
        "567": "EXAMPLE:\u00a0 DTFT of an FIR Filter The following FIR filter has a finite-length impulse response signal: Each impulse in is transformed using, and then combined according to the linearity property of the DTFT which gives Uniqueness of the DTFT The DTFT is a unique relationship between and ; in other words, two different signals cannot have the same DTFT. This is a consequence of the linearity property because if two different signals have the same DTFT, then we can form a third signal by subtraction and obtain However, from the definition it is easy to argue that has to be zero if its DTFT is zero, which in turn implies that.",
        "568": "The importance of uniqueness is that if we know a DTFT representation such as, we can start in either the time or frequency domain and easily write down the corresponding representation in the other domain. For example, if then we know that.",
        "569": "DTFT of a Pulse Another common signal is the -point rectangular pulse, which is a finite-length time signal consisting of all ones: Its forward DTFT is by definition where we have used the formula for the sum of terms of a geometric series to \u201csum\u201d the series and obtain a closed form expression for. This is a signal that we studied before in Chapter~\u203b as the impulse response of an -point running-sum filter, along with the running-average filter. In Sect. 6-7 the frequency response of the running-average filter was shown to be the product of a Dirichlet form and a complex exponential. Referring to the earlier results in Sect. 6-7 or further manipulating, we obtain another DTFT pair: Since the filter coefficients of the running-sum filter are times the filter coefficients of the running-average filter, there is no in the denominator of.",
        "570": "DTFT of a Right-Sided Exponential Sequence As an illustration of the DTFT of an infinite-duration sequence, consider a \u201cright-sided\u201d exponential signal of the form, where can be real or complex. Such a signal is zero for (on the left-hand side of a plot). It will decay \u201cexponentially\u201d for if ; it will remain constant at 1 if ; and it will grow exponentially if. Its DTFT is by definition We can obtain a closed form expression for by noting that which can now be recognized as the sum of all the terms of an infinite geometric series where the ratio between successive terms is. For such a series there is a formula for the sum that we can apply to give the final result There is one limitation, however. Going from the infinite sum to the closed-form result is only valid when or. Otherwise, the terms in the geometric series grow without bound and their sum will be infinite.",
        "571": "This DTFT pair is another widely used result, worthy of highlighting as we have done with the shifted impulse and pulse sequences. }} \tx[n]= a^{n}u[n] { X(e^{j e^{-j }} ",
        "572": "Use the uniqueness property of the DTFT along with to find whose DTFT is Use the linearity of the DTFT and to determine the DTFT of the following sum of two right-sided exponential signals:. Existence of the DTFT In the case of finite-length sequences such as the impulse response of an FIR filter, the sum defining the DTFT is always finite for any because it has a finite number of terms. Thus the frequency response of an FIR filter as in always exists (is finite). However, in the general case, where one or both of the limits on the sum in are infinite, the DTFT sum may diverge (become infinite). This is illustrated by the right-sided exponential sequence in Section~\u203b when.",
        "573": "A sufficient condition for the existence of the DTFT of a sequence emerges from the following manipulation that develops a bound on the size of : X(e^{j & = & x[n] e^{-j n} & & x[n] e^{-j n} ( of sum} of magnitudes}) & & x[n] e^{-j n} } ( of product} = of magnitudes}) & & x[n] It follows that a sufficient condition for the existence of the DTFT of is A sequence satisfying is said to be absolutely summable, and when holds, the infinite sum defining the DTFT in is said to converge to a finite result for all.",
        "574": " EXAMPLE:\u00a0 DTFT of Complex Exponential? Consider a right-sided complex exponential sequence, when. Applying the condition of to this sequence leads to Thus, the DTFT of a right-sided complex exponential is not guaranteed to exist, and it easy to verify that. On the other hand, if, the DTFT of exists and is given by the result of Section~\u203b with. The non-existence of the DTFT is also true for the related case of a two-sided sinusoid, defined as for. The Inverse DTFT Now that we have a condition for the existence of the DTFT, we need to address the question of the inverse discrete-time Fourier transform. The uniqueness property implies that if we have a table of known DTFT pairs such as, and, we can always go back and forth between the time-domain and frequency-domain representations simply by table lookup as in Exercise \u203b. However, with this approach, we would always be limited by the size of our table of known DTFT pairs.",
        "575": "Instead, we want to continue the development of the DTFT by studying a general expression for performing the inverse discrete-time Fourier transform. The DTFT is a function of the continuous variable, so an integral with respect to normalized frequency is needed to transform back to.",
        "576": " Observe that is an integer parameter in the integral, while now is a dummy variable of integration that disappears when the definite integral is evaluated at its limits. The variable can take on all integer values in the range, and hence, using we can extract each sample of a sequence whose DTFT is. We could verify that is the correct inverse discrete-time Fourier transform relation by substituting the definition of the DTFT in into and rearranging terms.",
        "577": "Instead of carrying out a general proof, we present a simpler and more intuitive justification by working with the shifted impulse sequence, whose DTFT is known to be The objective is to show that gives the correct time-domain result when operating on. If we substitute this DTFT into, we obtain The definite integral of the exponential must be treated as two cases: first, when,",
        "578": " and then for, Equations and show that the complex exponentials and (when viewed as periodic functions of ) are orthogonal to each other. Putting these two cases together, we have Thus, we have shown that correctly returns the sequence, when the DTFT is.",
        "579": "This example is actually strong enough to justify that the inverse DTFT integral will always work, because the DTFT of a general finite-length sequence will always be a linear combination of complex exponential terms like. The linearity property of the DTFT, therefore, guarantees that the inverse DTFT integral will recover a finite-length sequence that is the same linear combination of shifted impulses, which is the correct sequence for a finite-length signal. If the signal is of infinite extent, it can be shown that if is absolutely summable as in so that the DTFT exists, then recovers the original sequence from.",
        "580": "Recall that defined in is always periodic in with period. Use this fact and a change of variables to argue that we can rewrite the inverse DTFT integral with limits that go from 0 to, instead of to ; i.e., show that Bandlimited DTFT Ordinarily we define a signal in the time domain, but the inverse DTFT integral enables us to define a signal in the frequency domain by specifying its DTFT as a function of frequency. Once we specify the magnitude and phase of, we apply and carry out the integral to get the signal. An excellent example of this process is to define an ideal band limited signal, which is a function that is nonzero in the low frequency band and zero in the high frequency band. If the nonzero portion of the DTFT is a constant value of one with a phase of zero, then we have which is plotted in Fig.~\u203b(a).",
        "581": " Bandlimited DTFT. (a) DTFT is a rectangle bandlimited to, (b) inverse DTFT is a sinc function. For this simple DTFT function, the integrand of has a piecewise constant function that is relatively easy to integrate after we substitute the definition of into the inverse DTFT integral The integral has been broken into three cases for the three intervals where is either zero or one. Only the middle integral is nonzero, and the integration yields The last step uses the inverse Euler formula for sine.",
        "582": "The result of the inverse DTFT is the discrete-time signal where. This mathematical form, which is called a \u201csinc function,\u201d is plotted in Fig.~\u203b(b) for. Although the \u201csinc function\u201d appears to be undefined at, a careful application of LH rule, or the small angle approximation to the sine function, shows that the value is actually. Since a DTFT pair is unique, we have obtained another DTFT pair that can be added to our growing inventory. We will revisit this transform as a frequency response in Sect. \u203b when discussing ideal filters. Our usage of the term \u201csinc function\u201d refers to a form, rather than a specific function definition. The form of the \u201csinc function\u201d has a sine function in the numerator and the variable in the denominator. In signal processing, the normalized sinc function is defined as and this is the definition used in the MATLAB M-file sinc. If we expressed in in terms of this definition of the sinc function, we would write While it is nice to have the name sinc for this function, which turns up often in Fourier transform expressions, it can be cumbersome to figure out the proper argument and scaling. On the other hand, the term \u201csinc function\u201d is widely used as a convenient shorthand for any function of the general form of, so we will use the term henceforth in that sense. The sinc signal is important in discrete-time signal and system theory, but it is impossible to determine its DTFT by directly applying the forward transform summation. This can be seen by writing out the forward DTFT of a sinc function, which is the infinite summation on the left-hand side below. However, because of the uniqueness of the DTFT, we have obtained the desired DTFT transform pair by starting in the frequency domain with the correct transform and taking the inverse DTFT of to get the \u201csinc function\u201d sequence.",
        "583": "Another property of the sinc function sequence is that it is not absolutely summable. If we recall from that absolute summability is a sufficient condition for the DTFT to exist, then the sinc function must be an exception. We know that its DTFT exists because the right-hand side of is finite and well defined. Therefore, the transform pair shows that the condition of absolute summability is a sufficient, but not a necessary condition, for the existence of the DTFT.",
        "584": "Inverse DTFT for the Right-Sided Exponential Another infinite-length sequence is the right-sided exponential signal discussed in Section~\u203b. In this case, we were able to use a familiar result for geometric series to \u201csum\u201d the expression for the DTFT and obtain a closed-form representation On the other hand, suppose that we want to determine given in. Substituting this into the inverse DTFT expression gives",
        "585": " Although techniques exist for evaluating such integrals using the theory of complex variables, we do not assume knowledge of these techniques. However, all is not lost, because the uniqueness property of the DTFT tells us that we can always rely on the tabulated result in, and we can write the inverse transform by inspection.",
        "586": "The important point of this example and the sinc function example is that once a transform pair has been determined, by whatever means, we can use that DTFT relationship to move back and forth between the time and frequency domains without integrals or sums. Furthermore, in n Section~\u203b we will introduce a number of general properties of the DTFT that can be employed to simplify forward and inverse DTFT manipulations even more. The DTFT is the Spectrum So far, we have not used the term \u201cspectrum\u201d when discussing the DTFT, but it should be clear at this point that it is appropriate to refer to the DTFT as a spectrum representation of a discrete-time signal. Recall that we introduced the term spectrum in Chapter~\u203b to mean the collection of frequency and complex amplitude information required to synthesize a signal using the Fourier synthesis equation in in Section \u203b. In the case of the DTFT, the synthesis equation is the inverse transform integral, and the analysis equation provides a means for determining the complex amplitudes of the complex exponentials in the synthesis equation. To make this a little more concrete, we can view the inverse DTFT integral as the limit of a finite sum by writing in terms of the Riemann sum definition of the integral where is the spacing between the frequencies, with the range of integration being covered by choosing. The expression on the right in contains a sum of complex exponential signals whose spectrum representation is the set of frequencies together with the corresponding complex amplitudes. This is illustrated in Fig. \u203b which shows the values as gray dots and the rectangles have area equal to. Each one of the rectangles can be viewed as a spectrum line, especially when.",
        "587": " Riemann sum approximation to the integral of the inverse DTFT. Here, which is the DTFT of. In the limit as, the magnitudes of the spectral components become infinitesimally small, as does the spacing between frequencies. Therefore, suggests that the inverse DTFT integral synthesizes the signal as a sum of infinitely small complex exponentials with all frequencies being used in the sum.",
        "588": "The changing magnitude of specifies the relative amount of each frequency component that is required to synthesize. This is entirely consistent with the way we originally defined and subsequently used the concept of spectrum in Chapters~3-6, so we will henceforth feel free to apply the term spectrum also to the DTFT representation.",
        "589": "Properties of the DTFT We have motivated our study of the DTFT primarily by considering the problem of determining the frequency response of a filter, or more generally the Fourier representation of a signal. While these are important applications of the DTFT, it is also important to note that the DTFT also plays an important role as an \u201coperator\u201d in the theory of discrete-time signals and systems. This is best illustrated by highlighting some of the important properties of the DTFT operator. The Linearity Property As we showed in Section \u203b, the DTFT operation obeys the scaling property and the principle of superposition; i.e., it is a linear operation. This is summarized in The Time-Delay Property When we first studied sinusoids, the phase was shown to depend on the time-shift of the signal. The simple relationship was \u201cphase equals the negative of frequency times time-shift.\u201d This concept carries over to the general case of the Fourier transform. The time-delay property of the DTFT states that time-shifting results in a phase change in the frequency domain: ",
        "590": "The reason that the delay property is so important and useful is that equation shows that multiplicative factors of the form in frequency-domain expressions always signify time delay. EXAMPLE:\u00a0 Delayed Sinc Function Let where is the sinc function of ; i.e., Using the time-delay property and the result for in, we can write down the following expression for the DTFT of with virtually no further analysis: Notice that the magnitude plot of is still a rectangle as in Fig.~\u203b(a); delay only changes the phase. To prove the time-delay property, consider a sequence, which we see is simply a time-shifted version of another sequence.",
        "591": "We need to compare the DTFT of vis-a-vis the DTFT of. By definition, the DTFT of is If we make the substitution for the index of summation in, we obtain Since the factor does not depend on and is common to all the terms in the sum on the right in, we can write as Therefore, we have proved that time-shifting results in a phase change in the frequency domain.",
        "592": "The Frequency-Shift Property Consider a sequence where the DTFT of is. The multiplication by a complex exponential causes a frequency shift in the DTFT of compared to the DTFT of. By definition, the DTFT of is If we combine the exponentials in the summation on the right side of, we obtain Therefore, we have proved the following general property of the DTFT:",
        "593": " ",
        "594": "DTFT of a Complex Exponential An excellent illustration of the frequency-shifting property comes from studying the DTFT of a finite-duration complex exponential. This is an important case for bandpass filter design and for spectrum analysis. In spectrum analysis, we would expect the DTFT to have a very large value at the frequency of the finite-duration complex exponential signal, and the frequency-shifting property makes it easy to see that fact.",
        "595": "Consider a length- complex exponential signal which is zero for and. An alternative representation for is the product of a length- rectangular pulse times the complex exponential. where the rectangular pulse is equal to one for, and zero elsewhere. To determine an expression for the DTFT of we use the DTFT of which is known to contain a Dirichlet form as in (also see Table \u203b on p. ). Figure~\u203b(a) shows the DTFT magnitude for a length-20 rectangular pulse, i.e.,. The peak value of the Dirichlet is at, and there are zeros at integer multiples of, when.",
        "596": "Then the DTFT of is obtained with the frequency shifting property: where is a frequency shifted version of the Dirichlet form Illustration of DTFT frequency shifting property. (a) DTFT magnitude for the length-20 rectangular window, (b) DTFT of a length-20 complex exponential with amplitude, and whose frequency is. Since the exponential terms in only contribute to the phase, this result says that the magnitude is a frequency shifted Dirichlet that is scaled by. For, and, the length-20 complex exponential, will have the DTFT magnitude shown in Fig.~\u203b(b). Notice that the peak of the shifted Dirichlet envelope is at the frequency of the complex exponential,. The peak height is the product of the Dirichlet peak height and the amplitude of the complex exponential,.",
        "597": "DTFT of a Real Cosine Signal A sinusoid is composed of two complex exponentials, so the frequency shifting property would be applied twice to obtain the DTFT. Consider a length- sinusoid",
        "598": " which we can write as the sum of complex exponentials at frequencies and as follows: ",
        "599": "Using the linearity of the DTFT and for the two frequencies leads to the expression where the function is the Dirichlet form in. In words, the DTFT is the sum of two Dirichlets: one shifted up to and the other down to.",
        "600": " DTFT of a length-20 sinusoid with amplitude, and frequency. Figure~\u203b shows as a function of for the case with and. The DTFT magnitude exhibits its characteristic even symmetry, and the peaks of the DTFT occur near. Furthermore, the peak heights are equal to approximately, which can be shown by evaluating for, and assuming that the value of is determined entirely by the first term in.",
        "601": "In Section~\u203b, we will revisit the fact that isolated spectral peaks are often indicative of sinusoidal signal components. Knowledge that the peak height depends on both the amplitude and the duration will be useful in interpreting spectrum analysis results for signals involving multiple frequencies.",
        "602": "Convolution and the DTFT Perhaps the most important property of the DTFT concerns the DTFT of a sequence that is the discrete-time convolution of two sequences. The following property says that the DTFT transforms convolution into multiplication. To illustrate the convolution property, consider the convolution of two signals, where a finite-length signal with three nonzero values and is a signal whose length maybe be finite or infinite. When the signal is convolved with we can write If we then take the DTFT of, we obtain where the delay property applied to a term like will create the term. Then we observe that the term on the right-hand side of can be factored out to write",
        "603": " }} H(e^{j and, therefore, we have the desired result which is multiplication of the DTFTs as asserted in. }} H(e^{j Filling in the signal values for in this example we obtain }} H(e^{j",
        "604": "The steps above do not depend on the numerical values of the signal, so a general proof could be constructed along these lines. In fact, the proof would be valid for signals of infinite length, where the limits on the sum in would be infinite. The only additional concern for the infinite-length case would be that the DTFTs and must exist as we have discussed before. EXAMPLE:\u00a0 Frequency Response of Delay The delay property is a special case of the convolution property of the DTFT. To see this, recall that we can represent delay as the convolution with a shifted impulse so the impulse response of a delay system is. The corresponding frequency response (i.e., DTFT) of the delay system is Therefore, using the convolution property, the DTFT of the output of the delay system is which is identical to the delay property of. Filtering is Convolution The convolution property of LTI systems provides an effective way to think about LTI systems. In particular, when we think of LTI systems as \u201cfilters\u201d we are thinking of their frequency responses, which can be chosen so that some frequencies of the input are blocked while others pass through with little modification. As we discussed in Sect.~\u203b, the DTFT plays the role of spectrum for both finite-length signals and infinite-length signals",
        "605": "The convolution property reinforces this view. If we use to write the expression for the output of an LTI system using the DTFT synthesis integral, we have which can be approximated with a Riemann sum as in Sect.~\u203b Now we see that the complex amplitude at each frequency is modified by the frequency response of the system evaluated at the given frequency. This is exactly the same result as the sinusoid-in gives sinusoid-out property that we saw in Ch. 6 for case where the input is a discrete sum of complex exponentials. In Sect. \u203b, we will expand on this idea and define several ideal frequency-selective filters whose frequency responses are ideal versions of filters that we might want to implement in a signal processing application. Energy Spectrum and the Autocorrelation Function An important result in Fourier transform theory is Parsevals Theorem: The lefthand side of is called the energy in the signal; it is a scalar. Thus the righthand side of is also the energy, but the DTFT shows how the energy is distributed versus frequency. Therefore, the DTFT is called the magnitude-squared spectrum or the energy spectrum of. If we first define the energy of a signal as the sum of the squares then the energy is a single number that is often a convenient measure of the size of the signal.",
        "606": "EXAMPLE:\u00a0 Energy of the Sinc Signal The energy of the sinc signal (evaluated in the time domain) is While it is impossible to evaluate this sum directly, application of Parsevals theorem yields because the DTFT of the sinc signal is one for. A simple interpretation of this result is that the energy is proportional to the bandwidth of the sinc signal and evenly distributed in across the band. Autocorrelation Function The energy spectrum is the Fourier transform of a time-domain signal which turns out to be the autocorrelation of. The autocorrelation function is widely used in signal detection applications. Its usual definition is equivalent to the following convolution operation: By making the substitution for the dummy index of summation, we can write which is the basic definition of the autocorrelation function for. Observe that in, the index serves to shift with respect to when both sequences are thought of as functions of. It can be shown that is maximum at ; i.e., when is perfectly aligned with. The independent variable in is often called the \u201clag\u201d by virtue of its meaning as a shift between two copies of the same sequence. From it follows that ; i.e., the energy of a signal is the value of its autocorrelation function at.",
        "607": "Determine and plot the autocorrelation function for the signal. In we use the time-reversed signal. Show that the DTFT of is. In Ch. 6 we saw that the frequency response for a real impulse response must be conjugate symmetric. Since the frequency response function is a DTFT, it must also be true that the DTFT of a real signal is conjugate symmetric. Show that if is real,. Using the results of Exercises~\u203b and \u203b for real signals, the DTFT of the autocorrelation function is A useful relationship results if we represent in terms of its inverse DTFT; i.e., If we evaluate both sides of at, we see that the energy of the sequence can also be computed from as follows: Finally, equating and we obtain Parsevals Theorem.",
        "608": "Ideal Filters In any practical application of LTI discrete-time systems, the frequency response function would be derived by a filter design procedure that would yield an LTI system that could be implemented with finite computation. However, in the early phases of the system design process it is common practice to start with ideal filters that have simple frequency responses that provide ideal frequency-selectivity. Ideal Lowpass Filter An ideal lowpass filter (LPF) has a frequency response that consists of two regions: the passband near (DC) where the frequency response is one, and the stopband away from, where it is zero. Frequency response of an ideal LPF with its cutoff at,rad/s. Recall that must also be periodic with period. An ideal lowpass filter is therefore defined as The frequency is called the cutoff frequency of the LPF passband. Figure~\u203b shows a plot of for the ideal LPF. The shape is rectangular and is even symmetric about. As discussed in Section~\u203b, this property is needed because real-valued impulse responses lead to filters with a conjugate symmetric frequency responses. Since has zero phase, it is real-valued, so being conjugate symmetric is equivalent to being an even function.",
        "609": "In Chapter 6 we saw that the frequency response for a real impulse response must be conjugate symmetric. Show that a frequency response defined with linear phase is conjugate symmetric, which would imply that its inverse DTFT is real. The impulse response of the ideal lowpass filter, found by applying the DTFT pair in, is a sinc function form The ideal lowpass filter is impossible to implement because the impulse response is non-causal and, in fact, has nonzero values for large negative indices as well as large positive indices. However, that does not invalidate the ideal lowpass filter concept; i.e., the idea of selecting the low frequency band and rejecting all other frequencies. Even the moving average filter discussed in Section~\u203b might be a satisfactory lowpass filter in some applications.",
        "610": "Figure~\u203b shows the frequency response of an 11-point moving average filter.",
        "611": " Magnitude response of an 11-point moving average filter. (Also shown in full detail in Figure~\u203b.) Note that this causal filter has a low-pass-like frequency response magnitude, but it is far away from zero in what might be considered the stopband. In more stringent filtering applications, we need better approximations to the ideal characteristic. In a practical application, the process of filter design involves mathematical approximation\t of the ideal filter with a frequency response that is close enough to the ideal frequency response while corresponding to an implementable filter.",
        "612": "The following example shows the power of the transform approach when dealing with filtering problems.",
        "613": "EXAMPLE:\u00a0 Ideal Lowpass Filtering Consider an ideal lowpass filter with frequency response given by and impulse response in. Now suppose that the input signal to the ideal LPF is a bandlimited sinc signal Working in the time domain, the corresponding output of the ideal LPF would be given by the convolution expression Evaluating this convolution directly in the time domain is impossible both analytically and via numerical computation. However, it is straightforward to obtain the filter output if we use the DTFT because in the frequency domain the transforms are rectangles and they are multiplied. From, the DTFT of the input is Therefore, the DTFT of the ideal filters output is, which would be of the form",
        "614": " When multiplying the DTFTs to get the right-hand side of, the product of the two rectangles is another rectangle whose width is the smaller of smaller of and, so the bandlimit frequency is ",
        "615": "Since we want to determine the output signal, we must take the inverse DTFT of. Thus, using to do the inverse transformation, the convolution in evaluates to another sinc signal The result given and is easily seen from a graphical solution that shows the rectangular shapes of the DTFTs. With and, sketch plots of from and in on the same set of axes and then verify the result in. We can generalize the result of Example~\u203b in several interesting ways. First, when we can see that the output is the impulse response of the ideal LPF, so the input in a sense acts like an impulse to the ideal LPF. Furthermore, for ideal filters where a band of frequencies is completely removed by the filter, many different inputs could produce the same output. Also, we can see that if the bandlimit of the input to an ideal LPF is less than the cutoff frequency, i.e.,, then the input signal will pass through the filter unchanged. Finally, if the input consists of a desired bandlimited signal plus some sort of competing signal such as noise whose spectrum extends over the entire range, then if the signal spectrum is concentrated in a band, it follows by the principle of superposition that an ideal LPF with cutoff frequency will pass the desired signal without modification while removing all frequencies in the spectrum of the competing signal above the cutoff frequency. This is often the motivation for using a LPF. Ideal Highpass Filter The ideal highpass filter (HPF) has its stopband centered on low frequencies, and its passband extends from out to. (The highest normalized frequency in a sampled signal is of course.) Figure~\u203b shows an ideal HPF with its cutoff frequency at rad/s.",
        "616": " Frequency response of an ideal HPF with cutoff at rad/s. In this case, the high frequency components of a signal will pass through the filter unchanged while the low frequency components will be completely eliminated. Highpass filters are often used to remove constant levels (DC) in sampled signals. Like the ideal LPF, we should define the ideal highpass filter with conjugate symmetry so that the corresponding impulse response will be a real function of time.",
        "617": "If is an ideal LPF with its cutoff frequency at as plotted in Fig.~\u203b, show that the frequency response of an ideal highpass filter with cutoff frequency can be represented by Hint: Try plotting the function. Using the results of Exercise~\u203b, show that the impulse response of the ideal highpass filter is Ideal Bandpass Filter The ideal bandpass filter has a passband centered away from the low frequency band, so it has two stopbands, one near DC and the other at high frequencies. Two cutoff frequencies must be given to specify the ideal BPF, for the lower cutoff, and for the upper cutoff. That is, the ideal bandpass filter has frequency response Figure~\u203b shows an ideal BPF with its cutoff frequencies at and. Once again we use a symmetrical definition of the passbands and stopbands which is required to make the corresponding impulse response real.",
        "618": " Frequency response of an ideal BPF with its passband from to rad/s. In this case, all frequency components of a signal that lie in the band are passed unchanged through the filter, while all other frequency components are completely removed.",
        "619": "If is an ideal BPF with its cutoffs at and, show by plotting that the filter defined by could be called an ideal band-reject filter. Determine the edges of the stopband of the band-reject filter.",
        "620": "Practical FIR Filters Ideal filters are useful concepts, but not practical since they cannot be implemented with a finite amount of computation. Therefore, we perform filter design to approximate an ideal frequency response to get a practical filter. For FIR filters, a filter design method must produce filter coefficients for the time-domain implementation of the FIR filter as a difference equation The filter coefficients are the values of the impulse response, and the DTFT of the impulse response determines the actual magnitude and phase of the designed frequency response, which can then be assessed to determine how closely it matches the desired ideal response. There are many ways to approximate the ideal frequency response, but we will concentrate on the method of windowing which can be analyzed via the DTFT. Windowing The concept of windowing is widely used in signal processing. The basic idea is to extract a finite section of a very long signal via multiplication. This approach works if the window function is zero outside of a finite-length interval. In filter design the window will truncate the infinitely long ideal impulse response, and then modify the truncated impulse response. The simplest window function is the -point rectangular window which is the same as the rectangular pulse studied in Sect. \u203b. Multiplying by a rectangular window will only truncate a signal.",
        "621": "The important idea of windowing is that the product will extract values from the signal starting at. Thus the following are equivalent The name window comes from the idea that we can only \u201csee\u201d values of the signal within the window interval when we \u201clook\u201d through the window. Multiplying by is looking through the window. When we change, the signal shifts, and we see a different length- section of the signal.",
        "622": "The nonzero values of the window function do not have to be all ones, but they should be positive. For example, the -point Hamming window is defined as The MATLAB function hamming(L) will generate a vector with values given by. The stem plot of the Hamming window in Fig.~\u203b shows that the values are larger in the middle and taper off near the ends. Length-21 Hamming window is nonzero only for. The window length can be even or odd, but an odd-length Hamming window is easier to characterize. Its maximum value is 1.0 which occurs at the midpoint index location, and the window is symmetric about the midpoint, with even symmetry because. Filter Design Ideal Filters are given by their frequency response, consisting of perfect passbands and stopbands. The ideal filters cannot be FIR filters because there is no finite set of filter coefficients whose DTFT will be the ideal frequency response. Recall that the impulse response of the ideal LPF is an infinitely long sinc function as shown by the following DTFT pair: where is the cutoff frequency of the ideal LPF, which separates the passband from the stopband. The sinc function is infinitely long.",
        "623": "Window the Ideal Impulse Response In order to make a practical FIR filter we can multiply the sinc function by a window to produce a length- impulse response. However, we must also shift the sinc function so that its main lobe is in the center of the window, because intuitively we should use the largest values from the ideal impulse response. From the shifting property of the DTFT, the time shift of will introduce a linear phase in the DTFT. Thus the impulse response obtained from windowing is the following where is the window, either rectangular or Hamming. Since the nonzero domain of the window starts at, the resulting FIR filter will be casual. We usually say that the practical FIR filter has an impulse response that is a windowed version of the ideal impulse response. Impulse Responses for (a) Length-25 LPF with rectangular window, i.e., truncated sinc. (b) Length-25 LPF using 25-pt Hamming window multiplied by a sinc function. The windowing operation is shown in Fig.~\u203b(a) for the rectangular window which truncates the ideal impulse response to length-. In Fig.~\u203b(b) the Hamming windowed impulse response results from truncating the ideal impulse response, and also weighting the values to reduce the ends more than the middle. The midpoint value is preserved, i.e.,, while the first and last points are 8 of their original values, e.g.,. A continuous outline of the Hamming window is drawn in (a) to show the weighting that will be applied to the truncated ideal impulse response. The benefit of using the Hamming window will come from the fact that it smoothly tapers the ends of the truncated ideal lowpass impulse response.",
        "624": "Frequency Response of Practical Filters A causal length- Practical Filter is useful if its frequency response closely approximates the desired frequency response of an ideal filter. Although it is possible to take the DTFT of a windowed sinc, the resulting formula is quite complicated and does not offer much insight. Instead we can evaluate the frequency reponse directly using MATLABs freqz function because we have a simple formula for the filter coefficients.",
        "625": "Figure~\u203b(a) shows the magnitude response for an FIR filter whose impulse response is a length-25 truncated ideal impulse response with. The passband of the actual filter is not flat but it oscillates above and below the desired passband value of one; the same behavior is exhibited in the stopband. These passband and stopband ripples are usually observed for practical FIR filters that approximate ideal LPFs, and they are particularly noticeable with the rectangular window.",
        "626": " Frequency response magnitudes for (a) Length-25 LPF with rectangular window, i.e., truncated sinc. (b) Length-25 LPF using 25-pt Hamming window. The phase response of both of these filters is a linear phase with slope. Ideal LPF with is shown in gray. Figure~\u203b(b) shows the magnitude response for an FIR filter whose impulse response is a length-25 Hamming-windowed sinc. In this case, the ripples are not visible on the magnitude plot because the scale is linear and the ripples are tiny, less than 0.003. In terms of approximating the value of one in the passband and zero in the stopband, the Hamming-windowed ideal lowpass filter is much better. However, this improved approximation comes at a cost\u2013-the edge of the passband near the cutoff frequency has a much lower slope. In filter design, we usually say it \"falls off more slowly\" from the passband to the stopband. Before we can answer the question of which filter is better we must decide whether ripples are more important than the fall-off rate from passband to stopband, or vice versa. Passband Defined for the Frequency Response Frequency-selective digital filters, e.g., LPFs, BPFs and HPFs, have a magnitude response that is close to one in some frequency regions, and close to zero in others. For example, the plot in Fig.~\u203b(a) is a lowpass filter whose magnitude is within (approximately) of one when.",
        "627": " LPF template showing passband and stopband ripple tolerances along with the transition zone. (a) Length-25 LPF with rectangular window, i.e., truncated sinc. (b) Length-25 LPF using 25-pt Hamming window. Only the positive half of the frequency axis is shown, because the magnitude response is an even function whenever the impulse response is real. This region where the magnitude is close to one is called the passband of the filter. It will be useful to have a precise definition of the passband edges, so that the passband width can be measured and we can compare different filters.",
        "628": "From a plot of the magnitude response, e.g, via freqz in MATLAB, it is possible to determine the set of frequencies where the magnitude is very close to one, as defined by being less than, which is called the passband ripple. This set of frequencies in a passband should be a region of the form. A common design choice for the desired passband ripple is a value between 0.01 and 0.1, i.e., 1 to 10.",
        "629": "For a lowpass filter, the passband region extends from to, where the parameter is called the passband edge. For the two LPFs shown in Fig.~\u203b, we can make an {accurate measurement} of and from the zoomed plots in Fig.~\u203b. For the rectangular window case, a careful measurement gives a passband ripple size of, with the passband edge at. For the Hamming window case, we need the zoomed plot of the passband region to see the ripples, as in Fig.~\u203b(b).",
        "630": " Blowup of passband and stopband ripples for (a) Length-25 LPF with rectangular window, i.e., truncated ideal lowpass impulse response. (b) Length-25 LPF using 25-pt Hamming window. The error between the actual magnitude response and the ideal is shown, e.g.,. The ripples for the Hamming case are more than 30 times smaller. Then we can measure the passband ripple to be for the Hamming window case\u2013-more than thirty times smaller. Once we settle on the passband ripple height, we can measure the passband edge; in Fig.~\u203b(b) it is. Notice that the actual passband edges are not equal to the design parameter which is called the cutoff frequency. There is sometimes a confusion when the terminology \u201cpassband cutoff frequency\u201d is used to mean passband edge, which then implies that and might be the same, but after doing a few examples it should become clear that this is never the case.",
        "631": "Stopband Defined for the Frequency Response When the frequency response (magnitude) of the digital filter is close to zero, we have the stopband region of the filter. The stopband will be a region of the form, if the magnitude response of a LPF is plotted only for nonnegative frequencies. The parameter is called the stopband edge. In the rectangular window LPF example of Figs.~\u203b(a) and \u203b(a), the magnitude is close to zero when, i.e., high frequencies. The stopband ripple for this region is expected to be less than 0.1, and is measured to be.",
        "632": "We can repeat this process for the Hamming window LPF using Figs.~\u203b(b) and \u203b(b) to determine the stopband ripple, and then the set of frequencies where the magnitude is less than. The result is a stopband ripple measurement of 0.0031, and a corresponding stopband edge of.",
        "633": "Transition Zone of the LPF Unlike an ideal lowpass filter where the stopband begins at the same frequency where the passband ends, in a practical filter there is always a nonzero difference between the passband edge and the stopband edge. This difference is called the transition width of the filter:. The smaller the transition width, the better the filter because it is closer to the ideal filter which has a transition width of zero. For the two lowpass filters in Fig.~\u203b the measured transition widths are",
        "634": " EXAMPLE:\u00a0 Decrease Transition Width One property of the transition width is that it can be controlled by changing the filter order. There is an approximate inverse relationship, so doubling the order should reduce the transition width by roughly one half. We can test this idea on the length-25 rectangular window LPF in Fig.~\u203b(b) which has an order equal to 24. If we design a new LPF that has the same cutoff frequency,, but twice the order, i.e.,, then we can repeat the measurement of the bandedges, and the transition width. The result is and, so. Comparing to the value of above, the ratio is. Doubling the order once more to gives a transition width of, so the ratio is. For the Hamming window case, the measured transition width for is, and for,. The ratios are which matches the approximate inverse relationship expected. should be approximately. When comparing the transition widths in and, we see that, so the transition width of the Hamming window LPF is more than three times larger for. This empirical observation confirms the statement, \u201cwhen comparing equal-order FIR filters that approximate a LPF, the one with larger transition width will have smaller ripples.\u201d However, this statement does not mean that the ripples can be reduced merely by widening the transition width. Within one window type, such as Hamming window filters, changing the transition width will not change the ripples by more than a few percent. ",
        "635": "Summary of Filter Specifications The foregoing discussion of ripples, bandedges, and transition width can be summarized with the tolerance scheme shown in Fig. \u203b. The filter design process is to approximate the ideal frequency response very closely. Once we specify the desired ripples and bandedges, we can draw a template around the ideal frequency response. The template should, in effect, give the tradeoff between ripple size and transition width. Then an acceptable filter design would be any FIR filter whose magnitude response lies entirely within the template. The Hamming window method is just one possible design method among many that have been developed. GUI for Filter Design The SP-First GUI called filterdesign illustrates several filter design methods for LPF, BPF and HPF filters. The interface is shown in Fig.~\u203b. Both FIR and IIR filters can be designed, but we will only be interested in the FIR case which would be selected with the FIR button in the upper right. The default design method is the Window Method using a Hamming window. The window type can be changed by selecting another window type from the drop-down list in the lower right. To specify the design it is necessary to set the order of the FIR filter and choose one or more cutoff frequencies; these parameters can be entered in the edit boxes.",
        "636": " Interface for the filterdesign GUI. When the Filter Choice is set to FIR, many different window types can be selected, including the Hamming window and the Rectangular window (i.e., only truncation to a finite length). The specification of one or more cutoff frequencies must be entered using continuous-time frequency (in Hz), along with a sampling rate (, also in Hz). In normalized frequency, the cutoff frequency is. The plot initially shows the frequency response magnitude on a linear scale, with a frequency axis in Hz. Clicking on the word Magnitude will toggle the magnitude scale to a log scale in dB. Clicking on the word Frequency will toggle the frequency axis to normalized frequency, and also let you enter the cutoff frequency using. Recall that. The plotting region can also show the phase response of, or the impulse response of the filter. Right click on the plot region to get a menu. The Options menu provides zooming and a grid via Options->Zoom and Options->Grid.",
        "637": "The filter coefficients can be \u201cexported\u201d from the GUI by using the menu File->Export Coeffs. To make some filters for comparison, redo the designs in Fig.~\u203b and export the filter coefficients to the workspace under unique names. Then you can make your own plot of the frequency response in MATLAB using the freekz function (or freqz) followed by a plot command. For example, an interesting activity would be to design the filters in Fig.~\u203b to check the measurements of ripples and transition width.",
        "638": "Table of Fourier Transform Properties and Pairs In this chapter, we have derived a number of useful transform pairs, and we have also derived several important properties of Fourier transforms. Table~\u203b includes the Fourier transform pairs that we have derived in this chapter as well as one transform pairs (the left-sided exponential) that we did not derive. Basic discrete-time Fourier transform pairs. Table of DTFT Pairs Time-Domain: Frequency-Domain: 1 The basic properties of the Fourier transform are what make it convenient to use in designing and analyzing systems, so they are given in Table~\u203b on p.~ for easy reference. Basic discrete-time Fourier transform properties. Table of DTFT Properties Property Name Time-Domain: Frequency-Domain: Periodic in Linearity Conjugate Symmetry is real Conjugation Time-Reversal Delay Frequency Shift Modulation Convolution Autocorrelation Parsevals Theorem ",
        "639": "Summary and Links ",
        "640": "In this chapter we introduced the discrete-time Fourier transform (DTFT), and developed some of its basic properties for understanding the behavior of linear systems. The DTFT provides a frequency-domain representation for signals as well as systems, and like other Fourier transforms it generalizes the idea of a spectrum for a discrete-time signal. We obtained the DTFT by generalizing the concept of the frequency response, and showed how the inverse transform could be used to obtain the impulse response of various ideal filters. Also it is not surprising that the DTFT plays an important role in filter design for methods based on rectangular and Hamming windowing.",
        "641": "CHAPTER 8 The Discrete Fourier Transform This chapter builds on the definition and discussion of the DTFT in Chapter \u203b. The objective here is to define a numerical Fourier transform called the discrete-Fourier transform (or DFT) that results from taking frequency samples of the DTFT. We will show how the DFT can be used to compute a spectrum representation of any finite-length sampled signal very efficiently with the Fast Fourier Transform (FFT) algorithm. The DFT not only gives a spectrum representation of any finite-length sequence, but it can also be used as a representation of a periodic sequence formed by infinite repetition of one (finite-length) period. Finally, the DFT is the core computation needed in the spectrogram which provides a time-frequency spectrum analysis of a very long signal by doing DFTs of successive short sections extracted from the long signal.",
        "642": "It is safe to say that spectrum analysis is one of the most common operations used in the practice of signal processing. Sooner or later, most scientists and engineers will encounter a situation where a \u201csampled data signal\u201d has been obtained from an A-to-D converter and must be analyzed to determine its spectral properties. Often, spectrum analysis is used to discover whether or not the signal contains strong periodic components. A listing of natural phenomena that are cyclic, or nearly so, would contain hundreds of entries. Common examples include speech and music signals, as well as other natural observations such as tides and yearly sun spot cycles. In the case of sampled signals, the question of interest is how to derive the spectrum by doing numerical operations on a signal that is defined only by a finite set of numbers. This process of going from the sampled signal to its spectrum is called discrete-time spectrum analysis and the DFT is its workhorse operator.",
        "643": "Discrete Fourier Transform (DFT) The DTFT of a discrete-time signal can be viewed as a generalization of the spectrum concept introduced in Chapters~\u203b and \u203b where discrete lines in the frequency domain represented sums of complex exponentials in the time domain. The spectrum concept obtained from the DTFT is a continuous function of frequency, and its value as a Fourier transform is easiest to appreciate when and are defined as mathematical expressions. Because the spectrum is so useful, however, it is natural to want to be able to determine the spectrum of a signal even if is not described by a simple mathematical formula. Indeed, it would be handy to have a computer program that could calculate the spectrum from samples of a signal.",
        "644": "Two steps are needed to change the DTFT sum into a computable form: the continuous frequency variable must be sampled, and the limits on the DTFT sum must be finite. First, even though is a continuous variable, it does have a finite range so we can evaluate at a finite set of frequencies, denoted by.",
        "645": "Second, the DTFT sum will have a finite number of terms when the signal duration is finite. We cannot compute the transform of an infinite-duration signal, but it is common to operate on finite sections of a very long signal.",
        "646": "For a finite-duration signal the DTFT sampled in frequency becomes",
        "647": " We must choose a specific set of frequency samples, but which frequencies should be used to evaluate ? The usual domain for the spectrum is, but any interval of length would suffice.",
        "648": "For reasons that will become apparent, it is common to choose that interval to be and to evaluate at the equally spaced frequencies Note that this set of frequencies covers the range since would give, which is an alias frequency of. Substituting into gives frequency samples of the DTFT for. Equation is a \u201cfinite Fourier sum\u201d which is computable. The sum in must be computed for different values of the discrete frequency index. The index in the sum on the right is the counting index of the sum, and thus it disappears when the sum is computed. Since the left-hand side of depends only on frequency index we define.",
        "649": "When the number of frequency samples is equal to the signal length, the summation in with becomes: Equation is called the discrete Fourier transform or DFT in recognition of the fact that it is a Fourier transformation, and it is discrete in both time and frequency. The DFT takes samples in the time-domain and transforms them into values in the frequency-domain.",
        "650": "Typically, the values of are complex, while the values of are often real, but in could be complex.",
        "651": "EXAMPLE:\u00a0 Short-Length DFT In order to compute the 4-point DFT of the sequence, we carry out the sum four times, once for each value of. When, all the exponents in will be integer multiples of because. Thus we obtain the four DFT coefficients. Example \u203b illustrates an interesting fact about the limits of summation in where we have chosen to sum over samples of and to evaluate the DFT at frequencies. Sometimes, the sequence length of is shorter than, i.e.,, and is nonzero only in the interval. In Example \u203b, and. In such cases, we can simply append zero samples to the nonzero samples of and then carry out the -point DFT computation. These zero samples extend a finite-length sequence, but do not change its nonzero portion. The Inverse DFT The DFT is a legitimate transform because it is possible to invert the transformation defined in. In other words, there exists an inverse discrete Fourier transform (or IDFT), which is a computation that converts for back into the sequence for. The inverse DFT is Equations and define the unique relationship between an -point sequence and its -point DFT. Following our earlier terminology for Fourier representations, the DFT defined by is the analysis equation and IDFT defined by is the synthesis equation. To prove that these equations are a consistent invertible Fourier representation, we note that, because it is a finite, well-defined computation, would surely produce some sequence when evaluated for. So let us call that sequence until we prove otherwise. Part of the proof is given by the following steps:",
        "652": " }} e^{j(2 k/N) n}} &= { {x}[m] k/N) (n-m)} except for }m=n} } &= {x}[n] Several things happened in the manipulations leading up to the equality assertion of. On the second line, we substituted the right-hand side of for after changing the index of summation from to. We are allowed to make this change because is a \u201cdummy index\u201d, and we need to reserve for the index of the sequence that is synthesized by the IDFT. On the third line, the summations on and were interchanged. This is permissible since these finite sums can be done in either order.",
        "653": "Now we need to consider the term in parenthesis in. Exercise \u203b (below) states the required orthogonality result, which can be easily verified. If we substitute into the third line, we see that the only term in the sum on that will be nonzero is the term corresponding to. Thus for as we wished to show.",
        "654": "Orthogonality Property of Periodic Discrete-Time Complex Exponentials Use the formula to show that",
        "655": " where is any positive or negative integer including. EXAMPLE:\u00a0 Short-Length IDFT The 4-point DFT in Example \u203b is the sequence. If we compute the 4-point IDFT of this, we should recover when we apply the IDFT summation for each value of. As before, the exponents in will all be integer multiples of when. Thus we have verified that the length-4 signal can be recovered from its 4-point DFT coefficients,. DFT Pairs from the DTFT Since the DFT is a frequency sampled version of the DTFT for a finite-length signal, it is possible to construct DFT pairs by making the substitution. In Table \u203b, there are only four finite-length signals, so each of these has a DFT obtained by sampling. DFT of Shifted Impulse The impulse signal and the shifted impulse signal are the easiest cases because they are very simple signals. If we take the DFT of, the DFT summation simplifies to one term: It is tempting to develop all DFT pairs by working directly with the DFT summation, but it is much easier to get some pairs by frequency sampling known DTFTs. Recall that we already know the DTFT of the shifted impulse, so ",
        "656": "When the shift is zero in, the DFT of is, as before. We want to write as the pair, but since frequency sampling the DFT requires that the finite-length signal be defined over the interval, for now we must require that lie in that interval. In fact, we can remove this restriction, as will be discussed in Section \u203b. DFT of Complex Exponential The third and fourth cases involve the finite-length (rectangular) pulse whose DTFT involves a Dirichlet form which is in Ch. \u203b. Sampling the known DTFT will avoid reworking a messy summation. The third case is the finite-length (rectangular) pulse r_{L}[n] = The big simplification comes from the fact that the Dirichlet evaluated at integer multiples of is zero, except for, so we get The scaled discrete impulse at means that and all other DFT coefficients are zero. This result is confirmed in Fig.~\u203b, where we can see that the DFT is obtained by sampling the gray Dirichlet envelope exactly at its peak and at its zero crossings. The peak value is, and it is the only nonzero value in the DFT.",
        "657": " Magnitude of DFT coefficients for a 20-pt. DFT of a length-20 complex exponential whose frequency is which is an integer multiple of ; i.e., and. Substitute into the inverse -point DFT relation to show that the corresponding time-domain sequence is Computing the DFT The DFT representation in (\u203b) and (\u203b) is exceedingly important in digital signal processing for two reasons: the expressions have finite limits, making it possible to compute numeric values of and, and they are Fourier representations that have special properties like the DTFT that are useful in the analysis and design of DSP systems. Both the DFT and the IDFT summations can be regarded as computational methods for taking numbers in one domain and creating numbers in the other domain. The values in both domains might be complex. Equation, for example, is really separate summations, one for each value of. To evaluate one of the terms, we need complex additions and complex multiplications when. If we count up all the arithmetic operations required to evaluate all of the coefficients, the total is complex multiplications and complex additions. For example, when as in Example \u203b, the 9 complex multiplications and 12 complex additions are clearly shown. Note that terms involving do not require an actual multiplication. For the inverse transform, the multiplication by would require an additional 4 multiplications if done separately as shown in Example \u203b.",
        "658": "One of the most important discoveries in the field of digital signal processing was the fast Fourier transform, or FFT, a set of algorithms that can evaluate or with a number of operations proportional to rather than.",
        "659": " When is a power of two, the FFT algorithm computes the entire set of coefficients with approximately complex operations. The behavior becomes increasingly significant for large. For example, if, the FFT will compute the DFT coefficients with complex multiplications, rather than as required by direct evaluation of. The algorithm is most often applied when the DFT length is a power of two, but it also works efficiently if has many small-integer factors. On the other hand, when is a prime number, the standard FFT algorithm offers no savings over a direct evaluation of the DFT summation. FFT algorithms of many different variations are widely available in most computer languages, and for almost any computer hardware architecture. In MATLAB, the command is simply fft, and most other spectral analysis functions in MATLAB call fft to do the bulk of their work. The DFT of a vector x is computed using the statement X = fft( x, N ), where X is the DFT of x. MATLAB uses a variety of FFT algorithms for this computation depending on the value of, with the best case being equal to a power of 2. More details on the FFT and its derivation can be found in Section~\u203b at the end of this chapter. Matrix Form of the DFT and IDFT Another easy way to gain insight into the computation is to write the DFT summation as a matrix-vector multiplication, where the signal values and DFT coefficients become -element column vectors: In MATLAB, the DFT matrix can be obtained with the function dftmtx(N) for an matrix. Then taking the DFT would be a matrix-vector product: X = dftmtx(N)*x, where x is the vector of signal samples and X the vector of DFT coefficients. However, it is much more efficient to take the DFT of a vector using the statement X = fft( x, N ), where X is the DFT of x. MATLAB uses a variety of FFT algorithms for this computation depending on the value of with the best case being equal to a power of 2.",
        "660": "The IDFT can also be expressed as a matrix-vector product. Write out the typical entry of the IDFT matrix, and then use MATLAB to create a IDFT matrix. Check your work by multiplying the IDFT matrix by the DFT matrix (in MATLAB). Explain why the expected result should be an identity matrix.",
        "661": "Inherent Periodicity of in the DFT In this section we will study more properties of the DFT/IDFT. In the previous section, we examined properties of the DFT which are are tied to its interpretation as a frequency sampled version of the DTFT, and the fact that is periodic. This periodicity affects the placement of negative frequency components in, and also symmetries. In this section, we want to show that the IDFT summation requires that the signal must be periodic with period. This property might be surprising because the DFT has been defined for a finite-length -point sequence, and the DFT yields DFT coefficients. However, some transform properties implicitly involve operations that evaluate the indices or outside of, so periodicity is needed to explain properties such as the delay property and convolution.",
        "662": "DFT Periodicity for It is less obvious that the IDFT summation will imply periodicity.",
        "663": "However, we can ask what happens when the IDFT sum is evaluated for or.",
        "664": "In particular, consider evaluating at where is in the interval ; i.e., We have denoted the result in as because we are testing to see what value is computed at.",
        "665": " In observe that for all.",
        "666": "The result in says that the IDFT does not give a value of zero at, as it would if the IDFT returned the original finite-length sequence with values outside the interval. Instead, it repeats values of from within the interval. By the same process it can be shown that when we evaluate at any point where is an integer. In other words, the result from the IDFT sum is periodic in with period if is evaluated outside the base interval. We can express this succinctly as follows: where is the original sequence whose DTFT was sampled as in. The infinite sum on the right-hand side, which involves shifted copies of the same signal, is illustrated in Fig. \u203b. Even though the discussion leading to seems to assume that is zero outside the interval, it turns out that the relationship in is true for any signal that has a DTFT.",
        "667": " Illustration of the sum in. (a) A finite-length sequence of length 10. (b) Shifted copies of that are summed to make up the inherent periodic sequence with period 10. Equation is an exceedingly important observation about the IDFT because it provides the answer to the question: \u201cwhen can a sequence be reconstructed exactly from samples of its DTFT ?\u201d From it is clear that if for outside the base interval, then for. That is, the sequence can be reconstructed exactly from samples of its DTFT if it is a finite length- sequence. This could be termed the Sampling Theorem for the DTFT.",
        "668": "The inherent periodicity of the DFT/IDFT representation in both and forces us to interpret some of the familiar properties of Fourier representations in a special way. Specifically, we must never lose sight of the periodic sequence that is inherently represented by the DFT/IDFT representation. This is particularly important when considering signal operations such as delay and convolution. A good example of the time periodicity issue comes from reconsidering the DFT pair for a shifted impulse where we want to write the pair as. In this case, the IDFT periodicity issue arises when, as the following example illustrates.",
        "669": "EXAMPLE:\u00a0 DFT of Shifted Impulse Consider the 10-pt DFT of which should be by virtue of the DFT pair given in. If we take the 10-point IDFT of we will get a length-10 signal which is defined over the time index range. Here is one way to take the IDFT: Thus, the result of the IDFT has a nonzero value at, and seems to be different from which was nonzero at. For the 10-point DFT of, we can only take the DFT if we extend to a periodic signal that has impulses at. Mathematically, we would write which writes the periodic signal in the form of.",
        "670": "The Time Delay Property for the DFT As we showed in Chapter \u203b, the DTFT delay property is when is a time-shifted version of. If we use frequency sampling, then we expect the DFT delay property would be However, when and are used in the IDFT, it is the inherent periodic signal that is shifted by to give the inherent periodic signal. If we take the point of view that we only compute sequence values in the interval, then we are led to an unexpected result. We can see that this might be true by noting that if is nonzero over the entire interval, then there would not be room in that interval for. This is similar to the behavior already seen in Example \u203b for a time-shifted impulse. For example, if, then is nonzero for. This is illustrated in Fig. \u203b for the case.",
        "671": " Illustration of the time-shift property of the DFT: (a) A finite-length sequence of length 10. (b) The inherent periodic sequence for a 10-point DFT representation. (c) Time-shifted periodic sequence which is also equal to the IDFT of. (d) The sequence obtained by evaluating the 10-point IDFT of only in the interval. Figure \u203b(a) shows an original 10-point sequence. If we represent this sequence by its 10-point DFT, then we are implicitly representing the periodic sequence shown in Fig.~ \u203b(b), and if we form, then the corresponding periodic sequence is as shown in Fig.~\u203b(c). The open dots on dashed lines depict the periodic extension of the sequence in the base interval, which is shown with solid dots and lines. Finally, if we form and then use the 10-point IDFT to compute the sequence for, we obtain the result of Fig.~\u203b(d).",
        "672": "Comparing Figs. \u203b(a) and (d), it is clear that, however, the four samples on the left side of were originally the four samples on the right side of. Thus, is related to by a shift that is sometimes called \u201ccircular\u201d since the shifted samples appear to \u201crotate\u201d or \u201cwrap around\u201d within the base interval. This effect is often called time aliasing in recognition of the close analogy with the frequency aliasing that occurs in sampling continuous-time signals. A convenient way to represent this circular shift is through the use of modular arithmetic. ",
        "673": "EXAMPLE:\u00a0 Modulo- Arithmetic In number theory, a consistent algebraic system can be defined using remainders with respect to a fixed integer, called the modulus. Recall that any integer can be written uniquely as where the quotient is an integer and the remainder is nonnegative and less than the modulus. We write to denote the remainder of modulo-. For example, is equal to 8 because. For signal delay and convolution, we only need addition and subtraction of integer indices. Suppose that, and we want to add 7 and 6. The result for modulo-10 arithmetic is 3 because we do normal addition, and then reduce modulo-10, taking the positive remainder which is 3. For mod-10 arithmetic the remainder must always be a positive integer in the range 0 to. If we subtract 4 from 2, the result. When we count up modulo-10, the sequence is because adding 1 to 9 gives 10, and. Furthermore, if we evaluate for, we will start at which is equal to 6 and get. For a somewhat more compact general notation, we define index subtraction modulo- via The modulo- operation guarantees that. Thus for, the index cycles through the base interval starting at. In terms of this notation then, the correct DFT shift property is which emphasizes that the time shift is a circular shift with period.",
        "674": "Zero Padding While the signal in Fig.~\u203b(d) is the correct result for the 10-point DFT, it is not what we would normally want for a time-shifted signal. Equation and the example of Fig.~\u203b suggest a solution, however. First note that the signal is not a 10-point sequence if we start counting at. With the delay, the first four values of are zero and the shifted sequence goes from to, so the length of the shifted sequence is 14 samples. Thus, if we augment the sequence with 4 zero samples at the right hand end, it will be possible to shift the sequence to the right by 4 samples and still remain within the new base interval. This augmentation is known as \u201czero padding.\u201d Therefore, we must compute a 14-point (or larger) DFT if we wish to use multiplication of the DFT by to effect a time shift of 4 samples that does not \u201cwrap around.\u201d This is illustrated in Fig.~\u203b.",
        "675": " Illustration of the time-shift property of the DFT with zero padding: (a) A finite-length sequence of length 10 padded with 4 zeros. (b) The inherent periodic sequence for a 14-point DFT representation. (c) Time-shifted periodic sequence corresponding to. (d) The sequence obtained by evaluating the 14-point IDFT of only in the interval. Figure~\u203b(a) shows the sequence padded with 4 zero samples (which are, in fact, correct values for the finite-length sequence ). Figure~\u203b(b) shows the inherent periodic sequence when using the DFT representation with, and Fig.~\u203b(c) shows the shifted-by-4 periodic sequence corresponding to. Finally, if we form and then use the 14-point IDFT to compute the sequence for, we obtain the result of Fig.~\u203b(d). Now, clearly, for. A longer DFT with would also get the correct shifted signal, but would have extra zeros for.",
        "676": "The Convolution Property for the DFT One of the most important properties of the DTFT is that time-domain convolution becomes frequency-domain multiplication in the DTFT; i.e., Continuing with our view that the DFT is a sampled version of the DTFT, we will see that a similar property holds for the DFT, but keeping in mind what we have just learned about time delay and the DFT, it is will not be surprising to learn that the inherent periodicity of the DFT/IDFT representation leads to special considerations.",
        "677": "Since the DFT/IDFT representation deals with finite-length sequences, we want to study the convolution of two finite-length signals, each having a DFT representation. Therefore, the convolution expression can be written as a finite sum where we assume that except in the interval and except in the interval. That is, is an -point sequence and is an -point sequence. In Chapter 5 we learned that can be nonzero only in the interval, so the length of the sequence, counting the sample at, is.",
        "678": "Now assume that we obtain a DFT by sampling at the DFT frequencies ; i.e., where all three DFTs are -point DFTs. If the sequences and are not of the same length, then at least the shorter one is assumed to be zero-padded to length. Not surprisingly, convolution of two finite-length sequences is represented by the DFT as the product of their DFTs. However, we should not jump to conclusions because what the DFT represents is not the ordinary convolution as in Chapter 5 and in, but the periodic convolution of the inherent periodic sequences and as in where is the IDFT of. Note that in periodic (or circular) convolution, both sequences have the same period and the sum is over one period. The IDFT of is periodic with period as we have already seen. Furthermore, tells us that the periodic sequence is related to the sequence in as Now we can determine the length that will be needed to get. Since the desired convolution output in has length samples, we would require if we want over the base interval. This can be achieved by computing the DFTs and with satisfying this condition. Zero-padding of the sequences and to length will be needed.",
        "679": "This convolution property is illustrated by the time delay property of Sec. \u203b, which is a special case of the convolution property. First, recall that if we have a delay-by- system then its impulse response is, and In this case, the length of the sequence is samples because we start counting at. If is an -point sequence, then the total length of will be. We have shown that the DFT of is. Thus, the DFT product,, is identical what we would obtain using the time-delay property for the DFT if. ",
        "680": "EXAMPLE:\u00a0 Convolution of Pulses Suppose that is a length-10 rectangular pulse, and is a length-6 rectangular pulse. From Chapter 5, we know that the convolution result will have a trapezoidal shape. If we use 10-point DFTs, it is possible to work out the answer analytically. The 10-point DFT of is, because is all ones and we can use the DFT pair in with. Then we can carry out the multiplication of DFTs without knowing. Since is nonzero only when, we obtain The value of is the DFT coefficient at is Thus we have and we can take the length-10 IDFT to get The only nonzero term in the sum is the one for. The convolution result is a constant which is not what we want. If we change the length of the DFTs to we can get the correct convolution of the two rectangular pulses where the length is. This case cannot be done algebraically without a lot of tedious manipulations, so we would prefer to use MATLAB. x = ones(1,10); h=[1,1,1,1,1,1]; y16 = ifft( fft( h, 16 ).* fft( x, 16 ), 16); y10 = ifft( fft( h, 10 ).* fft( x, 10 ), 10); The results for the case are plotted in Fig. \u203b, but we omit the plots for because the signal will be constant. This example demonstrates the general result that periodic convolution as computed using the DFT and IDFT can be identical to the ordinary convolution of two finite-length sequences if the value of in the DFT/IDFT representation is greater than or equal to the sum of the lengths of the two sequences minus one, i.e.,. Illustration of convolving two rectangular pulses with length-16 DFTs: (a) The inherent periodic signal corresponding to the 16-point DFT of a length-6 pulse. (b) The inherent periodic signal corresponding to the 16-point DFT of a length-10 pulse. (c) The inherent periodic signal corresponding to ; is also equal to the circular convolution defined in. (d) The 16-point sequence obtained by evaluating the 16-point IDFT of only in the interval. ",
        "681": "Table of Discrete Fourier Transform Properties and Pairs \t \t In this chapter, we have derived a number of useful transform pairs, and we have also derived several important properties of discrete Fourier transforms. Table~\u203b on p.~ includes the discrete Fourier transform pairs that we have derived in this section. Basic discrete Fourier transform pairs. Table of DFT Pairs Time-Domain: Frequency-Domain: 1 The basic properties of the discrete Fourier transform are what make it convenient to use in designing and analyzing systems, so they are given in Table~\u203b on p.~ for easy reference. It is important to emphasize that these properties must all be interpreted in terms of the inherent periodicity of the DFT/IDFT representation. For example, the time-delay property applies to the inherent periodic sequence that we have denoted and the convolution property likewise concerns periodic convolution of the inherent periodic representations of and. Some of the properties in Table~\u203b have not been discussed in this chapter. They are nevertheless true and are included for completeness. ",
        "682": " Basic discrete Fourier transform properties. Table of DFT Properties Property Name Time-Domain: } Frequency-Domain: Periodic Linearity Conjugate Symmetry is real Conjugation Time-Reversal Delay (PERIODIC) Frequency Shift Modulation Convolution (PERIODIC) Parsevals Theorem = ",
        "683": "Spectrum Analysis of Discrete Periodic Signals In Chapter \u203b, Section 3-6, the Fourier Series integral was presented as the operator that \u201cFourier analyzes\u201d a continuous-time periodic signal to extract its spectrum. The resulting Fourier Series sum represents the periodic signal as a weighted sum of complex exponentials, and the frequencies of the complex exponentials are all integer multiples of a fundamental frequency. In this section, we will consider the Fourier analyzer for a discrete-time periodic signal by developing the Discrete Fourier Series (DFS) representation. The key to the DFS is the IDFT sum which synthesizes a periodic signal when evaluated outside of the interval, and which is also a weighted sum of complex exponentials whose frequencies are all integer multiples of. As a result, we will interpret the DFT as a method of Fourier analysis which extracts the spectrum of a discrete-time (sampled) periodic signal by computing the DFT of one period of its samples. Once we have established the DFS, we will be able to connect the spectrum analysis of a periodic discrete-time signal to the spectrum of a periodic continuous-time signal. Periodic Discrete-time Signal: Fourier Series As we showed in Section \u203b, the IDFT representation of a finite-length sequence is inherently periodic with period. When we use the IDFT to represent a finite-length sequence we usually restrict the evaluation of the IDFT to the range ; however, if we evaluate the IDFT outside that range we obtain the periodic sequence, which in is the infinite repetition of the finite-length sequence whose -point DFT is.",
        "684": "Another notable fact about the IDFT summation is that it is the sum of complex exponentials with uniformly spaced frequencies,. In other words, all the frequencies are integer multiples of. If we alter our point of view away from the representation of a finite-length sequence and focus on the fact that the -point IDFT gives a periodic result when evaluated outside the base interval, then it is reasonable to call the representation in the Discrete Fourier Series (DFS) for the periodic signal. Just as in the case of the Fourier series for continuous-time signals discussed in Sec.~\u203b, we now have a representation of a discrete-time periodic signal as a sum of harmonic complex exponentials. ",
        "685": "We want to use the IDFT to synthesize a periodic signal but the usual Fourier Series synthesis involves negatives frequencies, so we need to reassign some indices to be negative frequencies as in Sec. \u203b. When the periodic signal is real-valued, e.g., a sinusoid, then we would expect a synthesis formula with negative frequencies, as well as positive frequencies, e.g., In addition, the coefficients have complex conjugate symmetry, when is real. In the discrete-time case, the summation limit must be finite because the sum in is equivalent to the IDFT which has has only terms, and is a general representation for any periodic discrete-time signal whose period is. The number of terms in is, so we must have.",
        "686": "EXAMPLE:\u00a0 Synthesize a Periodic Signal from DFS Suppose that a signal is defined with a DFS summation like with specific values for the coefficients, i.e., If, make a list of the values of for to show that has a period equal to 5. \t\t\t\t\t\t\t Solution The summation formula for can be written out This expression can be evaluated by plugging in integer values for to obtain the following list of values for : Thus we see that repeats with a period of 5. The coefficients of the complex exponential terms in will become the Fourier Series coefficients. We already know that the DFT coefficients are periodic, i.e., as shown in Fig. \u203b. Thus we can identify the Fourier coefficients in with respect to those in to obtain The end result is that we can write a Discrete Fourier Series (DFS) for a periodic discrete-time signal by using the (scaled) DFT as an analysis summation to obtain the coefficients from one period of the signal.",
        "687": " In the DFS, the factor of is associated with the analysis summation.",
        "688": "EXAMPLE:\u00a0 Conjugate Symmetry of DFS Coefficients When getting the DFS coefficients from the DFT, there are two cases to consider: even and odd. The notation is much easier when is odd because we can write where is an integer. For example, when we have, so the Fourier Series would be The 5-point DFT of one period of is. On the other hand, when is even there is a complication. For example, when the summation in implies that, or, but using the 4-pt DFT we can write so the DFS representation of would be The case where is a special case that is similar to the ambiguity with treated in Sect. \u203b. The value of (or ) will be real when the signal is real, so it does not require a complex conjugate term in negative frequency. EXAMPLE:\u00a0 Period of a discrete-time sinusoid We might expect the fundamental period of a periodic signal to be the inverse of its fundamental frequency because this is true for continuous-time signals. However, for discrete-time signals this fact is often not true. The reason for this uncertainty is that the period of the discrete-time signal must be an integer.",
        "689": "Consider the signal, whose frequency is rads. The period of this signal is ; it is also the shortest period so we want to call 16 the fundamental period. If we take the 16-point DFT of one period of we get. Then we can convert these DFT coefficients into a DFS representation with and Now, consider the signal, whose frequency is rads; its period is not. Its period is also, and this is the shortest integer period. If we take the 16-point DFT of one period of we get. Then we can convert these DFT coefficients into a DFS representation with and The problem facing us is that the period of being 16 implies that the fundamental frequency is so should be the fifth harmonic, but the definition of the cosine has only one frequency which has to be the fundamental frequency. In fact, this inconsistency happens whenever we take the DFT of a sinusoid with frequency, and the integer is not a factor of.",
        "690": "Therefore, this example illustrates the fact that it is impossible to define a simple consistent relationship between the fundamental period and fundamental frequency of a discrete-time signal. Fundamental Period and Fundamental Frequency of Discrete-Time Signals We might expect the fundamental period of a periodic signal to be the inverse of its fundamental frequency because this is true for continuous-time signals. However, for discrete-time signals this fact is often not true. The reason for this uncertainty is the requirement that the period of the discrete-time signal be an integer. Since the discrete-time signal is only defined for integers, the definition of periodicity has to be The simplest examples that will illustrate some problems with periodicity come from studying the sinusoid. We would expect the sinusoid to be a periodic signal, but even that is not true. There are three cases to consider, depending on the frequency:",
        "691": " is an integer multiple of. is a rational multiple of, e.g.,. is an irrational multiple of, e.g.,. In the first case, we have This demonstrates that is a period of, but does not prove that it is the fundamental period. In fact, if is a integer factor of then is a shorter period. The second case is similar to the first. Suppose that is a rational number reduced to lowest terms, then This signal will have a period of, and might have a shorter period if is a factor of. In the third case, the discrete-time sinusoid is not periodic because an integer times an irrational number cannot equal an integer.",
        "692": "The net result of this discussion is that there is no point in thinking of the fundamental period as the inverse of its fundamental frequency. For further confirmation of this dilemma, consider the following example. EXAMPLE:\u00a0 Period of a discrete-time sinusoid Consider the signal, whose frequency is rads. The period of this signal is ; it is also the shortest period so maybe we want to call it the fundamental period. If we take the 16-point DFT of one period of we get. Then we can convert these DFT coefficients into a DFS representation with and Now, consider the signal, whose frequency is rads. Its period is also, and this is the shortest period. If we take the 16-point DFT of one period of we get. Then we can convert these DFT coefficients into a DFS representation with and The problem facing us is that seems to go with the fifth harmonic of a fundamental frequency, when we were expecting a fundamental frequency of for the cosine. In fact, that happens whenever we use the DFT because all the frequencies have to be integer multiples of. The preceding example is another illustration of the fact that it is impossible to get a consistent result for the fundamental period. On the other hand, the concept of a fundamental frequency has no such problems. In order to simplify our presentation, we will treat only the simplest case where with an integer. When we make the substitution in, the expression for becomes In this particular case, exactly samples are taken during each period of because. As a result, the sequence is guaranteed to be periodic and its period will be.",
        "693": "Sampling Bandlimited Periodic Signals The next task is to relate the DFS to the continuous-time Fourier Series. The connection is frequency scaling when sampling above the Nyquist rate (as done in Chapter~\u203b). Consider a periodic bandlimited continuous-time signal represented by the following finite Fourier series",
        "694": " where is the fundamental frequency (in Hz) and denotes the integer index of summation. This continuous-time signal is bandlimited because there is a maximum frequency, rad/s, in the expression for. When sampling, we must have Hz to satisfy the Nyquist rate criterion.",
        "695": "When is sampled at a rate, the sampled signal will also be a sum of discrete-time complex exponentials The discrete-time signal defined in might not be periodic, but if we restrict our attention to the special case where, then is guaranteed to be periodic with a period of. The number of samples in each period will be equal to the duration of the fundamental period times the sampling rate, i.e., is an integer. We can invert to write. When we make the substitution in, the expression for becomes",
        "696": " We recognize as the Discrete Fourier Series of a periodic signal whose period is. Comparing and, we see that the Fourier Series coefficients are identical for and.",
        "697": "The number of samples in one period must be large enough so that when we use the DFT of over one period to extract we will have at least DFT coefficients which can be used to get the DFS coefficients from using. Thus, the DFT-DFS relationship condition requires. In addition, the fact that can be rewritten as means that the sampling rate is times the fundamental frequency of. The Sampling Theorem requirement for no aliasing is, i.e., the sampling rate must be greater than the Nyquist rate, which is twice the highest frequency in the bandlimited signal. Since, the Nyquist rate condition implies that Therefore, in the case, the number of samples per period must be greater than twice the number of (positive) frequency components in the continuous-time Fourier representation of.",
        "698": "EXAMPLE:\u00a0 Fourier Series of a Sampled Signal Suppose that the following continuous-time signal and we want to determine the Fourier Series representation of the resulting discrete-time signal. In particular, we would like to determine which Fourier Series coefficients are nonzero. We need a sampling rate that is an integer multiple of the fundamental frequency and is also greater than the Nyquist rate (42 Hz). The two frequency components in are at 9 Hz and 21 Hz, so the fundamental is the least common divisor Hz. We must pick to satisfy the Nyquist rate condition, so for this example we use Hz.",
        "699": "The sum of sinusoids can be converted to a sum of complex exponentials, and then can be employed to represent the sampled signal as The four discrete-time frequencies are and. In order to write in the summation form of, we use.",
        "700": "In we want to emphasize the term in the exponents, so we write Now we can recognize this sum of four terms as a special case of with and, i.e., the range of the sum is. The only nonzero Fourier coefficients in are at those for, and their values are,,, and. These relationships between the DFS and the continuous-time Fourier Series, and also between the DFS and the DFT, are illustrated in Fig.~\u203b where Fig.~\u203b(a) shows a \u201ctypical\u201d spectrum for a band limited continuous-time periodic signal (as a function of ), and Fig.~\u203b(b) shows the spectrum for the corresponding periodic sampled signal (as a function of and also ). Notice the alias images of the original spectrum on either side of the base band where all the frequencies lie in the interval. When, the sampling rate is greater than the Nyquist rate and the entire continuous-time spectrum is found in the base band because no aliasing distortion occurs.",
        "701": " view of sampling a periodic signal. (a) Line spectrum of a bandlimited continuous-time periodic signal whose fundamental frequency is equal to. (b) Line spectrum of the discrete-time periodic signal obtained by sampling the signal in (a) above the Nyquist rate at times, giving lines at. (c) DFT coefficients shown as a line spectrum, where the DFT is taken over one period of the periodic discrete-time signal.} }",
        "702": "We know from Chapter \u203b that the spectrum of a sampled signal is periodic in with a period of. This attribute is shown in Fig. \u203b(b). Since the spectrum is periodic in with a period of with the spectral lines located at frequencies, an equivalent statement is that the spectrum is periodic in with a period equal to. To emphasize this point, Fig.~\u203b(b) shows two horizontal plotting axes: one for normalized frequency and the other for the indices that appear in. The DFT shown in Fig.~\u203b(c) is also subject to the same periodicity in, so the DFT coefficients for can be related to the DFS coefficients via, for.",
        "703": "Periodicity of the Spectrum of Sampled Signals As defined in Section 4.2, the spectrum plot of a multi-frequency discrete-time signal such as will consist of two sets of spectrum lines. The first set consists of lines at all the positive and negative frequencies in, i.e.,, for. This set of lines is called the base band because all the frequencies lie in the interval. The base band is the result when the discrete-time signal is formed by sampling above the Nyquist rate, i.e., no distortion or confusions due to aliasing, because the frequencies defining the spectrum of the original continuous-time signal are mapped into the interval of the normalized discrete-time frequency axis through the relation.",
        "704": "The second set contains the alias lines at all the frequencies formed by adding integer multiples of to the frequencies in the first set, i.e., the alias frequencies are The alias property results from the fact that complex exponentials with frequencies are indistinguishable from the complex exponential at, i.e., The nonzero values of correspond to exact copies of the original base band spectrum shifted by. This is illustrated in Fig.~\u203b where Fig.~\u203b(a) shows a \u201ctypical\u201d spectrum for a band limited continuous-time periodic signal (as a function of ), and Fig.~\u203b(b) shows the spectrum (as a function of and also ) for the corresponding periodic sampled signal when, i.e., greater than the Nyquist rate so that no aliasing distortion occurs. Notice the alias images of the original spectrum on either side of the base band.",
        "705": " view of sampling a periodic signal. (a) Line spectrum of a bandlimited continuous-time periodic signal whose fundamental frequency is equal to. (b) Line spectrum of the discrete-time periodic signal obtained by sampling the signal in (a) at times. The sampled signal is periodic in the time domain with a period equal to ; its line spectrum is periodic in the domain with a period of.} } We know from Chapter \u203b that the spectrum of a sampled signal is periodic in with a period of. This attribute is easy to see in Fig. \u203b(b). One way to emphasize this periodicity is to rewrite the formula for with one index as where is defined as. Since the spectrum is periodic in with a period of with the spectral lines located at frequencies, an equivalent statement is that the spectrum is periodic in with a period equal to. To emphasize this point, Fig.~\u203b(b) shows two horizontal plotting axes: one for normalized frequency and the other for the indices that appear in.",
        "706": "Spectrum Analysis of Sampled Signals Since discrete-time spectrum analysis is the process of finding the spectrum given the discrete-time signal, we want to investigate how to determine the coefficients directly from the samples by a numerical procedure that is equivalent to evaluating the Fourier series integral in. We cannot use the Fourier series integral directly because we only know at the sample points. On the other hand, when the Sampling Theorem is obeyed, i.e.,, exact reconstruction of the original bandlimited periodic continuous-time signal is theoretically possible, and then the Fourier integral could be evaluated. Therefore, it is reasonable to expect that the samples over one period, for, would be sufficient to recover the spectrum of the original continuous-time signal. The remainder of this section will demonstrate how numerical integration is the same as doing the DFT.",
        "707": "For a sampled signal, one possible approach is to approximate the Fourier integral with a Riemann sum, since that approximation would only involve values of at discrete time instants. It turns out that this method is exact rather than approximate in the special case where. The Riemann-sum approximation to an integral is where the integrand is sampled at equally spaced times, and. Thus, the Riemann-sum approximation to the Fourier integral of is where is the sampling interval for the numerical integration and the values are the times at which the integrand of is evaluated. If we set and recall that (or ) when is periodic with period, then and we can express as a sum over in terms of one period of as in The RHS is, of course the DFT multiplied by, so the DFT is approximating the Fourier Series integral. The factor of shows up when we equate the DFT coefficients to the DFS coefficients.",
        "708": "To see how compares to (equivalently ), we can substitute from into obtaining where we have changed the dummy summation index in to to keep it distinct from in. Now, interchanging the order of the summations (which is permissible because they are both finite sums), we obtain where is defined by the term in the large parentheses in.",
        "709": "The sum within the parentheses in can be simplified using the orthogonality property of discrete-time complex exponentials given in.",
        "710": "Since and are integers such that, it follows that and therefore it follows from that. Thus, we have shown that we can start with a given periodic sequence, and determine the coefficients of its discrete-time Fourier series without recourse to the Fourier integral, or the underlying implied (but not directly available) continuous-time periodic signal.",
        "711": "In summary, a periodic sequence having period can be represented by the following discrete Fourier series (DFS):",
        "712": " The Fourier coefficients in are obtained via: which is the Fourier analysis summation formula for computing the Fourier coefficients directly from one period of the sampled sequence.",
        "713": "Although the synthesis summation may be evaluated for any, in many cases it will be sufficient to evaluate the summation for indices within just one complete cycle of the periodic sequence. The rest of the periodic discrete-time signal could be constructed at other sample times either by evaluating or by repeating the values from one period as often as desired with successive shifts of samples.",
        "714": "Furthermore, we have shown that, except for the reordering in, the set of spectrum coefficients is identical to the set of Fourier coefficients of a continuous-time bandlimited periodic signal as in if the period is an integer number of sampling periods; i.e., if, with. This result is in a sense a formal proof of the truth of the Sampling Theorem for this special case, because we have shown that for the above stated conditions, it is possible to reconstruct the original bandlimited periodic signal for all using only the finite set of values of the signal for. Although this \u201cproof\u201d only holds for this restricted case, it suggests that Fourier spectrum analysis should hold the key to a more general proof of the Sampling Theorem. We will see that this is so in Chapter \u203b.",
        "715": "Spectrum Analysis of Periodic Signals In this section, we will show how the DFT can be used to analyze continuous-time periodic signals which have been sampled.",
        "716": "As a specific example, consider the periodic continuous-time signal The fundamental frequency of this signal is rad/sec, and the signal consists of five harmonics at 2, 4, 5, 16, and 17 times the fundamental. This signal is the same as the synthetic vowel studied in Section~\u203b. If we sample at the rate Hz, we will obtain exactly 40 samples per period since Hz is an integer multiple of Hz. The sampled discrete-time signal is The plot of in Fig.~\u203b(a) shows that it is periodic with a period of 40, i.e.,.",
        "717": " Periodic sequence in and the corresponding 40-pt. DFT spectrum. (a) Dark region indicates 40-pt. interval taken for analysis, (b) DFT magnitude spectrum, (c) DFT phase spectrum. ",
        "718": "Another way to arrive at the same conclusion is to note that has frequencies that are all multiples of which is therefore the fundamental frequency. Like the continuous-time case, in has harmonics at multiples of the normalized fundamental frequency numbered 2, 4, 5, 16, and 17.",
        "719": "Now we consider taking the DFT of with a DFT length equal to the period. First, one length-40 period of from Fig.~\u203b(a) is extracted and used in a 40-point DFT. The 40 DFT coefficients obtained are shown as magnitude and phase in Figs.~\u203b(b) and (c), respectively. These 40 DFT coefficients represent one period of the sequence exactly through the synthesis formula of the IDFT. The DFS is also a representation of as a sum of complex exponentials at the harmonic frequencies, so the DFT coefficients are related to the DFS coefficients as shown in.",
        "720": "Use the results of Section~\u203b, Exercise \u203b to determine a formula for, the 40-pt. DFT of in. Use 40 points of taken over the period. Verify that some of the nonzero DFT values are Determine the other nonzero values of, and also how many DFT coefficients are equal to zero. Show that the IDFT of defined in Exercise~\u203b gives in. Furthermore, show that the signal obtained via the IDFT is periodic with period. The plots of Fig.~\u203b(b,c) are the spectrum for five sinusoids, or ten complex exponentials, so Fig.~\u203b(b) consists of ten spectral lines, each one being just like the plot in Fig.~\u203b which shows the magnitude of the DFT for a single complex exponential signal of the form where is an integer.",
        "721": "We have seen that the DFT can be used to obtain the DFS when the DFT length exactly matches the period of. Without an exact match, the DFT result might be a reasonable approximation, but it will not be exact. To illustrate this point, consider a case where the sampling frequency is not an integer multiple of the fundamental frequency, i.e., is not an integer and, therefore, cannot be equal to the DFT length. An example is sampling the signal in at Hz, i.e.,, so the plot in Fig. \u203b(a) seems to have a period close to 40, but it is not exactly equal to 40.",
        "722": "If we proceed as though the period were 40, and compute the 40-pt DFT of samples of the sampled signal, we obtain the plot in Fig. \u203b(b). This DFT plot is quite similar to the DFT in Fig.~\u203b(b), but none of the DFT coefficients are exactly zero. Instead, each cosine will produce a contribution to the DFT that has many small values spread out over the entire range, as was seen in Fig~\u203b. The amplitude of the DFT values will tend to peak around the DFT indices closest to the same peak frequencies as before, where. However, the presence of many additional spectral components will give a \u201cblurred\u201d (or imprecise) spectral representation of the original continuous-time signal.",
        "723": " Discrete-time sequence whose period is approximately equal to 40 obtained by sampling in at Hz, and the corresponding 40-pt. DFT spectrum. (a) Dark region indicates 40-pt. interval taken for analysis, (b) DFT magnitude spectrum, (c) DFT phase spectrum. ",
        "724": "Use MATLAB to synthesize samples of in with Hz instead of 4000 Hz; call the result x4100. Next, use the MATLAB statement to compute the 40-pt. DFT. Finally, make a plot of the DFT magnitude stem(0:39,abs(X41)), and compare the resulting magnitude spectrum to that of Fig.~\u203b(b). Comment on zero regions of the DFT.",
        "725": "Windows Since the DFT is a finite sum, it can only be used to analyze finite-length signals. Even infinite-length periodic signals that have a DFS are analyzed by taking the DFT of one period. We have examined the case where the finite-length signal is the impulse response of an FIR system, and have shown that samples of the frequency response, or equivalently the DTFT, can be computed by zero-padding and taking a long DFT. Remember from Section~\u203b that the -point DFT of a finite-length sequence is identical to the DTFT of the sequence evaluated at frequencies with. That is, We recognized that even if since zero samples are effectively appended to the end of the sequence. This operation was called zero-padding in Section~\u203b.",
        "726": "In MATLAB, the DFT is computed by the function fft(x,N), which automatically zero-pads the sequence if the sequence length is smaller than the DFT length.",
        "727": "In this section, we want to study another aspect of finite-length signals, which is their use as windows for local spectrum analysis of short sections of very long signals. The concept of windowing is widely used in signal processing. The basic idea is to extract a finite section of a very long signal via multiplication where is the starting index of the extracted section. The product signal is a finite-length sequence if the window function is zero outside of an interval. For example, consider the simplest window function, which is the -point rectangular window defined as The essential idea of windowing is that the product will extract values from the signal starting at. Thus the following are equivalent The name window comes from the idea that we can only \u201csee\u201d the values of the signal within the window interval when we \u201clook\u201d through the window. Multiplying by is looking through the window. When we change, the signal shifts, and we see a different length- section of the signal. The nonzero values of the window function do not have to be all ones, but they should be positive. For example, the symmetric -point Hann, or von Hann, window is defined as The stem plot of for in Fig. \u203b shows that the window values are close to one in the middle and taper off near the ends. In MATLAB the function hanning(L) will generate the window values. Time-domain plot of Hann window. The window is zero for and. A related window is the symmetric -point Hamming window which is defined as The MATLAB function hamming(L) will generate a vector with values given by.",
        "728": "DTFT of Windows Windows are used often in spectrum analysis because they have desirable properties in the frequency domain. Thus is worth studying the DTFT of a few common windows. We already know the DTFT of the rectangular window\u2013-it involves the Dirichlet form. For the Hamming and von Hann windows, the analytic formulas for the DTFT do not provide much insight, so we prefer to obtain the DTFT numerically and make plots after taking a zero-padded DFT.",
        "729": "Figure~\u203b(a) shows plots of the magnitude of the DTFT for two Hann filters with lengths and.",
        "730": " DTFT of two Hann windows; length (colored lines), and length (black/gray lines) (a) as computed by 1024-point FFT with zero padding and (b) as plotted with DFT values reordered to place in the middle. The DTFTs were obtained using a -point DFT with zero padding. The frequency samples enable MATLABs plot function to draw a smooth curve even though the sample values are connected by straight lines. Figure~\u203b(b) shows that the DTFT of the Hann window is concentrated near. The main lobe around covers the range ; outside of this mainlobe region the DTFT is very close to zero.",
        "731": "When we compare for the two different window lengths, we see that the DTFT magnitude becomes more concentrated around as we increase. It can be shown that the first zero of occurs at for the Hann filter. Thus, increasing from 20 to 40 cuts the width of the passband in half (approximately). The peak amplitude at is also approximately doubled, going from 10.5 to 20.5, i.e., the general formula for the peak is. Plotting allows us to verify these results.",
        "732": "There is a property of the DTFT that says the DC value ( ) of the DTFT is equal to the sum of all the signal values in the time domain. Use the definition of the forward DTFT to prove this fact. Windows are used to extract sections of very long signals. An illustrative case is extracting a section of a sinusoid for spectrum analysis, which involves multiplying the sinusoid by the window and taking the DFT with zero padding to obtain samples of the DTFT. Within this context there are two questions that should be answered: first, how to choose a good window form, and second, how to choose the window length.",
        "733": "For the first question, consider an example that compares the rectangular window to the Hann window. Suppose that each window is applied to a sinusoid as, with a window length of 40. Since the window is multiplied by a cosine, the frequency shifting property of the DTFT predicts that the result will be the sum of two frequency-shifted copies of the DTFT of the window, i.e., where is the DTFT of the window. Figure \u203b shows the results, and it is easy to see the frequency shifting because the main lobes are centered at.",
        "734": " DTFT of windowed sinusoid via 1000-pt. DFT for (a) rectangular window (peak heights are 20), and (b) Hann window (peak heights are 10.25). Sinusoid frequency is rad. Tick marks on frequency axis spaced by rad. For the rectangular window shown in Fig. \u203b(a), the main lobe is narrow, extending over the range if measured between the first zero crossings on either side of the peak. However, the DTFT magnitude in Fig. \u203b(a) also exhibits many smaller lobes across the entire frequency range. These are called sidelobes. For the rectangular window these sidelobes are rather high and are the main reason that this window is considered inferior to most others. On the other hand, the Hann window DTFT shown in Fig.\u203b(b) has a wider mainlobe, but much smaller sidelobes. The main lobe of the Hann window DTFT extends over the range when measured between the first zero crossings. Thus the Hann window of length has a frequency-domain width that is twice that of a rectangular window of the same length. The sidelobes of the Hann DTFT magnitude in Fig.\u203b(b) are less than 1% of the peak heights.",
        "735": "If we pause to ask ourselves what the ideal DTFT should be for a windowed sinusoid of frequency, the answer is likely to be that we want to see only two spectrum lines at. In other words, the ideal main lobe should be very narrow (concentrated at the correct frequencies ), and the ideal sidelobes should be zero (or nearly zero). It turns out that these two expectations are in conflict when using finite-length windows, because it is known that reducing the sidelobes can only be done at the expense of broadening the main lobe.",
        "736": "In addition, a comparison of the two window DTFTs in Fig.\u203b shows that the peak heights are different. For the Hann window, we have already observed that the peak height will be ; for the rectangular window it is. In either case, the window is contributing a known scale factor to the result. If the objective is spectrum analysis of a sinusoid, as in this example, we know that the result should be a spectrum line whose height is half the amplitude of the sinusoid, or in the unit-amplitude case. In Fig. \u203b(a) dividing by will give, and also in Fig. \u203b(b) dividing by will give.",
        "737": "For the second question about length of the window, we demonstrated that the width of the mainlobe in the DTFT is inversely proportional to the window length. We continue with the same example of spectrum analysis of a windowed unit-amplitude sinusoid, but this time use two different Hann windows with lengths 20 and 40. Figure \u203b shows the resulting scaled DTFTs, where the known window scale factors have been divided out, so the peak values are.",
        "738": " Scaled DTFT of windowed unit-amplitude sinusoid via 1000-pt. DFT for (a) Hann window scaled by, and (b) Hann window scaled by. Sinusoid frequency is rad. Tick marks on frequency axis spaced by rad. If the mainlobe width is measured between first zero crossings, then for Fig. \u203b(a) the main lobe extends from to, and in Fig. \u203b(b) from to. Thus for, the mainlobe width is, and for,. The dependence of on is an inverse relationship which can be approximated as.",
        "739": "EXAMPLE:\u00a0 Sum of Two Sinusoids The importance of this analysis of frequency-domain window characteristics becomes clear when we consider signals that are composed of sums of sinusoids. If we have a signal with two frequencies where, then the windowed signal would be and the corresponding DTFT would be If we want to compute samples of in order to estimate and, it would be good to ensure that the main lobes of the terms in do not overlap. If the window is a Hann window of main lobe width, the main lobes will not overlap if. In this case we would obtain distinct peaks at and and the peak heights will be nearly equal to and. When there are two distinct peaks in the DTFT, we say that the two frequencies are resolved. The effect of window length on spectrum resolution is discussed further in Sec.~\u203b. ",
        "740": "The Spectrogram We have seen that the DFT can compute exact frequency-domain representations of both periodic and finite-length discrete-time signals. An equally important case occurs when the sampled signal is indefinitely long, but not necessarily periodic. Then there are two different viewpoints to consider: the long signal has the same global spectral content for its entire duration, or the long signal is actually the concatenation of many short signals with changing local spectral content. In the first case, we might be able to analyze the signal mathematically with the DTFT, but we cannot numerically calculate its frequency domain representation unless the long signal is actually a finite-length signal. Then we could compute its DFT but we we might have to wait an inordinately long time just to acquire all the samples, and even having done so, we would then be faced with a huge DFT computation. This would be unacceptable in a real-time system that needs to process signals on-line. When the long signal is the concatenation of many short signals, the appropriate frequency-domain representation would be the collection of many short-length DFTs. Then the temporal variations of the local frequency content would stand out. The spectrogram arises from this viewpoint where a short-length DFT is used to repeatedly analyze different short sections of a longer signal.",
        "741": "Good examples of indefinitely long signals are audio signals such as speech or music. In both of these cases, it is the temporal variation of the frequency content that is of interest. The pitch and timbre of speech which changes with time is how information is encoded in human speech. Likewise, we have already seen that music can be synthesized as a succession of tone combinations held constant for short time intervals. For audio signals we may have digital recordings that give very long sequences obtained by sampling for many minutes or even hours. For example, one hour of stereophonic music sampled at 44.1 kHz would be represented by samples per channel. If we want to compute the (global) DFT of the entire one hour of audio, the closest power-of-two FFT needed would be per channel. On the other hand, the local spectrum approach above would use short-length FFTs to analyze short time segments within the long signal. This is reasonable (and generally preferable) because a long recording probably contains a succession of short passages where the spectral content does not vary. It might even be true that there is a natural segment length for the audio signal that will dictate the FFT length.",
        "742": "An Illustrative Example ",
        "743": "To examine the points made above, we shall study a signal whose spectral properties vary with time. We begin by synthesizing a continuous-time signal that consists of four contiguous time intervals of cosine waves of different amplitudes and constant frequencies. Such a signal could be described by the equation ",
        "744": "The signal begins with a -second interval of a cosine signal of amplitude 5 and frequency Hz. At the waveform switches to a cosine of amplitude 2 and frequency, and retains these parameters throughout the interval. The last two intervals switch to different amplitude and frequency parameters at and as described by. The signal might continue indefinitely with changing parameters after, but it is sufficient to limit attention to the first four time intervals.",
        "745": "For a particular sampling frequency, the resulting sampled signal would be where,, and are dimensionless frequency quantities, and the durations are,,, and samples. Figure~\u203b(a) shows the first 801 samples of when,,, and. These normalized frequencies would be the result when Hz and Hz,, and, and Hz. However, other values of,,,, and could give the same normalized frequencies and, therefore, the same sequence of samples.",
        "746": "Suppose that the sampling frequency is Hz. What values of,, and in will give values,,, and in ?",
        "747": "Use MATLAB to synthesize the sampled signal in with frequencies,,, and, and then use the D-to-A converter on your computer (soundsc in MATLAB) with Hz to listen to in. During the listening observe the differences among the four segments of the signal. Is what you hear completely consistent with the specified signal parameters (duration, frequency, intensity) given above for ?",
        "748": "Now suppose that we compute the DFT of the entire 10000-point sequence in using zero padding with. The resulting magnitude of the length-16384 DFT is plotted in Fig.~\u203b. DFT of the entire sequence in with DFT length. Only the magnitude is shown for. Horizontal axis is labeled with normalized frequency using. The obvious features in this figure are four narrow peaks and these give some clues as to the nature of the signal, but the interpretation is far from complete and is not straightforward. The four strong peaks might suggest a signal comprised of a sum of four cosine waves of different frequencies and comparable amplitudes. With some assurance, we can state the following about the signal whose DFT is plotted in Fig.~\u203b: The four peaks in the DFT spectrum occur at frequencies corresponding to approximately,,, and, so they probably represent sine or cosine wave components in at those frequencies. The peak heights of the DFT tell us something about the amplitudes of the sinusoidal components but, as demonstrated in Section \u203b, the peak height of the DFT of a cosine signal depends on both the amplitude of the cosine signal and its duration. The peak heights in Fig.~\u203b differ at most by a factor of two, but we know in this example that the actual amplitudes for frequencies,,, and are 2, 5, 0.5, and 2, respectively. Although the height of the peak is equal to the height of the peak at, in fact, there is a 10:1 ratio between the amplitudes of those cosine components. Similarly, the DFT peak heights at and are in the ratio 1:2, while we know that the ratio of amplitudes of those two cosines is 5:2. The reason for this is that",
        "749": "the DFT of a cosine of amplitude and duration achieves a maximum of approximately, as was demonstrated in Section \u203b. The amplitude and duration of the segment are and, while the corresponding values for the segment are and ; therefore, the peak heights are the same:. Note: the peak widths do convey information about the duration of the sinusoids, as we saw during the discussion of windowing in Section~\u203b. Figure \u203b tells us nothing about the time location or time variation of the signal properties. For example, we cannot tell whether the different components occur simultaneously (overlapping in time throughout the whole analysis interval) or sequentially (as they do in this example). This is because in either case, the signals are additively combined, and by the linearity of the DFT, the result is the sum of all the DFTs of the individual components. Aside: The change in starting time is a time shift of a signal which does change the phase, but not the magnitude of the DFT. ",
        "750": "Time-Dependent DFT A long DFT gives useful information about the frequency content of a signal such as in, but it would be much more useful if the DFT could be used also to track the time-varying properties of the signal. To formalize this concept for a signal that is an indefinitely long sequence, we define the time-dependent discrete Fourier transform of this signal as where is the analysis time index that specifies the starting index in where a short-length DFT will be taken.",
        "751": "This equation involves two steps: windowing and short-length DFTs. First of all, is called the analysis window. It is a sequence such as the Hann window in that is nonzero only in the interval, where is assumed to be much smaller than the total length of the sequence. Therefore, the product is also nonzero only for. In this way, the window selects a finite-length segment from the sequence starting at the analysis time index. Second, a short-length DFT of the finite-length windowed segment is computed to extract the spectrum for that local interval of the signal. By adjusting the analysis time index, we can move any desired segment of into the domain of the window, and thereby select any length- segment of for analysis. The length- segments are often called frames. Because the window imposes time localization, in is also known as the short-time discrete Fourier transform or STDFT.",
        "752": "Now the right-hand side of is easily recognized as the -point DFT of the finite-length sequence,",
        "753": "so can be evaluated efficiently for each choice of by an FFT computation. The selection of analysis segments is illustrated by Fig.~\u203b for two values of the analysis starting time index, and. Figure~\u203b(a) shows the first 801 samples of the discrete-time signal in plotted versus with dots for signal values. Also shown in Fig.~\u203b(a) are two shaded regions of length 301 samples. One region starts at and is shaded in color, and the other starts at and is shaded in gray. These are potential 301-sample analysis frames. Also shown in Fig.~\u203b(a) is the outline of the (bell-shaped) Hann window whose origin is fixed at. The colored samples in",
        "754": " Time-dependent Hann windowing of a signal using a length-301 window. (a) The signal and fixed window, (b) Fixed window with signal shifted by 400 samples, i.e.,. Fig.~\u203b(a) are the samples that are multiplied by the window when and are thus selected for analysis with the DFT to obtain, the DFT of.",
        "755": "Figure~\u203b(b) shows the fixed window sequence (in color) along with the sequence, i.e., the sequence shifted to the left by 400 samples. The gray samples in Figs.~\u203b(a) have become the colored samples in Fig.~\u203b(b) where these samples are multiplied by the window and are thus selected for DFT analysis to compute. To compute for any other value of, we simply form the \u201cwindowed\u201d sequence in the manner illustrated in Fig.~\u203b, and then compute the DFT of the resulting finite-length sequence as in. The shift is typically moved in jumps of samples, where. As long as the windowed segments will overlap and all samples of the signal will be included in at least one analysis window. A common default choice is, but making smaller will provide a smoother spectrogram at the cost of more computation. The best value of is dictated by the need to track fast temporal changes in the signal being analyzed.",
        "756": "To illustrate the nature of the short-time DFT, Fig.~\u203b shows plots of,,, and for the signal of with STDFT parameters and.",
        "757": " Short-time spectra for signal in Fig.~\u203b with and. (a) STDFT at analysis time index. (b) STDFT at. (c) STDFT at. (d) STDFT at. Note: DFT magnitudes for are connected as if plotting the corresponding DTFT at frequencies. Figure~\u203b confirms that,,, and each display a single peak centered at the frequency of the sinusoidal component at the corresponding analysis times. Furthermore, the amplitudes of the peaks are in correct proportion since for the analysis times chosen, there is only one frequency within the window and the window lengths are the same (. While plots like those in Fig. \u203b are useful for displaying the frequency-domain properties in the neighborhood of a specific analysis time, they do not give a complete overview of how the signal parameters vary with time. In this example, we selected the analysis time indices, and, because we knew that these positions would show the different frequencies. In a practical setting, where we do not know the details of the signal in advance, the spectrogram display is much more effective and widely used.",
        "758": "The Spectrogram Display The STDFT computation results in a two-dimensional function, where the dimension represents frequency because is the analysis frequency, and the dimension represents time since the analysis window-position times are. Since the STDFT is a function of both frequency and time, there might be a different local spectrum for each analysis time. Although is complex valued with a magnitude and phase, we usually plot only the magnitude versus with a three-dimensional graphical display. A variety of display methods can be used to plot the magnitude as a function of both and, including waterfall plots (a sequence of plots like those in Fig.~\u203b stacked vertically), perspective plots, contour plots, or grayscale images. In addition, plotting the log magnitude, versus, is often used to show a wider amplitude range. The preferred form is the spectrogram, which is a grayscale (or pseudocolor) image where the gray level at point is proportional to the magnitude or the log magnitude,. Usually, large magnitudes are rendered as black, and small ones white, but pseudo-color images are also common.",
        "759": "Spectrograms of the signal in are shown in Fig.~\u203b; other examples can be seen in Figs.~\u203b, \u203b, \u203b, \u203b, \u203b, and \u203b.",
        "760": " Spectrograms of test signal calculated with spectgr (linear magnitude). (a) Window length. (b) Window length. For both cases, and. The horizontal axis is analysis time and the vertical axis is frequency. Analysis time can be given as a sample index as in Fig. \u203b, or converted to continuous-time via the sampling rate. Likewise, frequency can be converted from the index to hertz, or to normalized frequency as in Fig. \u203b. In the spectrogram image for a real signal, usually only the frequency range for, or is shown because the negative frequency region is conjugate symmetric. For the complex signal case, the frequency range is either, or, depending on whether or not the FFT values are reordered prior to being displayed. Interpretation of the Spectrogram A glance at Fig.~\u203b confirms that the spectrogram provides a much clearer time-frequency picture of the characteristics of the signal defined in than does either the single plot of a long DFT as in Fig~\u203b, or the four snapshots in Fig.~\u203b. The four sinusoidal components and their starting/ending times are evident in Fig.~\u203b(a). On the other hand, the dark horizontal bars in Fig.~\u203b(b) are wider and less clearly defined, so it is hard to determine the exact frequency. However, it is clear in both spectrograms where the sinusoidal components are located in time. The dashed lines at,, and show the locations of four individual DFTs that comprise Fig.~\u203b. That is, the DFT plots in Figs.~\u203b(a), (b), (c) and (d) are columns of the gray-scale spectrogram image shown in Fig.~\u203b(a). The magnitude of the DFT is mapped to a gray-scale shading which is visible along the colored, black and gray vertical marker lines, respectively. Figures~\u203b(a\u2013d) are, therefore, \u201cvertical slices\u201d of the spectrogram. A comparison of the two spectrograms in Fig.~\u203b yields some valuable insight into the effect of the window length in short-time Fourier analysis. The window length is in Fig.~\u203b(a) and in Fig.~\u203b(b). Along the time axis ) at points where the frequency changes abruptly, there are \u201cblurry\u201d regions; these occur at, and. The blurred regions occur whenever the analysis window straddles a region of rapid change since we will have signal samples within the window from both sides of the change point. For example, when, the signal interval will be shifted under the point window prior to the DFT, so the first half of will have frequency and the second half will have frequency. Thus the STDFT should have lower peaks at both frequencies. The longer the window, the wider will be this fuzzy region of ambiguity. This effect is seen in a comparison of Figs.~\u203b(a) and (b) because the regions of fuzziness are much wider for the window. This example illustrates the general principle that precise location of temporal changes in the signal requires a short window. Frequency Resolution Another important point is that the vertical width of the horizontal bars (for the sinusoids) is different in the two spectrograms. In fact, the width of the bars in Fig.~\u203b(b) is approximately 4 times the width of the bars in Fig.~\u203b(a), while the window lengths are 75 and 301, respectively. This suggests that for a signal having multiple frequency components overlapping in time, the spectrogram may not show two distinct peaks if the frequencies are too close together. In other words, the spectrogram may need a longer window to \u201cresolve\u201d two closely spaced frequency components into separate peaks in the frequency domain. This resolution issue is illustrated by a test example where the signal is composed of multi-frequency sinusoids with closely spaced frequencies.",
        "761": "The two spectrograms shown in Fig.~\u203b were computed for two different window lengths for the signal By comparing the spectrograms we can see the effect of the window length on frequency resolution. ",
        "762": " Spectrograms of test signal. (a) Window length. (b) Window length. FFT length was 1024 with zero padding. During the first interval the two frequencies are very close together at and (a spacing of rad), while in the third interval, the two frequencies are farther apart at and (a spacing of rad). The two frequencies in the first interval are both evident in Fig.~\u203b(a), but not in Fig.~\u203b(b) where they are merged into a single broad horizontal bar. On the other hand, during the third time interval, two clearly separated bars are evident in both spectrograms at the frequencies and, i.e., the two frequency components are resolved. This resolution is possible because, although the bars in Fig.~\u203b(b) are 3\u20134 times wider than the corresponding features in Fig.~\u203b(a), the spacing between the frequencies and is approximately four times the spacing between and. This example illustrates the general principle that frequency resolution can be improved by lengthening the analysis window.",
        "763": " Short-time spectra for signal in at two different time indices. (a) Hann window length, time index. (b),, (c),, and (d),. FFT length was 1024 with zero padding. To see the frequency resolution more clearly, Fig.~\u203b shows \u201cvertical slices\u201d taken along the dashed lines in Fig.~\u203b(a) and (b) corresponding to analysis times and 8000, which are in the middle of the segments having two sinusoids with closely spaced frequencies. Figure~\u203b(a) corresponds to the slice at with the window length, and it shows two distinct peaks around the frequencies and. However, Fig.~\u203b(c) shows the slice at with the window length. In this case there is only a single broad peak, and we conclude that the two frequencies are \u201cnot resolved.\u201d On the other hand, for the slices at shown in Figs.~\u203b(b) and (d), the two frequencies are more widely separated, and we see two distinct peaks in both of the spectrogram slices. However, for the slice at, if the window length were reduced below 75, the two peaks in Fig.~\u203b(d) would broaden and eventually as the window length decreases we would see only a single broad peak spanning the two frequencies at and. As a final comment, note that the vertical slice DFTs show clearly that the amplitude of the two sinusoids is different, but the amplitude difference is much harder to discern in the gray-scale spectrogram image.",
        "764": "Use MATLAB to generate the signal Then using a Hann window, compute the spectrogram slice at. Use window lengths of and and compute the DFTs to produce plots like Fig.~\u203b(b). Determine whether or not the two sinusoidal components are \u201cresolved.\u201d As suggested by Fig.~\u203b, the width of the peaks for the DFTs of the sinusoidal components depends inversely on the window length. In the previous section \u203b, we learned that what we are seeing in Fig.~\u203b(a) is the DTFT of the window shifted in frequency to the frequency of the sinusoid. Furthermore, it is a general principle of Fourier transforms that there is an inverse relation between window length and frequency width. Therefore, the frequency width of the DTFT of a window sequence such as the Hann window can be written as an inverse relationship where the constant is a small number, usually for \u201cgood\u201d windows. For the Hann window, the zero-crossing width is, so for the length-75 Hann window, which is enough to resolve the and frequency peaks at. If two frequencies differ by less than, their STDFT peaks will blend together. Thus, the ability to resolve two sinusoids, i.e., see two distinct peaks, depends inversely on the window length. This makes sense intuitively because the longer the window, the longer the time to observe the periodicity (or non-periodicity) of a signal. On the other hand, if the window is very short, e.g., less than one period of the signal, then we have virtually no information about any periodicity of the signal, and it is reasonable to expect this to be reflected in the STDFT.",
        "765": "There is still one more point to make about the window length, and this is the issue of \u201ctime resolution.\u201d We observed in Fig.~\u203b that the window length is also important in tracking the temporal changes in a signal. The \u201cfuzzy\u201d transition regions are approximately 4 times as wide in Fig.~\u203b(a) as in Fig.~\u203b(b) because the window length goes from to. Therefore, a rule of thumb often given is that in order to track rapid changes, we need to keep the window length as short as possible.",
        "766": "We are forced, therefore, into a tradeoff situation: the window should be short to track temporal changes, but long to resolve closely-spaced frequency components. For this reason, when studying the properties of an unknown signal, it is common to compute spectrograms with differing window lengths. Features that are obvious in one case may be obscured in another, but with multiple spectrograms and a good understanding of how window length (and shape) affect the spectrogram image, it is often possible to determine much useful information about the signal. This time-frequency information can be combined with physical models for signal generation and/or detection to obtain very accurate models of the signal that can be useful in a variety of applications. A notable and ubiquitous application is audio coding for digital storage and transmission. Audio coders like MP3 are based upon the same type of frequency analysis computations employed to generate the spectrogram image. Because the human auditory system incorporates a type of frequency analysis, short-time frequency-domain analysis is a very natural way to incorporate auditory perception effects such as masking into the process of compressing the digital representation of an audio signal. By a careful frequency analysis \u201cframe by frame\u201d, it is possible to mask coding errors by literally \u201chiding\u201d them beneath prominent spectral components. Spectrograms in MATLAB Since the spectrogram can be computed by doing many FFTs of windowed signal segments, MATLAB is an ideal environment for doing the DFT calculation and displaying the image. Specifically, the MATLAB spectrogram computation evaluates for length- signal segments that are separated by, i.e.,. The frequencies are scaled by and the times by, because the default MATLAB display presents the spectrogram versus frequency in hertz and time in seconds. Quite often the default is, so there is a 50% overlap of the signal segments analyzed by the DFT after windowing. The value of is the DFT length, so zero padding would be done when. Picking a large value for will give many frequency samples along the dimension because the frequency index ranges from 0 to. When the sampling rate associated with is available, the frequency locations for the DFT output can be scaled to hertz and the time axis can be converted from sample index to analysis time in seconds which is convenient for labeling the axes of the spectrogram display. The computation of the spectrogram can be expressed in a simple MATLAB program that involves one for loop. The core of the program is shown below for the special case of an even-length window with 50% overlap of signal segments.",
        "767": " % L = signal segment length, also window length (even integer) % N = FFT (DFT) lengths % wn = window signal, L-point column vector such as the Hann window, or hanning.m % xn = input signal (column vector) % assume the overlap is 50% Lx = length(xn); NwinPos = 2*ceil(Lx/L); X = zeros(N,NwinPos); for ii=0:NwinPos-1 X(:,ii+1) = fft(wn.*xn(ii*L/2 + (1:L)),N); end In recent versions of MATLAB the command that invokes the computation of is",
        "768": " [S,F,T] = spectrogram(X,WINDOW,NOVERLAP,NFFT,Fs,yaxis) The final string yaxis is needed to override the default behavior that puts the frequency axis on the horizontal and the time axis vertical. The outputs from spectrogram are S, a two-dimensional array containing the complex-valued spectrogram values, F, a vector of all the analysis frequencies, and T, a vector containing the starting times of the signal segments being windowed. The inputs are the signal X, the window coefficients WINDOW, the overlap of signal segments NOVERLAP, the FFT length NFFT, and the sampling frequency Fs. In addition, the default spectrogram window is Hamming, so it should be replaced with a call to the Hann function, e.g., hanning(L). Note that the window skip parameter in is the window length minus the overlap, so the overlap should be less than the window length, but choosing Noverlap equal to length(window)-1 would generate a lot of needless computation (and possibly a very large image!), because the window skip would be. It is common to pick the overlap to be somewhere between 50 percent and 90 percent of the window length i.e.,, depending on how smooth the final spectrogram image needs to be. See help spectrogram in MATLAB for more details.",
        "769": "The spectrogram image can be displayed by using any one of MATLABs 3-D display functions. To get the inverted gray-scale images shown in this chapter, use the following:",
        "770": " imagesc( T, F, abs(S) ) axis xy, colormap(1-gray) The color map of (1-gray) gives a negative gray scale that is useful for printing, but on a computer screen it might preferable to use color, e.g., colormap(jet). Finally, it may be advantageous to use a logarithmic amplitude scale in imagesc in order to see tiny amplitude components, as well as big ones.",
        "771": "The Fast Fourier Transform (FFT) The material in this section is optional reading. It is included for completeness, since the FFT is the most important algorithm and computer program for doing spectrum analysis. Derivation of the FFT In Section~\u203b we discussed the FFT as an efficient algorithm for computing the DFT. In this section, we will give the basic divide-and-conquer method that leads to the FFT. From this derivation, it should be possible to write an FFT program that runs in time proportional to time. We need to assume that is a power of two, so that the decomposition can be carried out recursively. Such algorithms are called radix-2 algorithms.",
        "772": "The DFT summation and the IDFT summation are essentially the same, except for a minus sign in the exponent of the DFT and a factor of in the inverse DFT. Therefore, we will concentrate on the DFT calculation, knowing that a program written for the DFT could be modified to do the IDFT by changing the sign of the complex exponentials and multiplying the final values by. The DFT summation can be broken into two sets, one sum over the even-indexed points of and another sum over the odd-indexed points. At this point, two clever steps are needed: First, the exponent in the second sum must be broken into the product of two exponents, so we can factor out the one that does not depend on. Second, the factor of two in the exponents can be associated with the in the denominator of. Now we have the correct form. Each of the summations is a DFT of length, so we can write The formula for reconstructing from the two smaller DFTs has one hidden feature: It must be evaluated for. The -point DFTs give output vectors that contain elements; e.g., the DFT of the odd-indexed points would be for. Thus we need an extra bit of information to calculate, for. It is easy to verify that and likewise for the DFT of the even-indexed points, so we need merely to periodically extend the results of the -point DFTs before doing the sum in. This requires no additional computation.",
        "773": "The decomposition in is enough to specify the entire FFT algorithm: Compute two smaller DFTs and then multiply the outputs of the DFT over the odd indices by the exponential factor. Refer to Fig.~\u203b, where three levels of the recursive decomposition can be seen. If a recursive structure is adopted, the two DFTs can be decomposed into four -point DFTs, and those into eight -point DFTs, etc. If is a power of two, this decomposition will continue times and then eventually reach the point where the DFT lengths are equal to two. For two-point DFTs, the computation is trivial: The two outputs of the two-point DFT are the sum and the difference of the inputs. The last stage of computation would require two-point DFTs.",
        "774": "",
        "775": " Block diagram of a radix-2 FFT algorithm for. The width of the lines is proportional to the amount of data being processed. For example, each -point DFT must transform a data vector containing elements. FFT Operation Count The foregoing derivation is a bit sketchy, but the basic idea for writing an FFT program using the two-point DFT and the complex exponential as basic operators has been covered. However, the important point about the FFT is not how to write the program, but rather the number of operations needed to complete the calculation. When it was first published, the FFT made a huge impact on how people thought about problems, because it made the frequency-domain accessible numerically. Spectrum analysis became a routine calculation, even for very long signals. Operations such as filtering, which seem to be natural for the time-domain, could be done more efficiently in the frequency-domain for very long FIR filters.",
        "776": "The number of operations needed to compute the FFT can be expressed in a simple formula. We have said enough about the structure of the algorithm to count the number of operations. The count goes as follows: the point DFT can be done with two point DFTs followed by complex multiplications and complex additions, as we can see in. Thus, we have where is the number of complex multiplications for a length- DFT, and is the number of complex",
        "777": " Number of operations for radix-2 FFT when is a power of two. Notice how much smaller is than. 24 64 48 256 64 192 128 1024 160 512 320 4096 384 1280 768 16384 896 3072 1792 65536 2048 7168 4096 262144 additions. This equation can be evaluated successively for, because we know that and. Table~\u203b lists the number of operations for some transform lengths that are powers of two. The formula for each can be derived by matching the table:",
        "778": " Since complex number operations ultimately must be done as multiplies and adds between real numbers, it is useful to convert the number of operations to real adds and real multiplies. Each complex addition requires two real additions, but each complex multiplication is equivalent to four real multiplies and two real adds. Therefore, we can put two more columns in Table~\u203b with these counts.",
        "779": "The bottom line for operation counts is that the total count is something proportional to. The exact formulas from Table~\u203b are for the number of real multiplications and additions, respectively. Even these counts are a bit high because certain symmetries in the complex exponentials can be exploited to further reduce the computations.",
        "780": "Summary and Links In this chapter we introduced the discrete Fourier transform (DFT) and discrete-time Fourier transform (DTFT), and we have shown how these concepts can be useful in computing spectrum representations of signals and for understanding the behavior of linear systems. We showed how the DFT can be derived from the Fourier series integral for continuous-time periodic signals, and we showed how the DFT could be used in a variety of ways. We also showed the relationship between the DFT and the DTFT and illustrated how the DTFT can simplify the solution of problems involving linear time-invariant systems. In subsequent chapters, we will see how the DFT and DTFT are related to yet another transform called the -transform.",
        "781": "CHAPTER 9 -Transforms In this chapter we introduce the -transform, which brings polynomials and rational functions into the analysis of linear discrete-time systems. We will show that FIR convolution is equivalent to polynomial multiplication and that common algebraic operations, such as multiplying, dividing, and factoring polynomials, can be interpreted as combining or decomposing LTI systems. The most common { -transform}s are rational functions, i.e., a numerator polynomial divided by a denominator polynomial. The roots of these polynomials are important, because most properties of digital filters can be restated in terms of the locations of these roots.",
        "782": "The -transform method is introduced in this chapter for FIR filters and finite-length sequences in general. We will use the FIR case to introduce the important concept of \u201cdomains of representation\u201d for discrete-time signals and systems. In this text, we consider three domains of representation of signals and systems: the -domain or time domain (the domain of sequences, impulse responses and difference equations), the -domain or frequency domain (the domain of frequency responses and spectrum representations), and the -domain (the domain of -transforms, operators, and poles and zeros). The value of having three different domains of representation is that a difficult analysis in one domain is often much easier in one of the other domains. Therefore, increased understanding will result from developing skills for moving from one representation to another. For example, the cascade combination of LTI systems, which in the -domain seems to require the new (less familiar) technique of convolution, is converted in the -domain into the more familiar algebraic operation of polynomial multiplication. It is important, however, to note that the \u201creal\u201d or \u201cactual\u201d domain is the -domain where the signals are generated and processed, and where the implementation of filters takes place. The frequency domain has physical significance when analyzing sound, but is seldom used for implementation. The -domain exists primarily for its convenience in mathematical analysis and synthesis.",
        "783": "Definition of the -Transform A finite-length signal can be represented by the relation and the -transform of such a signal is defined by the formula where we will assume that represents any complex number; i.e., is the independent (complex) variable of the -transform. Although is the conventional definition of the -transform, it is instructive to note that can be written in the form which emphasizes the fact that is simply a polynomial of degree in the variable.",
        "784": "When we use to determine the -transform of the signal, we transform into a new representation. Indeed, it is often said that we \u201ctake the -transform of.\u201d All that we have to do to obtain is to construct a polynomial whose coefficients are the values of the sequence. Specifically, the th sequence value is the coefficient of the th power of in the polynomial. Obviously, it is just as easy to go from back to. We can recover from simply by extracting the coefficient of the th power of and placing that coefficient in the th position in the sequence. This operation is sometimes called taking the inverse -transform. In order to emphasize this unique correspondence between a sequence and its -transform, we will use the notation In general, a -transform pair is a sequence and its corresponding -transform, which we will denote as Notice that is the independent variable of the sequence. Thus, we say that represents the signal in the -domain. Since is often an index that counts time in a sampled time waveform, we also refer to as the time\u2013domain representation of the signal. Similarly, note that is the independent variable of the -transform. Thus, we say that represents the signal in the -domain, and in taking the -transform of a signal, we move from the time domain to the -domain.",
        "785": "As a simple, but very important, example of a -transform pair, suppose that. Then, from the definition,, it follows that. To emphasize this correspondence we use the notation When the sequence is defined with numerical values, we can take the -transform and get a polynomial.",
        "786": "EXAMPLE:\u00a0 -transform of a Signal Consider the sequence given in the following table: 0 1 2 3 4 5 0 0 2 4 6 4 2 0 The -transform of this sequence is This example shows how to find the -transform given the sequence. The following example illustrates the inverse -transform operation, i.e., finding the sequence if we are given its -transform.",
        "787": "EXAMPLE:\u00a0 Inverse -transform Consider the -transform given by the equation We can give in tabular form as in Example \u203b, or we can give an equation for the sequence values as a function of in the form Alternatively, using the representation in terms of impulse sequences, the corresponding sequence is At this point, we have a definition of the -transform, and we have seen how to find it for a given sequence and how to find the sequence given the -transform, but why would we want to transform from the -domain to the -domain? This is the obvious question to ask at this point, and the remainder of this chapter will attempt to answer it.",
        "788": "The -Transform and Linear Systems The { -transform} is indispensable in the design and analysis of LTI systems. The fundamental reason for this has to do with the way that LTI systems respond to the particular input signal for.",
        "789": "The -Transform of an FIR Filter Recall that the general difference equation of an FIR filter is An alternative representation of the input-output relation is the convolution sum where is the impulse response of the FIR filter. Remember that the impulse response is identical to the sequence of difference equation coefficients, as shown in the following table:",
        "790": " c c c c c c c c c c } & & 0 & 1 & 2 & & & & 0 & & & & & & 0 ",
        "791": "which can be represented in the more compact notation, ",
        "792": "To see why the -transform is of interest to us for FIR filters, let the input to system in be the signal where is any complex number. Recall that we have already considered such inputs in Chapter \u203b, where we used. As with our discussion of the frequency response, the qualification \u201cfor all \u201d is an extremely important detail. Because we want to avoid any consideration of what might happen at a starting point such as, we think of this as having the input start at, and we assume that for finite values of, the start-up effects have disappeared, i.e., we are concerned only with the \u201csteady-state\u201d part of the output. For the more general complex exponential input, the corresponding output signal is The term inside the parentheses is a polynomial in whose form depends on the coefficients of the FIR filter. It is called the system function of the FIR filter. From our previous definition of the -transform, this polynomial is observed to be the -transform of the impulse-response sequence. Using the notation introduced in the previous section, we define the system function of an FIR filter to be Therefore, we have the following important result: We have just shown that for FIR filters, if the input is for, then the corresponding output is That is, the result of convolving the sequence with the sequence is, where is the -transform of. This is a very general statement. In Chapter \u203b, it will be shown that it applies to any LTI system, not just FIR filters. Thus, the operation of convolution, which really is synonymous with the definition of an LTI system, appears to be closely linked to the -transform.",
        "793": "Equation is general enough to find the -transform representation of any FIR filter, because the polynomial coefficients are exactly the same as the filter coefficients from the FIR filter difference equation, or, equivalently, the same as the impulse-response sequence from. Thus, the FIR filter difference equation can be transformed easily into a polynomial in the -domain simply by replacing each \u201cdelay by \u201d (i.e., in ) by.",
        "794": "The system function is a function of the complex variable. As we have already noted, in is the -transform of the impulse response, and for the FIR case, it is an th-degree polynomial in the variable. Therefore will have zeros (i.e., values such that ) that (according to the fundamental theorem of algebra) completely define the polynomial to within a multiplicative constant.",
        "795": "EXAMPLE:\u00a0 Zeros of System Function Consider the FIR filter The -transform system function is Thus, the zeros of are and. Note that the filter has a system function with the same zeros, but the overall constant is 1 rather than 6. This simply means that. Find the system function of an FIR filter whose impulse response is Find the impulse response of an FIR filter whose system function is Hint: Multiply out the factors to get a polynomial and then determine the impulse response by \u201cinverse -transformation.\u201d",
        "796": "Properties of the -Transform In Section \u203b we gave the general definition of the -transform, and we showed that for finite-length sequences, it is possible to go uniquely back and forth between the sequence and its -transform. In this sense, we have demonstrated that the -transform is a unique representation of any finite-length sequence (including the impulse response of an FIR filter). In Section \u203b we showed that the -transform arises naturally out of the convolution of the impulse-response sequence with a sequence. In this section, we will explore several properties of the -transform representation and indicate how the -transform can be extended to the infinite-length case.",
        "797": "The Superposition Property of the z-Transform The -transform is a linear transformation. This is easily seen by considering the sequence, where both and are assumed to be finite with length less than or equal to. Using the definition of, we write Thus, we have demonstrated the superposition property for the -transform: This property leads to another way to interpret the -transform of a finite-length sequence, as illustrated in Example~\u203b.",
        "798": "EXAMPLE:\u00a0 -transform of a Signal Recall that any finite-length sequence can be represented as a sum of scaled and shifted impulse sequences as in Furthermore, recall from that for a single shifted unit impulse sequence, Thus, applying to each impulse in and then adding the individual -transforms according to, we obtain as before The Time-Delay Property of the z-Transform Another important property of the -transform is that the quantity in the -domain corresponds to a time shift of 1 in the -domain. We will illustrate this property with a numerical example. Consider the length-6 signal defined by the following table of values:",
        "799": " 0 1 2 3 4 5 0 3 1 4 1 5 9 0 The -transform of is the polynomial (in ) Recall that the signal values are the coefficients of the polynomial and that the exponents correspond to the time locations of the values. For example, the term indicates that the signal value at is 4, i.e., ",
        "800": "Now consider the effect of multiplying the polynomial by : The resulting polynomial is the -transform representation of a signal, which is found by using the polynomial coefficients and exponents in to determine the values of at all time positions. The result is the following table of values for :",
        "801": " 0 1 2 3 4 5 6 0 0 3 1 4 1 5 9 0 Each of the signal samples has moved over one position in the table; i.e.,. In general, for any finite-length sequence, multiplication of the -transform polynomial by simply subtracts one from each exponent in the polynomial, thereby creating a delay of one. Thus, we have the following fundamental relation: which we will refer to as the unit-delay property of the -transform.",
        "802": "The unit-delay property can be generalized for the case of shifting by more than one sample by simply applying times. The general result is A General z-Transform Formula So far, we have defined the -transform only for finite-length signals. Our definition assumes that the sequence is nonzero only in the interval. It is possible to extend the definition to signals of infinite length by simply extending the upper or lower limits to and respectively, i.e., However, infinite sums may cause serious mathematical difficulties and require special attention. Summing an infinite number of complex numbers could result in an infinite result. In mathematical terms, the sum might not converge. Although we will consider the infinite-length case in Chapter \u203b, the careful mathematical development of a complete -transform theory for signal and system analysis is better left to another, more advanced, course.",
        "803": "The -Transform as an Operator The delay property stated in Section \u203b suggests that the quantity is in some sense equivalent to a delay or time shift. This point of view leads to a useful, but potentially confusing, interpretation of the -transform as an operator. To see how this interpretation comes about, we will consider the system function of the unit-delay system.",
        "804": "Unit-Delay Operator The unit-delay system is one of the basic building blocks for the FIR difference equation. In the time domain, the unit-delay operator is defined by It is instructive to find the -transform representation of this system by letting the input to the unit-delay system be the signal where is a complex number. With the input signal, the output of the unit delay is simply In other words, the input signal is multiplied by, in the particular case where.",
        "805": "Strictly speaking, the expression in is misleading, because we must remember that it holds only for. However, it is common to use the quantity interchangeably with the unit-delay operator symbol, so that we can say that for any input the action of the unit-delay system is represented by the operator ; i.e., The brackets enclose the signal operated on by just as in. Thus, if we are careful in our interpretation, we can use the symbol to stand for the delay operator, and many authors use and interchangeably.",
        "806": "We know from the delay property of Section \u203b that if, then ; i.e., for any finite-length sequence, multiplies to produce. This is the precise way in which represents a unit delay; i.e., it is not appropriate to write without the brackets around, since this mixes the -domain and the -domain.",
        "807": "Operator Notation Consider a system that calculates the first difference of two successive signal values: The -transform operator that represents the first-difference system is, because we can write the \u201coperator\u201d equation This equation has the following interpretation: The operator \u201c1\u201d leaves unchanged, and the operator delays before subtracting it from.",
        "808": "Another simple example would be a system that delays by more than one sample, e.g., by samples: In this case, the system function is and the operator is, an obvious generalization of the unit-delay case.",
        "809": "Derive the -transform operator for the first-difference system by working the input through the system. Write as. Operator Notation in Block Diagrams The delay operator concept is particularly useful in block diagrams of LTI systems. In a block diagram representation of the FIR filter, the -transform works as follows: All the unit delays become operators in the transform domain, and, owing to the superposition property of the -transform, the scalar multipliers and adders are the same as in the time-domain representation. Figure \u203b shows the -domain representation of a block diagram for a two-point FIR filter\u2013-the operator represents the unit-delay operator.",
        "810": " Computational structure for a first-order FIR filter whose difference equation is. The block diagram uses to denote the unit delay, because they are equivalent. Draw a block diagram similar to Fig.~\u203b for the first difference system: ",
        "811": "Convolution and the -Transform In Section \u203b, we observed that a unit delay of a signal in the -domain is equivalent to multiplication by of the corresponding -transform in the -domain. The impulse response of the unit-delay system is so a delay by one sample is equivalent to the convolution",
        "812": " The system function of the unit-delay system is the -transform of its impulse response so Furthermore, the unit-delay property states that delay by one sample multiplies the -transform by ; i.e., Therefore, we observe that in the case of the unit delay, the -transform of the output is equal to the -transform of the input multiplied by the system function of the LTI system, i.e., More importantly, this result is true for any LTI system.",
        "813": "To show that convolution is converted into a product of { -transform}s, recall that the discrete convolution of two finite-length sequences and is given by the formula: where is the order of the FIR filter. To prove the desired result, we can apply the superposition property and the general delay property to find the -transform of as given by. This leads to If is a finite-length sequence, is a polynomial, so proves that convolution is equivalent to polynomial multiplication. This result is illustrated in the following example.",
        "814": "EXAMPLE:\u00a0 Convolution via The -transform method can be used to convolve the following signals: The -transforms of the sequences and are: Both and are polynomials in, so we can compute the -transform of the convolution by multiplying these two polynomials, i.e., Since the coefficients of any -polynomial are just the sequence values, with their position in the sequence being indicated by the power of, we can \u201cinverse transform\u201d to obtain Now we look at the convolution sum for computing the output. If we write out a few terms, we can detect a pattern that is similar to the -transform polynomial multiplication. Notice how the index of and the index of sum to the same value (i.e., ) for all products that contribute to. The same thing happens in polynomial multiplication because exponents add.",
        "815": "In Section \u203b on p.~ we demonstrated a synthetic multiplication tableau for evaluating the convolution of with. Now we see that this is also a process for multiplying the polynomials and. The procedure is repeated below for the numerical example of this section.",
        "816": " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 In the -transforms,, and, the power of is implied by the horizontal position of the coefficient in the tableau. Each row is produced by multiplying the row by one of the values and shifting the result right by the implied power of. The final answer is obtained by summing down the columns. The final row is the sequence of values of or, equivalently, the coefficients of the polynomial. In this section we have established that convolution and polynomial multiplication are essentially the same thing. Indeed, the most important result of -transform theory is: This result will be seen to have many implications far beyond its use as a basis for understanding and implementing convolution.",
        "817": "Use the -transform of and the system function to find the output of a first-difference filter when is the input. Compute your answer by using polynomial multiplication and also by using the difference equation: What is the degree of the output -transform polynomial that represents ? Cascading Systems One of the main applications of the -transform in system design is its use in creating alternative filters that have exactly the same input\u2013output behavior. An important example is the cascade connection of two or more LTI systems. In block diagram form, the cascade is drawn with the output of the first system connected to the input of the second. The input signal is and the overall output is. The sequence is an intermediate signal that can be thought of as temporary storage.",
        "818": " Cascade of two LTI systems. As we have already seen in Section \u203b on p.~, if and are the respective impulse responses of the first and second systems, then the overall impulse response from input to output in Fig.~\u203b is. Therefore, the -transform of the overall impulse response of the cascade of the two systems is the product of the individual -transforms of the two impulse responses. That is, An important consequence of this result follows easily from the fact that multiplication is commutative; i.e.,. This implies that convolution must also be a commutative operation and that the two systems can be cascaded in either order to obtain the same overall system response.",
        "819": "EXAMPLE:\u00a0 for Cascade To give a simple example of this idea, consider a system described by the difference equations which represent a cascade of two first-order systems as in Fig.~\u203b. The output of the first system is the input to the second system, and the overall output is the output of the second system. The intermediate signal in must be computed prior to being used in. We can combine the two filters into a single difference equation by substituting from the first system into the second, which gives Thus we have proved that the cascade of the two first-order systems is equivalent to a single second-order system. It is important to notice that the difference equation defines an algorithm for computing that is different from the algorithm specified by and together. However, the above analysis shows that with perfectly accurate computation, the outputs of the two different implementations would be exactly the same.",
        "820": "Working out the details of the overall difference equation as we have just done would be extremely tedious if the systems were higher-order. The -transform simplifies these operations into the multiplication of polynomials. The first-order systems have system functions: Therefore, the overall system function is which matches the difference equation in. Note that, even in this simple example, the -domain solution is more straightforward than the -domain solution. Use -transforms to combine the following cascaded systems into a single difference equation for in terms of. Factoring z-Polynomials If we can multiply -transforms to get higher-order systems, we can also factor -transform polynomials to break down a large system into smaller modules. Since cascading systems is equivalent to multiplying their system functions, the factors of a high-order polynomial would represent component systems that make up in a cascade connection.",
        "821": "EXAMPLE:\u00a0 Split into Cascade Consider the following example One of the roots of is, so is a factor of. The other factor can be obtained by division The factorization of as gives the cascade shown in the block diagram of Fig.~\u203b. The resulting difference equations for the cascade are Factoring into the product of a first-order system and a second-order system. Deconvolution The cascading property leads to an interesting question that has practical application. Can we use the second filter in a cascade to undo the effect of the first filter? What we would like is for the output of the second filter to be equal to the input to the first. Stated more precisely, suppose that we have the cascade of two filters and, and is known. Is it possible to find so that the overall system has its output equal to the input? If so, the -transform analysis tells us that its system function would have to be, so that Since the first system processes the input via convolution, the second filter tries to undo convolution, so the process is called deconvolution. Another term for this is inverse filtering, and if, then is said to be the inverse of (and vice versa).",
        "822": "EXAMPLE:\u00a0 Deconvolution If we take a specific example, we can generate a solution in terms of -transforms. Suppose that. We want, so we require that Since is known, we can solve for to get What are we to make of this example? It seems that the deconvolver for an FIR filter must have a system function that is not a polynomial, but a rational function (ratio of two polynomials) instead. This means that the inverse filter for an FIR filter cannot be also an FIR filter, and deconvolution suddenly is not as simple as it appeared. Since we have not yet considered the possibility of anything but polynomial system functions, we cannot give the solution in the form of a difference equation. However, in Chapter \u203b we will see that other types of LTI systems exist that do have rational system functions. We will therefore return to the inverse filtering problem in Chapter \u203b.",
        "823": "Relationship Between the z-Domain and the The system function has a functional form that is identical to the form of the frequency response formula. This is quite easy to see for the FIR filter if we repeat the formula for the frequency response alongside the formula for the system function : There is a clear correspondence between the - and -domains if we make the substitution in. Demo: 3-Domains Specifically, it is exceedingly important to note that the connection between and the -transform is ",
        "824": "The relationship between the -domain and the -domain hinges on the important formula To see why this relationship is the key, we need only recall that if the signal is the input to an LTI filter, the resulting output is. If the value of is, then where is obviously the same as what we have called the frequency response.",
        "825": "The -Plane and the Unit Circle The notation emphasizes the connection between the -domain and the -domain because it indicates explicitly that the frequency response is obtained from the system function by evaluating for a specific set of values of. Recall that since the frequency response is periodic with period, we need only evaluate it over one period, such as. If we substitute these values of into, we see that the corresponding values of all have unit magnitude and that the angle varies from to. In other words, the values of lie on a circle of radius 1 and range from the point all the way around the circle and back to the point. Quite naturally, the contour on which all the values of lie is called the unit circle. This is illustrated in Fig. \u203b, which shows the unit circle and a typical point, which is at a distance 1 from the origin and at an angle of with respect to the real axis of the -plane.",
        "826": "The graphical representation of Fig.~\u203b gives us a convenient way of visualizing the relationship between the -domain and the -domain. Because the -domain lies on a special part of the -domain \u2013- the unit circle \u2013- many properties of the frequency response are evident from plots of system function properties in the -plane. For example, the periodicity of the frequency response is obvious from Fig.~\u203b, which shows that evaluating the system function at all points on the unit circle requires moving through an angle of radians. Since frequency is equivalent to angle in the -plane, radians in the -plane correspond to an interval of radians of frequency. Continuing around the unit circle more times simply cycles through more periods of the frequency response.",
        "827": " The complex -plane including the unit circle, where. The angle around the circle is determined by the frequency. The orange shading of the unit circle corresponds to a frequency interval from 0 to. The gray dots on the unit circle are the points which correspond to. The Zeros and Poles of We have already seen that the system function for an FIR system is essentially determined by its zeros. This is illustrated by the following example.",
        "828": "EXAMPLE:\u00a0 Zeros and Poles of Consider the system function which can be expressed in the following different forms: or, if we multiply by, we obtain the following two equivalent forms: Equations \u2013 give four different equivalent forms of. The factored form in shows clearly that the zeros of are at locations,, and in the -plane. Equation also shows that for. Values of for which is undefined (infinite) are called poles of. In this case, we say that the term represents three poles at or that has a third-order pole at.",
        "829": "We have stated that the poles and zeros determine the system function to within a constant. As an illustration, note that the polynomial has exactly the same poles and zeros as in. Although it is perhaps less obvious, the locations of both the poles and the zeros are also clear when is written in the form, since each factor of the form always can be expressed as which shows that each factor of the form represents a zero at and a pole at. When contains only negative powers of, it is usually most convenient to use the representations of the form of and since the negative powers of have a direct correspondence to the difference equation and the impulse response.",
        "830": "It is useful to display the zeros and poles of as points in the complex -plane. The plot in Fig.~\u203b shows the three zeros and the three poles for Example \u203b. Such a plot is called a pole-zero plot. This plot was generated in MATLAB using the zplane function. Each zero location is denoted by a small circle, and the three poles at are indicated by a single with a numeral 3 beside it. In general, when all the poles are not concentrated at, the symbol will mark the location of each pole. Since the unit circle is where is evaluated to obtain the frequency response of the LTI system whose system function is, it is also shown for reference as a gray circle in Fig.~\u203b.",
        "831": " Zeros and poles of marked in a -plane that includes the unit circle. Significance of the Zeros of In Section \u203b we showed that the zeros of a polynomial system function are sufficient to determine except for a constant multiplier. The system function determines the difference equation of the filter because the polynomial coefficients of are the coefficients of the difference equation. The difference equation is the direct link between an input and the corresponding output. However, there are some inputs where knowledge of the zero locations is sufficient to make a precise statement about the output without actually computing it using the difference equation. Such signals are of the form for all, where the subscript signifies that is a particular complex number. In this case, the output is The quantity is a complex constant, which, through complex multiplication, causes a magnitude and phase change of the input signal. In particular, if is one of the zeros of, then so the output will be zero.",
        "832": "EXAMPLE:\u00a0 Nulling Signals with Zeros For example, when, the roots are As shown in Fig.~\u203b, these zeros are all on the unit circle, so complex sinusoids with frequencies,, and will be set to zero by the system. That is, the output resulting from each of the following three signals will be zero: As illustrated by this example, the zeros of the system function that lie on the unit circle correspond to frequencies at which the gain of the system is zero. Thus, complex sinusoids at those frequencies are blocked, or \u201cnulled\u201d by the system.",
        "833": " Demo: MATLAB GUI: PeZ Double-check the fact that the inputs,, and determined in Example \u203b produce outputs that are zero everywhere by substituting these signals into the difference equation to show that the complex phasors cancel out for all values of. Also show that since the filter is linear, it will also null out signals such as, which is the sum of and. Nulling Filters We have just shown that if the zeros of lie on the unit circle, then certain sinusoidal input signals are removed or nulled by the filter. Therefore, it should be possible to use this result in designing an FIR filter that can null a particular sinusoidal input. Such capability is often needed to eliminate jamming signals in a radar or communications system. Similarly, 60-Hz interference from a power line could be eliminated by placing a null at the correct frequency.",
        "834": "Zeros in the -plane can remove only signals that have the special form. If we want to eliminate a sinusoidal input signal, then we actually have to remove two signals of the form ; i.e., Each complex exponential can be removed with a first-order FIR filter, and then the two filters would be cascaded to form the second-order nulling filter that removes the cosine. The second-order FIR filter will have two zeros at and. The signal will be nulled by a filter with system function because at, i.e., Similarly, will remove. Thus the second-order nulling filter will be the product Figure \u203b shows the two zeros needed to remove components at. For the example depicted in Fig.~\u203b, the numerical values for the coefficients of are Thus the nulling filter that will remove the signal from its input is the FIR filter whose difference equation is ",
        "835": " Zeros on unit circle for second-order nulling filter to remove sinusoidal components at. There are two poles at the origin. Graphical Relation Between z and The equation provides the link between the -domain and the -domain. As we have shown in, the frequency response is obtained by evaluating the system function on the unit circle of the -plane. This correspondence can be given a useful graphical interpretation. By considering the pole-zero plot of the system function, we can visualize how the frequency response plot of results from evaluating on the unit circle, and also how it depends on the poles and zeros of. As an example, we show in Fig.~\u203b a plot obtained by evaluating the -transform magnitude over a region of the -plane that includes the unit circle as well as values both inside and outside the unit circle. The system in this case is an 11-point running sum; i.e., it is an FIR filter where the coefficients are all equal to one. The system function for this filter is In Section \u203b we will show that the zeros of the system function of this filter are on the unit circle at angles, for. This means that the polynomial in can be represented in the form Recall that each factor of the form represents a zero at and a pole at. Thus, displays the 10 zeros of at, for and the 10 poles at.",
        "836": " -transform for an FIR filter evaluated over the region of the -plane that includes the unit circle. Values along the unit circle are shown as a dark line where the frequency response (magnitude) is evaluated. The view is from the fourth quadrant, so the point is in the foreground on the right. In the magnitude plot of Fig.~\u203b, we observe that the zeros pin down the three-dimensional plot around the unit circle. Inside the unit circle, the values of become very large owing to the poles at. The frequency response is obtained by selecting the values of the -transform along the unit circle in Fig.~\u203b. A plot of versus is given in Fig.~\u203b. The shape of the frequency response can be explained in terms of the zero locations shown in Fig.~\u203b by recognizing that the poles at push up, but the zeros along the unit circle make at regular intervals except for the region near (i.e., around ). The unit circle values follow the ups and downs of as goes from to with.",
        "837": "This example illustrates that an intuitive picture of the frequency response of an LTI system can be visualized from a plot of the poles and zeros of the system function. We simply need to remember that a pole will \u201cpush up\u201d the frequency response and a zero will \u201cpull it down.\u201d Furthermore, a zero on the unit circle will force the frequency response to be zero at the frequency corresponding to the angular position of the zero.",
        "838": " Demo: 3-Domains Frequency response (magnitude only) for the 11-point running sum. These are the values along the unit circle in the -plane. There are 10 zeros spread out uniformly along the frequency axis. Frequency response (magnitude only) for the 11-point running sum. These are the values along the unit circle in the -plane. There are 10 zeros spread out uniformly along the frequency axis. One frame of a 3-Domain Movie showing the different domains: time domain, frequency domain, the -transform domain and a pole-zero plot. In this movie the zero pair moves around on the unit circle. ",
        "839": "Useful Filters Now that we understand the tie between the and domains, we can exploit that knowledge to design filters with desirable characteristics. In this section, we will look at a special class of bandpass filters (BPFs) that are all close relatives of the running-sum filter.",
        "840": "The -Point Running-Sum Filter Generalizing from the previous section, the -point running-sum filter has system function Recalling the formula from p.~ for the sum of terms of a geometric series, can be represented in the forms The final form for is a rational function where the numerator polynomial is and the denominator is. The zeros of will be determined by the roots of the numerator polynomial, i.e., the values of such that Since for, an integer, it is easy to see by substitution that each of the values satisfy and therefore these numbers are the roots of the th-order equation in. Because the values in satisfy the equation, they are called the th roots of unity. The zeros of the denominator in, which are either (of order ) or, would normally be the poles of. However, since one of the th roots of unity is, i.e., in, that zero of the numerator cancels the corresponding zero of the denominator, so that only the term really causes a pole of. Therefore, it follows that can be expressed in the factored form EXAMPLE:\u00a0 from For a 10-point running-sum filter ( ), the system function is A pole-zero diagram for this case is shown in Fig.~\u203b, and the corresponding frequency response for the running-sum filter is shown in Fig.~\u203b. The factors of the numerator are the tenth roots of unity, and the zero at is canceled by the corresponding term in the denominator. This explains why we have only nine zeros around the unit circle with the gap at. The nine zeros around the unit circle in Fig~\u203b show up as zeros along the axis in Fig.~\u203b at, and it is the gap at that allows the frequency response to be larger at. The other zeros around the unit circle keep small, thereby creating the \u201clowpass\u201d filter frequency response shown in Fig.~\u203b.",
        "841": " Zero and pole distribution for the 10-point running-sum filter. There are nine zeros spread out uniformly along the unit circle, and nine poles at the origin. Zero and pole distribution for the 10-point running-sum filter. There are nine zeros spread out uniformly along the unit circle, and nine poles at the origin. A Complex Bandpass Filter Now we have a new insight that tells us how to control the frequency response of an FIR filter by placing its zeros on the unit circle. This viewpoint makes it easy to create other FIR filters where we control the location of the passband. If we move the passband to a frequency away from, then we have a filter that passes a small band of frequency components\u2013-a bandpass filter, or BPF.",
        "842": "The obvious way to move the passband is to use all but one of the roots of unity as the zeros of an FIR filter. A formula for this new filter is where the index denotes the one omitted root at. An example is shown in Fig.~\u203b for and. In the general case, the passband of the filter whose system function is given by should move from the interval around to a like interval around because the zero is missing at that frequency. Figure~\u203b confirms our intuition, because with the normalized frequency of the peak is. This filter is a bandpass filter, since frequencies outside the narrow band around are given much less gain than those in the passband of the filter.",
        "843": " Zero and pole distribution for the 10-point complex BPF. The zero at angle is the one missing from the tenth roots of unity. The other nine zeros are spread out at equal angles around the unit circle; there are nine poles at the origin. Zero and pole distribution for the 10-point complex BPF. The zero at angle is the one missing from the tenth roots of unity. The other nine zeros are spread out at equal angles around the unit circle; there are nine poles at the origin. The formula in is ideal for seeing how to make the frequency response of a BPF, but it is not very useful for calculating the filter coefficients of the bandpass filter. If one attempts a direct multiplication of the factors in, nine complex terms must be combined. When all the algebra is finished, the resulting filter coefficients will turn out to be complex-valued. This fact is obvious if we realize that the zeros in Fig.~\u203b cannot all be grouped as complex-conjugate pairs.",
        "844": "Another strategy is needed to get the filter coefficients. One idea is to view the zero distribution in Fig.~\u203b as a rotation of the zeros of the running-sum filter in Fig.~\u203b. Note that rotation of the -plane representation will have the corresponding effect of shifting the frequency response along the -axis by the amount of the rotation. The desired rotation in this case is by the angle. So the question is how to move the roots of a polynomial through a rotation. The answer is that we must multiply the th filter coefficient by where is the desired angle of rotation.",
        "845": "Consider the following general operation on a polynomial : Every occurrence of the variable in the polynomial is replaced by. The effect of this substitution on the roots of is to multiply them by and make these the roots of. For the simple example, The two roots of are now and.",
        "846": "In the case of the complex bandpass filter, is the running-sum system function and the parameter is a complex exponential. Remember that multiplication by a complex exponential will rotate a complex number through the angle. Now it is easy to get the new filter coefficients Thus the filter coefficients of the complex bandpass filter are Another way to determine the frequency response of the complex bandpass filter is to compute it directly, as in This equation shows that the frequency response of the system whose filter coefficients are given by is a shifted (by ) version of the frequency response of the -point running-sum filter. A Bandpass Filter with Real Coefficients The obvious disadvantage of the previous strategy is that the resulting filter coefficients are complex. We can modify the strategy slightly to get a bandpass filter with real coefficients if we just take the real part of the complex BPF coefficients. Thus the th filter coefficient is now With these real filter coefficients, the new BPF can be written as the sum of two complex BPFs. By expanding the coefficients of in terms of complex exponentials we obtain where is a complex bandpass filter centered on frequency and is a complex bandpass filter centered on frequency. Figure~\u203b shows the frequency response for and. There are zeros of the frequency response at some of the frequencies because both component filters have zeros at all these frequencies except for. As is the case with any real-valued filter, the magnitude of the frequency response exhibits a symmetry about.",
        "847": " Frequency response (magnitude only) for the 10-point real BPF. Notice the two passbands at \u2014 one at positive frequency, the other at negative frequency. The frequency response in Fig.~\u203b has two peaks at, so there must be two missing zeros on the unit circle at angles. In Fig.~\u203b we see that the two zeros at and have been replaced by a single real zero. Thus, there are eight zeros on the unit circle and one on the real axis for a total of nine zeros, which is the order of the -transform polynomial. The location of this new zero appears to be at, which is the real part of the missing unit-circle zeros.",
        "848": " Pole-zero distribution for the 10-point real BPF. Of the original 10 roots of unity, two have been dropped off the unit circle at the angles, but a new one appears on the real axis. There are nine poles at the origin. An algebraic manipulation will uncover the exact location of the new zero. We use the numerator-denominator representation and combine the two terms over a common denominator. To make the notation simpler, let, so that the conjugate is. Then, The two factors in the denominator cancel corresponding factors in the numerator polynomial, leaving (in this case ) of the th roots of unity. The term is the new zero at which is the real part of the canceled zeros.",
        "849": "Practical Bandpass Filter Design Although much better filters can be designed by more sophisticated methods, the example of bandpass filter design discussed in Section \u203b is a useful illustration of the power of the -transform to simplify the analysis of such problems. Its characterization of a filter by the zeros (and poles) of is used continually in filter design and implementation. The underlying reason is that the -transform converts difficult problems involving convolution and frequency response into simple algebraic ideas based on multiplying and factoring polynomials. Thus, skills with basic algebra become essential everyday tools in design.",
        "850": "As a final example of FIR filters, we present a high-order FIR filter that has been designed by a computer-aided filter design program. Most digital filter design is carried out by software tools that permit much more sophisticated control of the passband and stopband characteristics than we were able to achieve with the simple analysis of Section \u203b. Design programs such as remez and fir1 can be found in the MATLAB software augmented with the Signal Processing Toolbox.",
        "851": "Although it is not our intention to discuss any of these methods or even how they are used, it is interesting to examine the output from the program fir1 to get a notion of what can be achieved with a sophisticated design method.",
        "852": "The software allows us to specify the frequency range for the passband, and then computes a good approximation to an ideal filter that has unity gain over the passband and zero gain in the stopband. An example is shown in Figs.~\u203b, \u203b, and \u203b for a length\u201324 FIR bandpass filter.",
        "853": " Impulse response for the 24-point BPF designed with MATLABs fir1 function. These are the FIR filter coefficients,, which are needed in the difference equation implementation. ",
        "854": " Frequency response (magnitude only) for the 24-point BPF. The passbands are the intervals. The design was performed with fir1(23,2*[0.2,0.4],kaw) where kaw = kaiser(24,2.75) is a Kaiser window. The impulse response of the 24-point FIR bandpass filter is given in Fig.~\u203b. Notice that the coefficients would be labeled, from. The plot shows an obvious symmetry about a midpoint at. Table~\u203b lists the values for the impulse response and filter coefficients.",
        "855": " Filter coefficients of 24-point FIR bandpass. These are the coefficients used in the difference equation to implement the filter whose frequency response is shown in Fig.~\u203b. Notice the wide passbands in the frequency ranges corresponding to. In these intervals, the gain of the filter deviates only slightly from one, so the amplitudes of sinusoidal signals with these frequencies will not be affected by the filter. We will see in Section~\u203b that this filter has linear phase, so these frequencies will only experience delay.",
        "856": "Also note that in the regions and the gain is very nearly equal to zero. These regions are the \u201cstopbands\u201d of the filter, since the amplitudes of sinusoids with these frequencies will be multiplied by a very small gain and thus blocked from appearing in the output.",
        "857": "Observe that the frequency response tapers smoothly from the passbands to the stopbands. In these transition regions, sinusoids will be reduced in amplitude according to the gain shown in Fig.~\u203b. In many applications, we would want such transition regions to be very narrow. Ideally, we might even want them to have zero width. While this is theoretically achievable, it comes with a high price. It turns out that for an FIR frequency selective (lowpass, bandpass, highpass) filter, the width of the transition region is inversely proportional to, the order of the system function polynomial. The higher the order, the narrower the transition regions can be, and as, the transition regions shrink to zero. Unfortunately, increasing also increases the amount of computation required to compute each sample of the output, so a trade-off is always required in any practical application.",
        "858": "Figure \u203b shows the pole-zero plot of the FIR filter. Notice the distinctive pattern of locations of the zeros. In particular, note how the zeros off the unit circle seem to be grouped into groups of four zeros. Indeed, for each zero that is not on the unit circle, there are also zeros at the conjugate location, at the reciprocal location, and at the conjugate reciprocal location. These groups of four zeros are strategically placed by the design process to form the passband of the filter. Similarly, the design process places zeros on (or close to) the unit circle to ensure that the gain of the filter is low in the stopband regions of the frequency axis. Also, note that all complex zeros appear in conjugate pairs, and since the system function is a twenty-third-order polynomial, there are 23 poles at. In the Section~\u203b, we will show that these properties of the pole-zero distribution are the direct result of the symmetry of the filter coefficients.",
        "859": " Zero and pole distribution for a 24-point BPF designed with MATLABs fir1 function. The section of unit circle corresponding to the passband of in Fig.~\u203b is outlined in black. ",
        "860": "Properties of Linear-Phase Filters The filter that was discussed in Section \u203b is an example of a class of systems where the sequence of coefficients (impulse response) has symmetry of the form, for. Such systems have a number of interesting properties that are easy to show in the -domain representation.",
        "861": "The Linear-Phase Condition FIR systems that have symmetric filter coefficients (and, therefore, symmetric impulse responses) have frequency responses with linear phase. An example that we have already studied is the -point running sum, whose coefficients are all the same and therefore clearly satisfy the condition, for. The example of Section \u203b also has linear phase because it satisfies the same symmetry condition. To see why linear phase results from this symmetry, let us consider a simple example where the system function is of the form Thus, and the length of the sequence is samples. After working out the frequency response for this special case, the generalization will be obvious. First, observe that we can write as If were greater, we would simply have more groups of factors of the form. Now when we substitute, each of these factors will become a cosine term, i.e., In our specific example, we have shown that is of the form where, in this case, and is the real function By following this style of analysis for the general case, it is easy to show that holds whenever, for. In the general case, it can be shown that the result depends on whether the integer is even or odd Equation shows that the frequency response of any symmetric filter has the form of a real amplitude function times a linear-phase factor. The latter factor, as we have seen in Section \u203b on p.~, corresponds to a delay of samples. Thus, the analysis presented in Section \u203b for the running-average filter is typical of what happens in the general symmetric FIR case. The main difference is that by choosing the filter coefficients carefully, as in Section \u203b, we can shape the function to have a much more selective frequency response.",
        "862": "Locations of the Zeros of FIR Linear-Phase Systems If the filter coefficients satisfy the condition, for, it follows that To demonstrate this \u201creciprocal property\u201d of linear-phase filters, consider a 4-point system of the form Following the same style for a general, is easily proved for the general case.",
        "863": "The reciprocal property of linear-phase filters is responsible for the distinctive pattern of zeros in the pole-zero plot of Fig.~\u203b. The zeros that are not on the unit circle occur as quadruples. Furthermore, these quadruples are responsible for creating the passband of the BPF. Zeros on the unit circle occur in pairs because the complex conjugate partner must be present, and these zeros are mainly responsible for creating the stopband of the filter.",
        "864": "These properties can be shown to be true in general for linear-phase filters. Specifically,",
        "865": " When, for, then if is a zero of, so are its conjugate, its inverse, and its conjugate inverse; i.e., are all zeros of. The conjugate zeros are included because the filter coefficients, which are also the coefficients of, are real. Therefore, all of the zeros of must occur in complex-conjugate pairs. The inverse property is true because the filter coefficients are symmetric. Using, and assuming that is a zero of, we get Since, then we must have also. Most FIR filters are designed with a symmetry property, so the zero pattern of Fig.~\u203b is typical.",
        "866": "Summary and Links The -transform method was introduced in this chapter for FIR filters and finite-length sequences in general. The -transform reduces the manipulation of LTI systems into simple operations on polynomials and rational functions. Roots of these -transform polynomials are quite important because filter properties such as the frequency response can be inferred directly from the root locations.",
        "867": "We also introduced the important concept of \u201cdomains of representation\u201d for discrete-time signals and systems. There are three domains: the -domain or time domain, the -domain or frequency domain, and the -domain. With three different domains at our disposal, even the most difficult problems generally can be simplified by switching to one of the other domains.",
        "868": "Among the laboratory projects on the website, we have already provided two on the topic of FIR filtering in Chapters~\u203b and~\u203b. Lab #7 deals with FIR filtering of sinusoidal waveforms, convolution and deconvolution. Lab #8 deals with the frequency response of common filters such as the first difference and the -point averager. In Labs #9 and #10, FIR filters will be used in practical systems, such as a touch-tone decoder in Lab #9, and piano note detection in Lab #10. Each of these labs should be easier to understand and simpler to carry out with the newly acquired background on -transforms.",
        "869": " Lab: #9: Encoding and Decoding Touch-Tones Lab: #10: Octave Band Filtering The website also contains some demonstrations of the relationship between the -plane and the frequency domain and time domain.",
        "870": " A three-domain movie that shows how the frequency response and the impulse response of an FIR filter change as a zero location is moved. Several different filters are demonstrated.",
        "871": " Demo: 3-Domains FIR A movie that animates the relationship between the -plane and the unit circle where the frequency response lies.",
        "872": " Demo: Z to Freq The MATLAB program PeZ, to facilitate exploring the three domains. The M-files for PeZ can be copied and run under MATLAB.",
        "873": " Demo: PeZ GUI A tutorial on how to run PeZ. Demo: PeZ Tutorial ",
        "874": "CHAPTER 10 IIR Filters This chapter introduces a new class of LTI systems that have infinite duration impulse responses. For this reason, systems of this class are often called infinite-impulse-response (IIR) systems or also IIR filters. In contrast to FIR filters, IIR digital filters involve previously computed values of the output signal as well as values of the input signal in the computation of the present output. Since the output is \u201cfed back\u201d to be combined with the input, these systems are examples of the general class of feedback systems. From a computational point of view, since output samples are computed in terms of previously computed values of the output, the term recursive filter is also used for these filters.",
        "875": "The -transform system functions for IIR filters are rational functions that have both poles and zeros at nonzero locations in the -plane. Just as for the FIR case, we will show that many insights into the important properties of IIR filters can be obtained directly from the pole-zero representation.",
        "876": "We begin this chapter with the first-order IIR system, which is the simplest case because it involves feedback of only the previous output sample. We show by construction that the impulse response of this system has an infinite duration. Then the frequency response and the -transform are developed for the first-order filter. After showing the relationship among the three domains of representation for this simple case, we consider second-order filters. These filters are particularly important because they can be used to model resonances such as would occur in a speech synthesizer, as well as many other natural phenomena that exhibit vibratory behavior. The frequency response for the second-order case can exhibit a narrowband character that leads to the definition of bandwidth and center frequency, both of which can be controlled by appropriate choice of the feedback coefficients of the filter. The analysis and insights developed for the first- and second-order cases are readily generalized to higher-order systems.",
        "877": "The General IIR Difference Equation FIR filters are extremely useful and have many nice properties, but that class of filters is not the most general class of LTI systems. This is because the output is formed solely from a finite segment of the input signal. The most general class of digital filters that can be implemented with a finite amount of computation is obtained when the output is formed not only from the input, but also from previously computed outputs. Demo: IIR Filtering The defining equation for this class of digital filters is the difference equation",
        "878": " ",
        "879": "The filter coefficients consist of two sets: and. For reasons that will become obvious in the following simple example, the coefficients are called the feedback coefficients, and the are called the feed-forward coefficients. In all, coefficients are needed to define the recursive difference equation.",
        "880": "Notice that if the coefficients are all zero, the difference equation reduces to the difference equation of an FIR system. Indeed, we have asserted that defines the most general class of LTI systems that can be implemented with finite computation, so FIR systems must be a special case. When discussing FIR systems we have referred to as the order of the system. In this case, is the number of delay terms in the difference equation and the degree or order of the polynomial system function. For IIR systems, we have both and as measures of the number of delay terms, and we will see that the system function of an IIR system is the ratio of an th-order polynomial to an th-order polynomial. Thus, there can be some ambiguity as to the order of an IIR system. In general, we will define, the number of feedback terms, to be the order of an IIR system.",
        "881": "EXAMPLE:\u00a0 IIR Block Diagram Rather than tackle the general form given in, consider the first-order case where, i.e., The block diagram representation of this difference equation, which is shown in Fig.~\u203b, is constructed by noting that the signal is computed by the left half of the diagram, and we \u201cclose the loop\u201d by computing from the delayed output and adding it to to produce the output. This diagram clearly shows that the terms feed-forward and feedback describe the direction of signal flow in the block diagram.",
        "882": " First-order IIR system showing one feedback coefficient and two feed-forward coefficients and. We will begin by studying a simplified version of the system defined by and depicted in Fig.~\u203b. This will involve characterizing the filter in each of the three domains: time domain, frequency domain, and -domain. Since the filter is defined by a time\u2013domain difference equation, we begin by studying how the difference equation is used to compute the output from the input, and we will illustrate how feedback results in an impulse response of infinite duration.",
        "883": "Time\u2013Domain Response To illustrate how the difference equation can be used to implement an IIR system, we will begin with a numerical example. Assume that the filter coefficients in are,, and, so that and assume that the input signal is In other words, the total duration of the input is four samples, as shown in Fig.~\u203b.",
        "884": " Input signal to recursive difference equation. ",
        "885": "Although it is not a requirement, it is logical that the output sequence values should be computed in normal order; i.e., from left to right in a plot of the sequence. Furthermore, since the input is zero before, it would be natural to assume that is the starting time of the output. Thus, we will consider computing the output from the difference equation in the order. For example, the value of is 2, so we can evaluate at obtaining Immediately we run into a problem: The value of at is not known. This is a serious problem, because no matter where we start computing the output, we will always have the same problem; at any point along the -axis, we will need to know the output at the previous time. If we know the value, however, we can use the difference equation to compute the next value of the output signal at time. Once the process is started, it can be continued indefinitely by iteration of the difference equation. The solution requires the following two assumptions, which together are called the initial rest conditions. ",
        "886": " INITIAL REST CONDITIONS The input must be assumed to be zero prior to some starting time, i.e., for. We say that such inputs are suddenly applied. The output is likewise assumed to be zero prior to the starting time of the signal, i.e., for. We say that the system is initially at rest if its output is zero prior to the application of a suddenly applied input. These conditions are not particularly restrictive, especially in the case of a real-time system, where a new output must be computed as each new sample of the input is taken. Real-time systems must, of course, be causal in the sense that the computation of the present output sample must not involve future samples of the input or output. Furthermore, any practical device would have a time at which it first begins to operate. All that is needed is for the memory containing the delayed output samples to be set initially to zero.",
        "887": "With the initial rest assumption, we let for, so now we can evaluate as Once we have started the recursion, the rest of the values follow easily, since the input signal and previous outputs are known. This output sequence is plotted in Fig.~\u203b up to.",
        "888": " Output signal from recursive difference equation of for the input of Fig.~\u203b. For, the sequence is proportional to, because the input signal ends at. One key feature to notice in Fig.~\u203b is the structure of the output signal after the input turns off. For this range of, the difference equation becomes Thus the ratio between successive terms is a constant, and the output signal decays exponentially with a rate determined by. Therefore, we can write the closed form expression for the rest of the sequence once the value for is known.",
        "889": "Linearity and Time Invariance of IIR Filters When applied to the general IIR difference equation of, the condition of initial rest is sufficient to guarantee that the system implemented by iterating the difference equation is both linear and time-invariant. Although the feedback terms make the proof more complicated than the FIR case (see Section \u203b on p.~), we can show that, for suddenly applied inputs and initial rest conditions, the principle of superposition will hold because the difference equation involves only linear combinations of the input and output samples. Furthermore, since the initial rest condition is always applied just before the beginning of a suddenly applied input, time invariance also holds.",
        "890": "Assume that the input to the difference equation is, where is given by and Fig.~\u203b. Use iteration to compute the corresponding output for using the assumption of initial rest. Compare your result to the output plotted in Fig.~\u203b and verify that the system behaves as if it is both linear and time-invariant. Impulse Response of a First-Order IIR System In Chapter \u203b we showed that the response to a unit impulse sequence characterizes a linear time-invariant system completely. Recall that when, the resulting output signal, denoted by, is by definition the impulse response. Since all other input signals can be written as a superposition of weighted and delayed impulses, the corresponding output for all other signals can be constructed from weighted and shifted versions of the impulse response,. That is, since the recursive difference equation with initial rest conditions is an LTI system, its output can always be represented as the convolution sum Therefore, it is of interest to characterize the recursive difference equation by its impulse response.",
        "891": "To illustrate the nature of the impulse response of an IIR system, consider the first-order recursive difference equation with, By definition, the difference equation must be satisfied by the impulse response for all values of. We can construct a general formula for the impulse response in terms of the parameters and by simply constructing a table of a few values and then writing down the general formula by inspection. The following table shows the sequences involved in the computation:",
        "892": " 0 1 2 3 4 0 1 0 0 0 0 From this table it is obvious that the general formula is } 0 & } If we recall the definition of the unit step sequence } 0 & } can be expressed in the form } 0 & } where multiplication of by provides a compact representation of the conditions and.",
        "893": "EXAMPLE:\u00a0 Impulse Response For the example of with and, the impulse response is } 0 & } This is the impulse response of the system in. Substitute the solution into the difference equation and verify that the difference equation is satisfied for all values of. Find the impulse response of the following first-order system: Assume that the system is at rest for. Plot the resulting signal as a function of. Pay careful attention to where the nonzero portion of the impulse response begins. A slightly more general problem would be to find the impulse response of the first-order system when a shifted version of the input signal is also included in the difference equation, i.e., Because this system is linear and time-invariant, it follows that its impulse response can be thought of as a sum of two terms as in Notice that the impulse response still decays exponentially with rate dependent only on.",
        "894": "Find the impulse response of the following first-order system: Plot the resulting impulse response as a function of. Response to Finite-Length Inputs For finite-length inputs, the convolution sum is easy to evaluate for either FIR or IIR systems. Suppose that the finite-length input sequence is so that for and. Then it follows from that the corresponding output satisfies EXAMPLE:\u00a0 IIR Response to General Input As an example, consider again the LTI system defined by the difference equation, whose impulse response was shown in Example~\u203b to be For the input of and Fig.~\u203b, it is easily seen that To evaluate this expression for a specific time index, we need to take into account the different regions over which the individual terms are nonzero. If we do, we obtain A comparison to the output obtained by iterating the difference equation (see Fig.~\u203b on p.~) shows that we have obtained the same values of the output sequence by superposition of scaled and shifted impulse responses. Example~\u203b illustrates two important points about IIR systems.",
        "895": " The initial rest condition guarantees that the output sequence does not begin until the input sequence begins (or later). Because of the feedback, the impulse response is infinite in extent, and the output due to a finite-length input sequence, being a superposition of scaled and shifted impulse responses, is generally (but not always) infinite in extent. This is in contrast to the FIR case, where a finite-length input always produces a finite-length output sequence. Find the impulse response of the first-order system and use it to find the output due to the input signal Write a formula that is the sum of four terms, each of which is a shifted impulse response. Assume the initial rest condition. Plot the resulting signal as a function of for. Step Response of a First-Order Recursive System When the input signal is infinitely long, the computation of the output of an IIR system using the difference equation is no different than for an FIR system; we simply continue to iterate the difference equation as long as samples of the output are desired. In the FIR case, the difference equation and the convolution sum are the same thing. This is not true in the IIR case, and computing the output using convolution is practical only in cases where simple formulas exist for both the input and the impulse response. Thus, in general, IIR filters must be implemented by iterating the difference equation. The computation of the response of a first-order IIR system to a unit step input provides a relatively simple illustration of the issues involved.",
        "896": "Again, assume that the system is defined by and assume that the input is the unit step sequence given by } 0 & } As before, the difference equation can be iterated to produce the output sequence one sample at a time. The first few values are tabulated here. Work through the table to be sure that you understand the computation.",
        "897": " 0 0 0 1 1 1 2 1 3 1 4 1 1 From the tabulated values, it is not difficult to see that a general formula for is With a bit of manipulation, we can get a simple closed-form expression for the general term in the sequence. For this we need to recall the formula which is the formula for summing the first terms of a geometric series. Armed with this fact, the formula for (when ) becomes ",
        "898": "Three cases can be identified:,, and. Further investigation of these cases reveals two types of behavior.",
        "899": " When, the term in the numerator will dominate and the values for will get larger and larger without bound. This is called an unstable condition and is usually a situation to avoid. We will say more about the issue of stability later in Sections \u203b and \u203b. When, the term will decay to zero as. In this case, the system is stable. Therefore, we can find a limiting value for as When, we might have an unbounded output, but not always. For example, when, we get for, and the output grows as. On the other hand, for, the output alternates; it is for even, but for odd. ",
        "900": "The MATLAB plot in Fig.~\u203b shows the step response for the filter Notice that the limiting value is, which can be calculated from the filter coefficients (a) Unit-step signal, (b) Step response of a first-order IIR filter. Now suppose that we try to compute the step response by the convolution sum Since both the input and the impulse response have infinite durations, we might have difficulty in carrying out the computation. However, the fact that the input and output are given by the formulas and makes it possible to obtain a result. Substituting these formulas into gives The and terms inside the sum will change the limits on the sum because for and for (or ). The final result is y[n] = which is identical to, the step response computed by iterating the difference equation. Notice that we were able to arrive at a closed-form expression in this case, because of the special nature of the input and impulse response. In general, it would be difficult or impossible to obtain such a closed-form result, but we can always use iteration of the difference equation to compute the output sample-by-sample.",
        "901": "System Function of an IIR Filter We saw in Chapter \u203b for the FIR case that the system function is the -transform of the impulse response of the system, and that the system function and the frequency response are intimately related. Furthermore, the following result was shown:",
        "902": " ",
        "903": "The same is true for the IIR case. The system function for an FIR filter is always a polynomial; however, when the difference equation has feedback, it turns out that the system function is the ratio of two polynomials. Ratios of polynomials are called rational functions. In this section we will determine the system function for the example of a first-order IIR system, and show how the system function, impulse response, and difference equation are related.",
        "904": "The General First-Order Case The general form of the first-order difference equation with feedback is Since this equation must be satisfied for all values of, we can take the -transform of both sides of the equation to obtain Subtracting the term from both sides of the equation leads to the following manipulations: Since the system is an LTI system, it should be true that, where is the system function of the LTI system. Solving this equation for, we obtain Thus, we have shown that for the first-order IIR system is a ratio of two polynomials. The numerator polynomial is defined by the weighting coefficients that multiply the input signal and its delayed versions; the denominator polynomial is defined by the feedback coefficients. That this correspondence is true in general should be clear from the analysis that leads to the formula for. Indeed, the following is true for IIR systems of any order:",
        "905": " ",
        "906": "In MATLAB, the filter function follows this same format. The statement implements an IIR filter, where the vectors bb and aa hold the filter coefficients for the numerator and denominator polynomials, respectively.",
        "907": "EXAMPLE:\u00a0 MATLAB for IIR Filter % The following feedback filter: would be implemented in MATLAB by where xx and yy are the input and output signal vectors, respectively. Notice that the aa vector has for its second element, just like in the polynomial. We can imagine that the filter coefficient multiplying is 1, so we always have 1 for the first element of aa. ",
        "908": "Find the system function (i.e., -transform) of the following feedback filter: ",
        "909": "Determine the system function of the system implemented by the following MATLAB statement: ",
        "910": "The System Function and Block-Diagram Structures As we have seen, the system function displays the coefficients of the difference equation in a convenient way that makes it easy to move back and forth between the difference equation and the system function. In this section, we will show that this makes it possible to derive other difference equations, and thus other implementations, by simply manipulating the system function.",
        "911": " Direct Form I Structure To illustrate the connection between the system function and the block diagram, let us return to the block diagram of Fig.~\u203b, which is repeated in Fig.~\u203b for convenience. Block diagrams such as Fig.~\u203b are called implementation structures, or, more commonly, simply structures, because they give a pictorial representation of the difference equations that can be used to implement the system.",
        "912": " First-order IIR system showing one feedback coefficient and two feed-forward coefficients and in Direct Form I structure. Recall that the product of two -transform system functions corresponds to the cascade of two systems. The system function for the first-order feedback filter can be factored into an FIR piece and an IIR piece, as in The conclusion to be drawn from this algebraic manipulation is that a valid implementation for is the pair of difference equations: We see in Fig.~\u203b that the polynomial is the system function of the feed-forward part of the block diagram, and that is the system function of a feedback part that completes the system. The system implemented in this way is called the Direct Form I implementation because it is possible to go directly from the system function to this block diagram (or the difference equations that it represents) with no other manipulations than to write the numerator and denominator as polynomials in the variable.",
        "913": " Direct Form II Structure We know that for an LTI cascade system, we can change the order of the systems without changing the overall system response. In other words, Using the correspondences that we have established leads to the block diagram shown in Fig.~\u203b. Note that we have defined a new intermediate variable as the output of the feedback part and input to the feed-forward part. Thus, the block diagram tells us that an equivalent implementation of the system is Again, because there is such a direct and simple correspondence between Fig.~\u203b and, this implementation is called the Direct Form II implementation of the first-order IIR system with system function First-order IIR system showing one feedback coefficient and two feedforward coefficients and in a Direct Form II structure. ",
        "914": "The block-diagram representation of Fig.~\u203b leads to a valuable insight. Notice that the input to each of the unit delay operators is the same signal. Thus, there is no need for two delay operations; they can be combined into a single delay, as in Fig.~\u203b. Since delay operations are implemented with memory in a computer, the implementation of Fig.~\u203b would require less memory that the implementation of Fig.~\u203b. Note, however, that both block diagrams represent the difference equations and.",
        "915": " First-order IIR system in Direct Form II structure. This is identical to Fig.~\u203b, except that the two delays have been merged into one. Find the -transform system function of the following set of cascaded difference equations: Draw the block diagrams of this system in both Direct Form I and Direct Form II. The Transposed Form Structure A somewhat surprising fact about block diagrams like Fig.~\u203b is that if the block diagram undergoes the following transformation:",
        "916": " All the arrows are reversed with multipliers being unchanged in value or location. All branch points become summing points, and all summing points become branch points. The input and the output are interchanged. then the overall system has the same system function as the original system. We will not prove this, but it is true for the kinds of block diagrams that we have just introduced. However, we can use the -transform to verify that this is true for our simple first-order system.",
        "917": "The feedback structure given in the signal-flow graph of Fig.~\u203b is the transposed form of the Direct Form II structure shown in Fig.~\u203b. In order to derive the actual difference equations, we need to write the equations that are defined by the signal-flow graph. There is an orderly procedure for doing this if we follow two rules:",
        "918": " Assign variable names to the inputs of all delay elements. For example, is used in Fig.~\u203b, so the output of the delay is. Write equations at all of the summing nodes; there are two in this case. The signal-flow graph specifies an actual computation, so and require three multiplies and two adds at each time step. Equation must be done first, because is needed in.",
        "919": " Computational structure for a general first-order IIR filter as a transposed Direct Form II structure. Owing to the feedback, it is impossible to manipulate these equations into one of the other forms by eliminating variables. However, we can recombine these two equations in the -transform domain to verify that we have the correct system function. First, we take the -transform of each difference equation: Now we eliminate by substituting the second equation into the first. so we get Thus, because they have the same system function, and are equivalent to the Direct Form I and, and to the Direct Form II and.",
        "920": "Why are these different implementations of the same system function of interest to us? They all use the same number of multiplications and additions to compute exactly the same output from a given input. However, this is true only when the arithmetic is perfect. On a computer with finite precision (e.g., 16-bit words), each calculation will involve round-off errors, which means that each implementation will behave slightly differently. In practice, the implementation of high-quality digital filters in hardware demands correct engineering to control round-off errors and overflows. Relation to the Impulse Response In the analysis of Section \u203b we implicitly assumed that the system function is the -transform of the impulse response of an IIR system. While this is true, we have demonstrated only that it is true for the FIR case. In the IIR case, we need to be able to take the -transform of an infinitely long sequence. As an example of such a sequence, consider. Applying the definition of the -transform from equation on p.~, we would write which is the sum of all the terms in a geometric series where the ratio between successive terms is. Thus, we know that if, then the sum is finite, and in fact is given by the closed-form expression The condition for the infinite sum to equal the closed-form expression can be expressed as. The values of in the complex plane satisfying this condition are called the region of convergence. From the preceding analysis, we can state the following exceedingly important -transform pair: We will have many occasions to use this result in this chapter.",
        "921": "EXAMPLE:\u00a0 from Impulse Response As an example of the use of this result, recall that in Section \u203b we showed by iteration that the impulse response of the system Thus, using the linearity property of the { -transform}, the delay property of the { -transform}, and the result of, the system function for this system is which is what we obtained before by taking the -transform of the difference equation and solving for. Summary of the Method In this section, we have illustrated some important analysis techniques. We have seen that it is possible to go from the difference equation directly to the system function. We have also seen that, in this simple example, it is possible by taking the inverse -transform to go directly from the system function to the impulse response of the system without the tedious process of iterating the difference equation. We will see that it is possible to do this, in general, by a process of inverse -transformation based on breaking the -transform into a sum of terms like the right-hand side of. Before developing this technique, which will be applicable to higher-order systems, we will continue to focus on the first-order IIR system to illustrate some more important points about the -transform and its relation to IIR systems.",
        "922": "Poles and Zeros An interesting fact about the -transform system function is that the numerator and denominator polynomials have zeros. These zero locations in the complex -plane are very important for characterizing the system. Although we like to write the system function in terms of to facilitate the correspondence with the difference equation, it is probably better for finding roots to rewrite the polynomials as functions of rather than. If we multiply the numerator and denominator by, we obtain In this form, it is easy to find the roots of the numerator and denominator polynomials. Demo: PeZ GUI Lab: PeZ Demo: PeZ Tutorial The numerator has one root at and the denominator has its root at ",
        "923": "If we consider as a function of over the entire complex -plane, the root of the numerator is a zero of the function, i.e., Recall that the root of the denominator is a location in the -plane where the function blows up so this location is called a pole of the system function.",
        "924": "Find the poles and zeros of the following -transform system function ",
        "925": "For the -transform of the following feedback filter determine the locations of the pole and zeros.",
        "926": "Poles or Zeros at the Origin or Infinity When the numerator and denominator polynomials have a different number of coefficients, we can have either zeros or poles at. We saw this in Chapter \u203b, where FIR systems, whose system functions have only a numerator polynomial, had a number of poles at equal to the number of zeros of the polynomial. If we count all the poles and zeros at, as well as, then we can assert that the number of poles equals the number of zeros. Consider the following examples.",
        "927": "EXAMPLE:\u00a0 Find Poles and Zeros % The -transform of the feedback filter is easily derived to be When we express in positive powers of we see that there is one pole at and a zero at. ",
        "928": "EXAMPLE:\u00a0 Zero at % For the -transform of the feedback filter we can write as The system has one pole at, and if we take the limit of as, we get. Thus, it also has one zero at. ",
        "929": "Both of the cases in Examples \u203b and \u203b have exactly one pole and one zero, if we count the zero at.",
        "930": "Determine the system function of the following feedback filter: Show that has a pole at, as well as. In addition, show that has two zeros at by taking the limit as.",
        "931": "Pole Locations and Stability The pole location of a first-order filter determines the shape of the impulse response. In Section \u203b we showed that a system having system function has an impulse response That is, an IIR system with a single pole at has an impulse response that is proportional to for. We see that if, the impulse response will die out as. On the other hand, if, the impulse response will not die out; in fact if, it will grow without bound. Since the pole of the system function is at, we see that the location of the pole can tell us whether the impulse response will decay or grow. Clearly, it is desirable for the impulse response to die out, because an exponentially growing impulse response would produce unbounded outputs even if the input samples have finite size. Systems that produce bounded outputs when the input is bounded are called stable systems. If, the pole of the system function is inside the unit circle of the -plane. It turns out that, for the IIR systems we have been discussing, the following is true in general:",
        "932": "A causal LTI IIR system with initial rest conditions is stable if all of the poles of its system function lie strictly inside the unit circle of the -plane. Thus, stability of a system can be seen at a glance from a -plane plot of the poles and zeros of the system function.",
        "933": "EXAMPLE:\u00a0 Stability from Pole Location The system whose system function is has a zero at and a pole at. Therefore the system is stable. Note that the location of the zero, which is outside the unit circle, has nothing to do with stability of the system. Recall that the zeros correspond to an FIR system that is in cascade with an IIR system defined by the poles. Since FIR systems are always stable, it is not surprising that stability is determined solely by the poles of the system function. An LTI IIR system has system function Plot the pole and zero in the -plane, and state whether or not the system is stable.",
        "934": "Frequency Response of an IIR Filter In Chapter \u203b, we introduced the concept of frequency response as the complex function that determines the amplitude and phase change experienced by a complex exponential input to an LTI system; i.e., if, then In Section \u203b on p.~ we showed that the frequency response of an FIR system is related to the system function by This relation between the system function and the frequency response also holds for IIR systems. However, we must add the provision that the system must be stable in order for the frequency response to exist and to be given by. This condition of stability is a general condition, but all FIR systems are stable, so up to now we have not had to be concerned with stability.",
        "935": "Recall that the system function for the general first-order IIR system has the form where the region of convergence of the system function is or. If we wish to evaluate for, then the values of on the unit circle should be in the region of convergence; i.e., we require to be in the region of convergence of the -transform. This means that, which was shown in Section \u203b to be the condition for stability of the first-order system. In Section \u203b we will give another interpretation of why stability is required for the frequency response to exist. Assuming stability in the first-order case, we get the following formula for the frequency response: A simple evaluation will verify that is a periodic function with a period equal to. This must always be the case for the frequency response of a discrete-time system.",
        "936": "Remember that the frequency response is a complex-valued function of frequency. Therefore, we can reduce to two separate real formulas for the magnitude and the phase as functions of frequency. For the magnitude response, it is expedient to compute the magnitude squared first, and then take a square root if necessary.",
        "937": "The magnitude squared can be formed by multiplying the complex in by its conjugate (denoted by ). For our first-order example, This derivation does not assume that the filter coefficients are real. If the coefficients were real, we would get the further simplification This formula is not particularly informative, because it is difficult to use it to visualize the shape of. However, it could be used to write a program for evaluating and plotting the frequency response. The phase response is even messier. Arctangents would be used to extract the angles of the numerator and denominator, and then the two phases would be subtracted. When the filter coefficients are real, the phase is Again, the formula is so complicated that we cannot gain insight from it directly. In a later section, we will use the poles and zeros of the system function together with the relationship to construct an approximate plot of the frequency response without recourse to formulas.",
        "938": "Frequency Response using MATLAB Frequency responses can be computed and plotted easily by many signal processing software packages. In MATLAB, for example, the function freqz is provided for just that purpose. The frequency response is evaluated over an equally spaced grid in the domain, and then its magnitude and phase can be plotted. In MATLAB, the functions abs and angle will extract the magnitude and the angle of each element in a complex vector.",
        "939": "EXAMPLE:\u00a0 Plot via MATLAB Consider the example In order to define the filter coefficients in MATLAB, we put all the terms with on one side of the equation, and the terms with on the other. Then we read off the filter coefficients and define the vectors aa and bb as Thus, the vectors aa and bb are in the same form as for the filter function. The following call to freqz will generate a 401-point vector HH containing the values of the frequency response at the vector of frequencies specified by the third argument, [-6:0.03:6]. Plots of the resulting magnitude and phase are shown in Fig.~\u203b. The frequency interval is shown so that the -periodicity of will be evident.",
        "940": " Frequency response (magnitude and phase) for a first-order feedback filter. The pole is at and the numerator has a zero at. In this example, we can look for a connection between the poles and zeros and the shape of the frequency response. For this system, we have the system function which has a pole at and a zero at. The point is the same as because. Thus, has the value zero at, since is zero at. In a similar manner, the pole at has an effect on the frequency response near. Since blows up at, the nearby points on the unit circle must have large values. The closest point on the unit circle is at. In this case, we can evaluate the frequency response directly from the formula to get Three-Dimensional Plot of a System Function The relationship between and the pole-zero locations of can be illustrated by making a three-dimensional plot of and then cutting out the frequency response. The frequency response is obtained by selecting the values of along the unit circle, i.e., as goes from to, the equation defines the unit circle.",
        "941": "In this section, we use the system function to illustrate the relationship between the system function and the frequency response. Figures~\u203b and \u203b are plots of the magnitude and phase of over the region of the -plane. In the magnitude plot of Fig.~\u203b, we observe that the pole (at ) creates a large peak that makes all nearby values very large. At the precise location of the pole,, but the grid in Fig.~\u203b does not contain the point, so the plot stays within a finite scale. The phase response in Fig.~\u203b also exhibits its most rapid transition at which is, the closest point on the unit circle to the pole at.",
        "942": "The frequency response is obtained by selecting the values of the -transform along the unit circle in Figs.~\u203b and \u203b. Plots of versus are given in Fig.~\u203b. The shape of the frequency response can be explained in terms of the pole location by recognizing that in Fig.~\u203b the pole at pushes up in the region near which is the same as.",
        "943": " Demo: Z to Freq The unit circle values follow the ups and downs of as goes from to.",
        "944": " -transform evaluated over a region of the -plane including the unit circle. Values along the unit circle are shown as a dark line where the frequency response (magnitude) is evaluated. The view is from the fourth quadrant, so the point is on the right. The first-order filter has a pole at and a zero at. -transform evaluated over a region of the -plane including the unit circle. Values along the unit circle are shown as a dark line where the frequency response (magnitude) is evaluated. The view is from the fourth quadrant, so the point is on the right. The first-order filter has a pole at and a zero at. Frequency response (magnitude and phase) for a first-order feedback filter. The pole is at and the numerator has a zero at. These are the values of along the unit circle in the -plane. ",
        "945": "Three Domains To illustrate the use of the analysis tools that we have developed, we consider the general second-order case.",
        "946": " Demo: 3-DomainsIIR The three domains:, and are depicted in Fig.~\u203b. The defining equation for the IIR digital filter is the feedback difference equation, which, for the second-order case, is This equation provides the algorithm for computing the output signal from the input signal by iteration using the filter coefficients. It also defines the impulse response.",
        "947": " Relationship among the -, -, and -domains. The filter coefficients play a central role. Following the procedures illustrated for the first-order case, we can also define the -transform system function directly from the filter coefficients as and we can also obtain the frequency response Since the system function is a ratio of polynomials, the poles and zeros of make up a small set of parameters that completely define the filter.",
        "948": "Finally, the shapes of the passbands and stopbands of the frequency response are highly dependent on the pole and zero locations with respect to the unit circle, and the character of the impulse response can be related to the poles. To make this last point for the general case, we need to develop one more tool\u2013-a technique for getting directly from. This process, which applies to any -transform and its corresponding sequence, is called the inverse -transform. The inverse -transform is developed in Section \u203b.",
        "949": "The Inverse -Transform and Some Applications We have seen how the three domains are connected for the first-order IIR system. Many of the concepts that we have introduced for the first-order system can be extended to higher-order systems in a straightforward manner. However, finding the impulse response from the system function is not an obvious extension of what we have done for the first-order case. We need to develop a process for inverting the -transform that can be applied to systems with more than one pole. This process is called the inverse -transform. In this section we will show how to find the inverse for a general rational -transform. We will illustrate the process with some examples. The techniques that we develop will then be available for determining the impulse responses of second-order and higher-order systems.",
        "950": "Revisiting the Step Response of a First-Order System In Section \u203b we computed the step response of a first-order system by both iteration and convolution. Now we will show how the -transform can be used for the same purpose. Consider a system whose system function is Recall that the -transform of the output of this system is, so one approach to finding the output for a given input is as follows: Determine the -transform of the input signal. Multiply by to get. Determine the inverse -transform of to get the output. Clearly, this procedure will work and will avoid both iteration and convolution if we can determine and if we can perform the necessary inverse transformation. Our focus in this section will be on deriving a general procedure for step~(\u203b).",
        "951": "In the case of the step response, we see that the input is a special case of the more general sequence ; i.e.,. Therefore, from it follows that the -transform of is so is Now we need to go back to the -domain by inverse transformation. A standard approach is to use a table of -transform pairs and simply look up the answer in the table. Our previous discussions in Chapter~\u203b and earlier in this chapter have developed the basis for a simple version of such a table. A summary of the -transform knowledge that we have developed so far is given in Table~\u203b. Although more extensive tables can be constructed, the results that we have assembled in Table \u203b are more than adequate for our purposes in this text.",
        "952": " Summary of important -transform properties and pairs. SHORT TABLE OF -TRANSFORMS 1. 2. 3. 4. 1 5. 6. Now let us return to the problem of finding given in. The technique that we will use is based on the partial fraction expansion of. This technique is based on the observation that a rational function can be expressed as a sum of simpler rational functions, i.e., If the expression on the right is pulled together over a common denominator, it should be possible to find and so that the numerator of the resulting rational function will be equal to. Equating the two numerators would give two equations in the two unknowns and. However, there is a much quicker way. A systematic procedure for finding the desired and is based on the observation that, for this example, Then we can evaluate at to isolate, i.e., With this result, it is easy to see that Similarly, we can find by Now using the superposition property of the -transform (entry 1 in Table \u203b), and the exponential -transform pair (entry 6 in Table \u203b), we can write down the desired answer as which, after some manipulation becomes If we substitute the value into, we get which is the same result obtained in Section \u203b both by iteration of the difference equation and by convolution.",
        "953": "With this example, we have established the framework for using the basic properties of { -transform}s together with a few basic -transform pairs to perform inverse -transformation for any rational -transform. We summarize this procedure in the following subsection.",
        "954": "A General Procedure for Inverse -Transformation Let be any rational -transform of degree in the denominator and in the numerator. Assuming that, we can find the sequence that corresponds to by the following procedure:",
        "955": " PROCEDURE FOR INVERSE -TRANSFORMATION ( ) \tFactor the denominator polynomial of and express the pole \tfactors in the form for. \tMake a partial fraction expansion of into a sum of terms of \tthe form Write down the answer as This procedure will always work if the poles,, are distinct. Repeated poles complicate the process, but can be handled systematically. We will restrict our attention to the case of non-repeated poles. Furthermore, this procedure can be applied to the inversion of any rational -transform, not just a system function. We will illustrate the use of this procedure with two examples.",
        "956": "EXAMPLE:\u00a0 Inverse -transform Let a -transform be We wish to write in the form Continuing the procedure for partial fraction expansion we obtain Therefore, ",
        "957": "Note that the poles at and give rise to terms in of the form. In Example \u203b, the degree of the numerator is and the degree of the denominator is. This is important because the partial fraction expansion works only for rational functions such that. The next example shows why this is so, and illustrates a method of dealing with this complication.",
        "958": "EXAMPLE:\u00a0 Long Divison Let be Now we must add a constant term to the partial fraction expansion, otherwise, we cannot generate the term in the numerator when we combine the partial fractions over a common denominator. That is, we must assume the following form for : How can we determine the constant ? One way is to perform long division of the denominator polynomial into the numerator polynomial until we get a remainder whose degree is lower than that of the denominator. In this case, the polynomial long division looks as follows: Thus, if we place the remainder over the denominator (in factored form), we can write as a rational part (fraction) plus the constant 1, i.e., The next step would be to apply the partial fraction expansion technique to the rational part of. Since the rational part turns out to be identical to in from Example \u203b, the results would be the same as in that example, so we can write as Therefore, from Table \u203b, Notice again that the time\u2013domain sequence has terms of the form. The constant term in the system function generates an impulse, which is nonzero only at (entry 4 in Table \u203b). ",
        "959": "Steady-State Response and Stability A stable system is one that does not \u201cblow up.\u201d This intuitive statement can be formalized by saying that the output of a stable system can always be bounded whenever the input is bounded. We can use the inverse -transform method developed in Section \u203b to demonstrate an important point about stability, the frequency response, and the sinusoidal steady-state response. To illustrate this point, consider the LTI system defined by From our discussion so far, we can state without further analysis that the system function of this system is and that the impulse response is We can state also that the frequency response is but this is true only if the system is stable ( ). The objective of this section is to define the concept of stability and demonstrate its impact on the response to a sinusoid applied at.",
        "960": "Recall from Section~\u203b and equations and that the output of this system for a complex exponential input is for. What if the complex exponential input sequence is suddenly applied instead of existing for all ? The -transform tools that we have developed make it easy to solve this problem. Indeed, the -transform is ideally suited for situations where the sequences are either finite-length sequences or suddenly applied exponentials. For the suddenly applied complex exponential sequence with frequency the -transform is found from entry 6 of Table~\u203b to be and the -transform of the output of the LTI system is Using the technique of partial fraction expansion, we can easily show that Therefore, the output due to the suddenly applied complex exponential sequence is Equation shows that the output consists of two terms. One term is proportional to an exponential sequence that is solely determined by the pole at. If, this term will die out with increasing, in which case it would be called the transient component. The second term is proportional to the input complex exponential signal, and the constant of proportionality term is, the frequency response of the system evaluated at the frequency of the suddenly applied complex sinusoid. This complex exponential component is the sinusoidal steady-state component of the output.",
        "961": "Now we see that the location of the pole of is crucial if we want the output to reach the sinusoidal steady state. Clearly, if, then the system is stable and the pole is inside the unit circle. For this condition, the exponential dies out and we can state that the limiting value for large Otherwise, if, the term proportional to will grow with increasing and soon dominate the output. The following example gives a specific numerical illustration.",
        "962": "EXAMPLE:\u00a0 Transient and Steady-State If,, and, the transient component is Similarly, the steady-state component is Figure~\u203b shows the real part of the total output (a), and also the transient component (b), and the steady-state component (c). The signals all start at when the complex exponential is applied at the input. Note how the transient component oscillates, but dies away, which explains why the steady-state component eventually equals the total output. In Fig.~\u203b, in (c) and in (a) look identical for. Illustration of transient and steady-state responses of an IIR system. (a) total output, (b) decaying transient component, (c) steady-state response. On the other hand, if the pole were at, the system would be unstable and the output would \u201cblow up\u201d as shown in Fig.~\u203b. In this case, the output contains a term that eventually dominates and grows without bound.",
        "963": " Illustration of an unstable IIR system. Pole is at. The result of Example \u203b can be generalized by observing that the only difference between this example and a system with a higher-order system function is that the total output would include one exponential factor for each pole of the system function as well as the term. That is, it can be shown that for a suddenly applied exponential input sequence, the output of an th-order IIR system will always be of the form where the s are the poles of the system function. Therefore, the sinusoidal steady state will exist and dominate in the total response if the poles of the system function all lie strictly inside the unit circle. This makes the concept of frequency response useful in a practical setting where all signals must have a beginning point at some finite time.",
        "964": "Second-Order Filters We now turn our attention to filters with two feedback coefficients, and. The general difference equation becomes the second-order difference equation As before, we can characterize the second-order filter in each of the three domains: time domain, frequency domain, and -domain. We start with the -transform domain because by now we have demonstrated that the poles and zeros of the system function give a great deal of insight into most aspects of both the time and frequency responses.",
        "965": " Demo: PeZ GUI -Transform of Second-Order Filters Using the approach followed in Section \u203b for the first-order case, we can take the -transform of the second-order difference equation by replacing each delay with (second entry in Table~\u203b), and also replacing the input and output signals with their { -transform}s: In the -transform domain, the input-output relationship is, so we can solve for by finding. For the second-order filter we get which can be solved for as Thus, the system function for an IIR filter is a ratio of two second-degree polynomials, where the numerator polynomial depends on the feed-forward coefficients and the denominator depends on the feedback coefficients. It should be possible to work problems such as Exercise \u203b by simply reading the filter coefficients from the difference equation and then substituting them directly into the -transform expression for.",
        "966": "Find system function of the following IIR filter: Conversely, given the system function, it is a simple matter to write down the difference equation.",
        "967": "For the system function write down the difference equation that relates the input to the output. EXAMPLE:\u00a0 Structure for The connection between and the difference equation can be generalized to higher-order filters. If we are given a fourth-order system the corresponding difference equation is As before, note the sign change in the feedback coefficients,. Structures for Second-Order IIR Systems The difference equation can be interpreted as an algorithm for computing the output sequence from the input. Other computational orderings are possible, and the -transform has the power to derive alternative structures through polynomial manipulations. Two alternative computational orderings that will implement the system defined by in are given in Figs.~\u203b and \u203b.",
        "968": " Direct Form II (DF-II), an alternative computational structure for the second-order recursive filter. Direct Form II (DF-II), an alternative computational structure for the second-order recursive filter. In order to verify that each block diagram in Fig.~\u203b has the correct system function, we need to write the equations of the structure at the adders, and then eliminate the internal variable(s). For the case of the Direct Form II in Fig.~\u203b, the equations at the output of the summing nodes are which can be rearranged into the form Since the system function is the ratio of to, we get Thus we have proved that the Direct Form II (DF-II) structure in Fig.~\u203b which implements the pair of difference equations is identical to the system defined by the single difference equation.",
        "969": "The Transposed Direct Form-II in Fig.~\u203b can be worked out similarly. The difference equations represented by the block diagram are Moving all the terms to the right-hand side and the terms to the left-hand side gives so we get by division Thus we have shown that the Transposed Direct Form II (TDF-II) is equivalent to the system function for the basic Direct Form-I difference equation in. Both examples illustrate the power of the -transform approach in manipulating polynomials that correspond to different structures.",
        "970": "In theory, the system with system function given by can be implemented by iterating any of the equations,, or. For example, the MATLAB function filter uses the TDF-II structure. However, as mentioned before, the reason for having different block diagram structures is that the order of calculation defined by equations,, and is different. In a hardware implementation, the different structures will behave differently, especially when using fixed-point arithmetic where rounding error is fed back into the structure. With double-precision floating-point arithmetic as in MATLAB, there is little difference. Indeed, the DF-II structure is used in MATLABs filter function.",
        "971": "Draw the block diagram of the Direct Form I difference equation defined by, and compare it to the other block diagrams in Figs.~\u203b and \u203b. Poles and Zeros Finding the poles and zeros of is less confusing if we rewrite the polynomials as functions of rather than. Thus the general second-order rational -transform would become after multiplying the numerator and denominator by. Recall from algebra the following important property of polynomials:",
        "972": " PROPERTY OF REAL POLYNOMIALS A polynomial of degree has roots.If all the coefficients of the polynomial are real, the roots either must be real or must occur in complex conjugate pairs. Therefore, in the second-order case, the numerator and denominator polynomials each have two roots, and there are two possibilities: Either the roots are complex conjugates of each other, or they are both real. We will now concentrate on the roots of the denominator, but exactly the same results hold for the numerator. From the quadratic formula, we get two poles at When, both poles are real; when, they are real and equal. However, when, the square root produces an imaginary result and we have complex-conjugate poles with values In polar form, the complex poles can be expressed as and, where the radius is and the angle satisfies EXAMPLE:\u00a0 Complex Poles The following has two poles and two zeros. The poles and zeros are The system function can be written in factored form as either of the two forms Since the numerator has no term, we have one zero at the origin. As is our custom, we plot these locations in the -plane and mark the pole locations with x and the zeros with o. See Fig.~\u203b. Pole-zero plot for system with. The unit circle is shown for reference. Impulse Response of a Second-Order IIR System We have derived the general -transform system function for the second-order filter and we have seen that the denominator polynomial has two roots that define the poles of the second-order filter. Expressing in terms of the poles gives Using the partial fraction expansion technique developed in Section \u203b, we can express the system function as Filtering} where and can be evaluated by. Therefore, the impulse response will have the form Furthermore, the poles may both be real or they may be a pair of complex conjugates. We will examine these two cases separately.",
        "973": " Real Poles If and are real, the impulse response is composed of two real exponentials of the form. This case is illustrated by the following example:",
        "974": "EXAMPLE:\u00a0 Second-Order: Real Poles Assume that from which we see that the poles are at and and that there are two zeros at. The poles and zeros of are plotted in Fig.~\u203b. We can extract the filter coefficients from and write the following difference equation which must be satisfied for any input and its corresponding output. Specifically, the impulse response would satisfy the difference equation ",
        "975": " Pole-zero plot for system of Example \u203b. The poles are at and ; there are two zeros at. which can be iterated to compute if we know the values of and, i.e., the values of the impulse response sequence just prior to where the impulse first becomes nonzero. These values are supplied by the initial rest condition, which means that and. The following table shows the computation of a few values of the impulse response:",
        "976": " 0 1 2 3 4 0 1 0 0 0 0 0 0 In contrast to the simpler first-order case, it is very difficult to guess the general th term for the impulse response sequence. Fortunately, we can rely on the inverse -transform technique to give us the general formula. Applying the partial fraction expansion to, we get which implies that Since both poles are inside the unit circle, the impulse response dies out for large, i.e., the system is stable. Find the impulse response of the following second-order system: Plot the resulting signal versus. Complex Poles Now let us assume that the coefficients and in the second-order difference equation are such that the poles of are complex. If we express the poles in polar form it is convenient to rewrite the denominator polynomial in terms of the parameters and. Basic algebra allows us to start from the factored form and derive the polynomial coefficients: The system function is therefore We can also identify the two feedback filter coefficients as so the corresponding difference equation is This parameterization is significant because it allows us to see directly how the poles define the feedback terms of the difference equation. For example, if we want to change the angle of the pole, then we vary the coefficient. Finally, we must remember that is valid only for the special case of complex-conjugate poles; when the poles are both real, the filter coefficients are EXAMPLE:\u00a0 Invert Complex Poles Consider the following system whose system function is A pole-zero plot for was already given in Fig.~\u203b. Using the partial fraction expansion technique, we can write in the form The two complex exponentials with frequencies combine to form the cosine. The impulse response is plotted in Fig.~\u203b.",
        "977": " Impulse response for system with. An important observation about the system in Example \u203b is that it produces a pure sinusoid when stimulated by an impulse. Such a system is an example of a sine wave oscillator. After being stimulated by the single input sample from the impulse, the system continues indefinitely to produce a sinusoid of frequency. This frequency is equal to the angle of the poles. A first-order filter (or a filter with all real poles) can only decay (or grow) as or oscillate up and down as, but a second-order system can oscillate with different periods. This is important when modeling physical signals such as speech, music, or other sounds.",
        "978": "Note that in order to produce the continuing sinusoidal output, the system must have its poles on the unit circle of the -plane, i.e.,. Also note that the angle of the poles is exactly equal to the radian frequency of the sinusoidal output. Thus, we can control the frequency of the sinusoidal oscillator by adjusting the coefficient of the difference equation while leaving fixed at.",
        "979": "EXAMPLE:\u00a0 Poles on Unit Circle As an example of an oscillator with a different frequency, we can use to define a difference equation with prescribed pole locations. If we take and, as shown in Fig.~\u203b, we get and. The system function of this system is The inverse -transform gives a general formula for : Once again, the frequency of the cosine term in the impulse response is equal to the angle of the pole,. Pole-zero plot for system with. The unit circle is shown for reference. If the complex conjugate poles of the second-order system lie on the unit circle, the output oscillates sinusoidally and does not decay to zero. If the poles lie outside the unit circle, the output grows exponentially, whereas if they are inside the unit circle, the output decays exponentially to zero.",
        "980": "EXAMPLE:\u00a0 Stable Complex Poles As an example of a stable system, if we take and, as shown in Fig.~\u203b, we get and, and the difference equation becomes The system function of this system is and the general formula for is In this case, the general formula for has a decay of multiplying a periodic cosine with period 6. The frequency of the cosine term in the impulse response is again the angle of the pole, ; while the decaying term is controlled by the radius of the pole, i.e.,. Pole-zero plot for system with. The unit circle is shown for reference. ",
        "981": "Frequency Response of Second-Order IIR Filter Since the frequency response of a stable system is related to the -transform by we get the following formula for the frequency response of a second-order system: Since contains terms like and, is guaranteed to be a periodic function with a period of.",
        "982": "The magnitude squared of the frequency response can be formed by multiplying the complex by its conjugate (denoted by ). Rather than work out a general formula, we take a specific numerical example to show the kind of formula that results.",
        "983": "EXAMPLE:\u00a0 Frequency Response of Second-Order Consider the case where the system function is The magnitude squared is derived by multiplying out all the terms in the numerator and denominator of, and then collecting terms where the inverse Euler formula applies. This formula is useful because it is expressed completely in terms of cosine functions. The procedure is general, so a similar formula could be derived for any IIR filter. Since the cosine is an even function, we can state that any magnitude-squared function will always be even, i.e., The phase response is a bit messier. If arctangents are used to extract the angle of the numerator and denominator, then the two phases must be subtracted. The filter coefficients in this example are real, so the phase is which is an odd function of. The formulas obtained in this example are too complicated to provide much insight directly. In a later section we will see how to use the poles and zeros of the system function to construct an approximate plot of the frequency response without recourse to such formulas. Frequency Response via MATLAB Tedious calculation and plotting of by hand is usually unnecessary if a computer program such as MATLAB is available. The MATLAB function freqz is provided for just that purpose. The frequency response can be evaluated over a grid in the domain, and then its magnitude and phase can be plotted. In MATLAB, the functions abs and angle will extract the magnitude and the angle of each element in a complex vector.",
        "984": "EXAMPLE:\u00a0 MATLAB for Consider the system introduced in Example \u203b: In order to define the filter coefficients in MATLAB, we put all the terms with on one side of the equation, and the terms with on the other. Then we read off the filter coefficients and define the vectors aa and bb. The following call to freqz will generate a vector HH containing the values of the frequency response at the vector of frequencies specified by the third argument, [-pi:(pi/100):pi]. A plot of the resulting magnitude and phase is shown in Fig.~\u203b. Since is always periodic with a period of, it is sufficient to make the frequency response plot over the range.",
        "985": "For this example, we can look for a connection between the poles and zeros and the shape of the frequency response. For this we have which has its poles at and its zeros at and. Since is the same as, we conclude that is zero at, because at ; likewise, the zero of at is a zero of at. The poles have angles of rad, so the poles have an effect on the frequency response near. Since is infinite at, the nearby points on the unit circle (at ) must have large values. In this case, we can evaluate the frequency response directly from the formula to get This value of the frequency response magnitude is a good approximation to the true maximum value, which actually occurs at. Frequency response (magnitude and phase) for a second-order feedback filter. The poles are at and the numerator has zeros at and. The shaded region shows the 3-dB bandwidth around the peak at. 3-dB Bandwidth The width of the peak of the frequency response in Fig.~\u203b is called the bandwidth. It must be measured at some standard point on the plot of. The most common practice is to use the 3-dB width, which is calculated as follows:",
        "986": " Determine the peak value for and then find the nearest frequency on each side of the peak where the value of the frequency response is. The 3-dB width is the difference between these two frequencies. In Fig.~\u203b, the true peak value is 10.526 at, so we look for points where. These occur at and, so the bandwidth is rad.",
        "987": "The 3-dB bandwidth calculation can be carried out efficiently with a computer program, but it is also helpful to have an approximate formula that can give quick \u201cback-of-the-envelope\u201d calculations. An excellent approximation for the second-order case with narrow peaks is given by the formula which shows that the distance of the pole from the unit circle controls the bandwidth. In Fig.~\u203b, the bandwidth evaluates to Thus we see that the approximation is quite good in this case, where the pole is rather close to the unit circle (radius = 0.9).",
        "988": "Three-Dimensional Plot of System Functions Since the frequency response is the system function evaluated on the unit circle, we can illustrate the connection between the and domains with a three-dimensional plot such as the one shown in Fig.~\u203b.",
        "989": " -transform evaluated over a region of the -plane including the unit circle. The view is from the fourth quadrant, so the point is at the right. Values along the unit circle are the frequency response (magnitude) for a second-order feedback filter. The poles are at and the numerator has zeros at. Figure~\u203b shows a plot of the system function at points inside, outside, and on the unit circle. The peaks located at the poles,, determine the frequency response behavior near. If the poles were moved closer to the unit circle, the frequency response would have a higher and narrower peak. The zeros at create valleys that lie on the unit circle at.",
        "990": " Demo: Z to Freq We can estimate any value of directly from the poles and zeros. This can be done systematically by writing in the following form: where and are the zeros and and are the poles of the second-order filter. The parameter is a gain term that may have to be factored out. Then the magnitude of the frequency response is Equation has a simple geometric interpretation. Each term or is the vector length from the zero or the pole to the unit circle position, shown in Fig.~\u203b. The frequency response at a fixed value of is the product of times the product of the lengths of the vectors to the zeros divided by the product of the lengths of the vectors to the poles. As we go around the unit circle, these vector lengths change. When we are on top of a zero, one of the numerator lengths is zero, so at that frequency. When we are close to a pole, one of the denominator lengths is very small, so will be large at that frequency.",
        "991": " -transform evaluated on the unit circle by using a product of vector lengths from the poles and zeros. denotes the vector from the pole to, and denotes the vector from the zero to, We can apply this geometric reasoning to estimate the magnitude of at in Fig.~\u203b. We begin by estimating the lengths of the vectors from the zeros and poles to the point, which is the same as. The lengths of the vectors from the zeros are then divided by the lengths of the vectors from the poles, so we get The gain was assumed to be 1.",
        "992": "An excellent way to practice with these ideas is to use the MATLAB GUI called PeZ (Fig.~\u203b).",
        "993": " Graphical User Interface (GUI) for PeZ which illustrates the three-domains concept. The user can enter poles and zeros and see the resulting impulse response and frequency response. In addition, the poles/zero can be moved with a mouse pointer and and the other plots will updated in real-time. ",
        "994": "Example of an IIR Lowpass Filter First-order and second-order IIR filters are useful and provide simple examples, but, in many cases, we use higher-order IIR filters because they can realize frequency responses with flatter passbands and stopbands and sharper transition regions. The butter, cheby1, cheby2, and ellip functions in MATLABs Signal Processing Toolbox can be used to design filters with prescribed frequency-selective characteristics. As an example, consider the system with system function given by. This system is an example of a lowpass elliptic filter whose numerator and denominator coefficients were obtained using the MATLAB function ellip. The exact call was ellip(3,1,30,0.3). Each of the three different forms above is useful: for identifying the filter coefficients, for sketching the pole-zero plot and the frequency response, and for finding the impulse response. Figure~\u203b shows the poles and zeros of this filter. Note that all the zeros are on the unit circle and that the poles are strictly inside the unit circle, as they must be for a stable system.",
        "995": " Pole-zero plot for a third-order IIR filter. From determine the difference equation (Direct Form I) for implementing this system. The system function was evaluated on the unit circle using the MATLAB function freqz. A plot of this result is shown in Fig.~\u203b. Note that the frequency response is large in the vicinity of the poles and small around the zeros. In particular, the passband of the frequency response is, which corresponds to the poles with angles at. Also, the frequency response is exactly zero at and since the zeros of are at these angles and lie on the unit circle.",
        "996": " Frequency response (magnitude and phase) for a third-order IIR filter. From or, determine the value of the frequency response at. Finally, Fig.~\u203b shows the impulse response of the system. Note that it oscillates and dies out with increasing because of the two complex conjugate poles at angles and radius 0.846. The decaying envelope is.",
        "997": " Impulse response for a third-order IIR filter. Use the partial fraction form to determine an equation for the impulse response of the filter. Hint: Apply the inverse -transform. The elliptic filter example described in this section is a simple example of a practical IIR lowpass filter. Higher-order filters can exhibit much better frequency-selective filter characteristics.",
        "998": "Summary and Links The class of IIR filters was introduced in this chapter, along with the -transform method for filters with poles. The -transform changes problems about impulse responses, frequency responses, and system structures into algebraic manipulations of polynomials and rational functions. Poles of the system function turn out to be the most important elements for IIR filters because properties such as the shape of the frequency response or the form of the impulse response can be inferred quickly from the pole locations.",
        "999": "We also continued to stress the important concept of \u201cdomains of representation.\u201d The -domain or time domain, the -domain or frequency domain, and the -domain give us three domains for thinking about the characteristics of a system. We completed the ties between domains by introducing the inverse -transform for constructing a signal from its -transform. As a result, even difficult problems such as convolution can be simplified by working in the most convenient domain and then transforming back to the original domain.",
        "1000": "Lab~#11 is devoted to IIR filters. This lab uses a MATLAB user interface tool called PeZ that supports an interactive exploration of the three domains.",
        "1001": " Lab: #11 PeZ, the,, and domains The PeZ tool has the potential for widespread use because it presents the user with multiple views of an LTI system: pole-zero domain, frequency response and impulse response. Similar capabilities are now being incorporated into many commercial software packages, e.g., sptool in MATLAB.",
        "1002": "The CD-ROM also contains the following demonstrations of the relationship between the -plane and the frequency domain and time domain:",
        "1003": " A set of \u201cthree-domain\u201d movies that show how the frequency response and impulse response of an IIR filter change as a pole location is varied. Several different filters are demonstrated. Demo: Three-Domains IIR A movie that animates the relationship between the -plane and the unit circle where the frequency response lies. Demo: Z to Freq The PeZ GUI can be used to construct different IIR and FIR filters. Demo: PeZ GUI A tutorial on how to use PeZ. Demo: PeZ Tutorial A demo that gives more examples of IIR filters. Demo: IIR Filtering "
    },
    "Chapter.Section": {
        "0": "-2.0",
        "1": "-2.0",
        "2": "-2.0",
        "3": "-2.0",
        "4": "-2.0",
        "5": "-2.0",
        "6": "-2.0",
        "7": "-2.0",
        "8": "-2.1",
        "9": "-2.2",
        "10": "-2.2",
        "11": "-2.2",
        "12": "-2.2",
        "13": "-2.2",
        "14": "-2.2",
        "15": "-2.2",
        "16": "-2.2",
        "17": "-2.2",
        "18": "-2.2",
        "19": "-2.2",
        "20": "-2.2",
        "21": "-2.2",
        "22": "-2.3",
        "23": "-2.3",
        "24": "-2.3",
        "25": "-2.4",
        "26": "-2.4",
        "27": "-2.4",
        "28": "-2.4",
        "29": "-2.4",
        "30": "-2.4",
        "31": "-2.5",
        "32": "-2.5",
        "33": "-2.5",
        "34": "-2.5",
        "35": "-2.5",
        "36": "-2.5",
        "37": "-2.5",
        "38": "-2.5",
        "39": "-2.5",
        "40": "-2.6",
        "41": "-2.6",
        "42": "-2.6",
        "43": "-2.6",
        "44": "-2.6",
        "45": "-2.7",
        "46": "-2.7",
        "47": "-2.7",
        "48": "-2.7",
        "49": "-1.0",
        "50": "-1.0",
        "51": "-1.0",
        "52": "-1.0",
        "53": "-1.1",
        "54": "-1.1",
        "55": "-1.1",
        "56": "-1.1",
        "57": "-1.1",
        "58": "-1.1",
        "59": "-1.2",
        "60": "-1.2",
        "61": "-1.2",
        "62": "-1.2",
        "63": "-1.2",
        "64": "-1.2",
        "65": "-1.2",
        "66": "-1.2",
        "67": "-1.2",
        "68": "-1.2",
        "69": "-1.2",
        "70": "-1.2",
        "71": "-1.2",
        "72": "-1.2",
        "73": "-1.2",
        "74": "-1.2",
        "75": "-1.3",
        "76": "-1.3",
        "77": "-1.3",
        "78": "-1.3",
        "79": "-1.3",
        "80": "-1.4",
        "81": "-1.4",
        "82": "-1.4",
        "83": "-1.5",
        "84": "-1.5",
        "85": "-1.5",
        "86": "-1.6",
        "87": "-1.6",
        "88": "-1.6",
        "89": "-1.6",
        "90": "-1.6",
        "91": "-1.6",
        "92": "-1.6",
        "93": "-1.6",
        "94": "-1.6",
        "95": "-1.6",
        "96": "-1.6",
        "97": "-1.6",
        "98": "-1.7",
        "99": "-1.7",
        "100": "-1.7",
        "101": "-1.7",
        "102": "-1.7",
        "103": "-1.7",
        "104": "-1.7",
        "105": "-1.7",
        "106": "-1.7",
        "107": "-1.7",
        "108": "-1.7",
        "109": "1.0",
        "110": "1.0",
        "111": "1.0",
        "112": "1.0",
        "113": "1.1",
        "114": "1.1",
        "115": "1.1",
        "116": "1.1",
        "117": "1.1",
        "118": "1.1",
        "119": "1.1",
        "120": "1.1",
        "121": "1.1",
        "122": "1.1",
        "123": "1.2",
        "124": "1.2",
        "125": "1.2",
        "126": "1.2",
        "127": "1.2",
        "128": "1.3",
        "129": "1.3",
        "130": "1.3",
        "131": "1.4",
        "132": "2.0",
        "133": "2.0",
        "134": "2.0",
        "135": "2.0",
        "136": "2.1",
        "137": "2.1",
        "138": "2.1",
        "139": "2.1",
        "140": "2.1",
        "141": "2.1",
        "142": "2.2",
        "143": "2.2",
        "144": "2.2",
        "145": "2.2",
        "146": "2.2",
        "147": "2.2",
        "148": "2.2",
        "149": "2.2",
        "150": "2.2",
        "151": "2.3",
        "152": "2.3",
        "153": "2.3",
        "154": "2.3",
        "155": "2.3",
        "156": "2.3",
        "157": "2.3",
        "158": "2.3",
        "159": "2.3",
        "160": "2.3",
        "161": "2.3",
        "162": "2.3",
        "163": "2.3",
        "164": "2.4",
        "165": "2.4",
        "166": "2.4",
        "167": "2.4",
        "168": "2.5",
        "169": "2.5",
        "170": "2.5",
        "171": "2.5",
        "172": "2.5",
        "173": "2.5",
        "174": "2.5",
        "175": "2.5",
        "176": "2.5",
        "177": "2.5",
        "178": "2.5",
        "179": "2.5",
        "180": "2.5",
        "181": "2.5",
        "182": "2.5",
        "183": "2.5",
        "184": "2.5",
        "185": "2.5",
        "186": "2.5",
        "187": "2.6",
        "188": "2.6",
        "189": "2.6",
        "190": "2.6",
        "191": "2.6",
        "192": "2.6",
        "193": "2.6",
        "194": "2.6",
        "195": "2.6",
        "196": "2.6",
        "197": "2.6",
        "198": "2.7",
        "199": "2.7",
        "200": "2.7",
        "201": "2.7",
        "202": "2.7",
        "203": "2.8",
        "204": "2.8",
        "205": "2.8",
        "206": "2.8",
        "207": "2.9",
        "208": "2.9",
        "209": "2.9",
        "210": "2.9",
        "211": "2.9",
        "212": "2.9",
        "213": "2.9",
        "214": "3.0",
        "215": "3.0",
        "216": "3.1",
        "217": "3.1",
        "218": "3.1",
        "219": "3.1",
        "220": "3.1",
        "221": "3.1",
        "222": "3.1",
        "223": "3.1",
        "224": "3.2",
        "225": "3.2",
        "226": "3.2",
        "227": "3.2",
        "228": "3.2",
        "229": "3.2",
        "230": "3.2",
        "231": "3.2",
        "232": "3.2",
        "233": "3.2",
        "234": "3.3",
        "235": "3.3",
        "236": "3.3",
        "237": "3.3",
        "238": "3.3",
        "239": "3.3",
        "240": "3.3",
        "241": "3.3",
        "242": "3.3",
        "243": "3.3",
        "244": "3.3",
        "245": "3.3",
        "246": "3.3",
        "247": "3.3",
        "248": "3.3",
        "249": "3.4",
        "250": "3.4",
        "251": "3.4",
        "252": "3.4",
        "253": "3.4",
        "254": "3.4",
        "255": "3.4",
        "256": "3.4",
        "257": "3.4",
        "258": "3.4",
        "259": "3.4",
        "260": "3.5",
        "261": "3.5",
        "262": "3.5",
        "263": "3.5",
        "264": "3.6",
        "265": "3.6",
        "266": "3.6",
        "267": "3.6",
        "268": "3.6",
        "269": "3.6",
        "270": "3.6",
        "271": "3.6",
        "272": "3.6",
        "273": "3.6",
        "274": "3.6",
        "275": "3.6",
        "276": "3.6",
        "277": "3.6",
        "278": "3.6",
        "279": "3.6",
        "280": "3.6",
        "281": "3.6",
        "282": "3.6",
        "283": "3.7",
        "284": "3.7",
        "285": "3.7",
        "286": "3.7",
        "287": "3.7",
        "288": "3.8",
        "289": "3.8",
        "290": "3.9",
        "291": "3.9",
        "292": "3.9",
        "293": "3.9",
        "294": "3.9",
        "295": "3.9",
        "296": "3.9",
        "297": "3.9",
        "298": "3.9",
        "299": "3.9",
        "300": "3.10",
        "301": "3.10",
        "302": "3.10",
        "303": "3.10",
        "304": "3.10",
        "305": "3.10",
        "306": "3.10",
        "307": "3.10",
        "308": "3.10",
        "309": "3.11",
        "310": "3.11",
        "311": "3.11",
        "312": "3.11",
        "313": "4.0",
        "314": "4.0",
        "315": "4.1",
        "316": "4.1",
        "317": "4.1",
        "318": "4.1",
        "319": "4.1",
        "320": "4.1",
        "321": "4.1",
        "322": "4.1",
        "323": "4.1",
        "324": "4.1",
        "325": "4.1",
        "326": "4.1",
        "327": "4.1",
        "328": "4.1",
        "329": "4.1",
        "330": "4.1",
        "331": "4.1",
        "332": "4.1",
        "333": "4.1",
        "334": "4.1",
        "335": "4.1",
        "336": "4.1",
        "337": "4.1",
        "338": "4.1",
        "339": "4.1",
        "340": "4.1",
        "341": "4.1",
        "342": "4.1",
        "343": "4.1",
        "344": "4.1",
        "345": "4.2",
        "346": "4.2",
        "347": "4.2",
        "348": "4.2",
        "349": "4.2",
        "350": "4.2",
        "351": "4.2",
        "352": "4.2",
        "353": "4.2",
        "354": "4.2",
        "355": "4.2",
        "356": "4.2",
        "357": "4.3",
        "358": "4.3",
        "359": "4.3",
        "360": "4.3",
        "361": "4.3",
        "362": "4.3",
        "363": "4.3",
        "364": "4.3",
        "365": "4.3",
        "366": "4.3",
        "367": "4.3",
        "368": "4.3",
        "369": "4.3",
        "370": "4.4",
        "371": "4.4",
        "372": "4.4",
        "373": "4.4",
        "374": "4.4",
        "375": "4.4",
        "376": "4.4",
        "377": "4.4",
        "378": "4.4",
        "379": "4.4",
        "380": "4.4",
        "381": "4.5",
        "382": "4.5",
        "383": "4.5",
        "384": "4.5",
        "385": "4.6",
        "386": "4.6",
        "387": "4.6",
        "388": "4.6",
        "389": "5.0",
        "390": "5.0",
        "391": "5.1",
        "392": "5.1",
        "393": "5.2",
        "394": "5.2",
        "395": "5.2",
        "396": "5.2",
        "397": "5.2",
        "398": "5.2",
        "399": "5.2",
        "400": "5.3",
        "401": "5.3",
        "402": "5.3",
        "403": "5.3",
        "404": "5.3",
        "405": "5.3",
        "406": "5.3",
        "407": "5.3",
        "408": "5.3",
        "409": "5.3",
        "410": "5.3",
        "411": "5.3",
        "412": "5.3",
        "413": "5.3",
        "414": "5.3",
        "415": "5.3",
        "416": "5.3",
        "417": "5.3",
        "418": "5.3",
        "419": "5.3",
        "420": "5.3",
        "421": "5.3",
        "422": "5.3",
        "423": "5.3",
        "424": "5.4",
        "425": "5.4",
        "426": "5.4",
        "427": "5.4",
        "428": "5.4",
        "429": "5.4",
        "430": "5.4",
        "431": "5.4",
        "432": "5.4",
        "433": "5.4",
        "434": "5.4",
        "435": "5.4",
        "436": "5.4",
        "437": "5.4",
        "438": "5.4",
        "439": "5.4",
        "440": "5.4",
        "441": "5.4",
        "442": "5.4",
        "443": "5.5",
        "444": "5.5",
        "445": "5.5",
        "446": "5.5",
        "447": "5.5",
        "448": "5.5",
        "449": "5.5",
        "450": "5.5",
        "451": "5.5",
        "452": "5.5",
        "453": "5.5",
        "454": "5.5",
        "455": "5.5",
        "456": "5.5",
        "457": "5.6",
        "458": "5.6",
        "459": "5.6",
        "460": "5.6",
        "461": "5.6",
        "462": "5.6",
        "463": "5.6",
        "464": "5.6",
        "465": "5.6",
        "466": "5.6",
        "467": "5.6",
        "468": "5.7",
        "469": "5.7",
        "470": "5.7",
        "471": "5.7",
        "472": "5.7",
        "473": "5.7",
        "474": "5.8",
        "475": "5.8",
        "476": "5.8",
        "477": "5.8",
        "478": "5.8",
        "479": "5.8",
        "480": "5.9",
        "481": "5.9",
        "482": "5.9",
        "483": "5.9",
        "484": "6.0",
        "485": "6.1",
        "486": "6.1",
        "487": "6.1",
        "488": "6.1",
        "489": "6.1",
        "490": "6.2",
        "491": "6.2",
        "492": "6.2",
        "493": "6.2",
        "494": "6.2",
        "495": "6.2",
        "496": "6.2",
        "497": "6.2",
        "498": "6.3",
        "499": "6.3",
        "500": "6.3",
        "501": "6.4",
        "502": "6.4",
        "503": "6.4",
        "504": "6.4",
        "505": "6.4",
        "506": "6.4",
        "507": "6.4",
        "508": "6.4",
        "509": "6.4",
        "510": "6.4",
        "511": "6.5",
        "512": "6.5",
        "513": "6.5",
        "514": "6.5",
        "515": "6.5",
        "516": "6.5",
        "517": "6.5",
        "518": "6.5",
        "519": "6.5",
        "520": "6.5",
        "521": "6.5",
        "522": "6.5",
        "523": "6.6",
        "524": "6.6",
        "525": "6.6",
        "526": "6.6",
        "527": "6.6",
        "528": "6.6",
        "529": "6.7",
        "530": "6.7",
        "531": "6.7",
        "532": "6.7",
        "533": "6.7",
        "534": "6.7",
        "535": "6.7",
        "536": "6.7",
        "537": "6.7",
        "538": "6.7",
        "539": "6.7",
        "540": "6.7",
        "541": "6.7",
        "542": "6.7",
        "543": "6.7",
        "544": "6.8",
        "545": "6.8",
        "546": "6.8",
        "547": "6.8",
        "548": "6.8",
        "549": "6.8",
        "550": "6.8",
        "551": "6.8",
        "552": "6.8",
        "553": "6.8",
        "554": "6.8",
        "555": "6.8",
        "556": "6.9",
        "557": "6.9",
        "558": "6.9",
        "559": "6.9",
        "560": "6.9",
        "561": "6.9",
        "562": "6.9",
        "563": "7.1",
        "564": "7.2",
        "565": "7.2",
        "566": "7.2",
        "567": "7.2",
        "568": "7.2",
        "569": "7.2",
        "570": "7.2",
        "571": "7.2",
        "572": "7.2",
        "573": "7.2",
        "574": "7.2",
        "575": "7.2",
        "576": "7.2",
        "577": "7.2",
        "578": "7.2",
        "579": "7.2",
        "580": "7.2",
        "581": "7.2",
        "582": "7.2",
        "583": "7.2",
        "584": "7.2",
        "585": "7.2",
        "586": "7.2",
        "587": "7.2",
        "588": "7.2",
        "589": "7.3",
        "590": "7.3",
        "591": "7.3",
        "592": "7.3",
        "593": "7.3",
        "594": "7.3",
        "595": "7.3",
        "596": "7.3",
        "597": "7.3",
        "598": "7.3",
        "599": "7.3",
        "600": "7.3",
        "601": "7.3",
        "602": "7.3",
        "603": "7.3",
        "604": "7.3",
        "605": "7.3",
        "606": "7.3",
        "607": "7.3",
        "608": "7.4",
        "609": "7.4",
        "610": "7.4",
        "611": "7.4",
        "612": "7.4",
        "613": "7.4",
        "614": "7.4",
        "615": "7.4",
        "616": "7.4",
        "617": "7.4",
        "618": "7.4",
        "619": "7.4",
        "620": "7.5",
        "621": "7.5",
        "622": "7.5",
        "623": "7.5",
        "624": "7.5",
        "625": "7.5",
        "626": "7.5",
        "627": "7.5",
        "628": "7.5",
        "629": "7.5",
        "630": "7.5",
        "631": "7.5",
        "632": "7.5",
        "633": "7.5",
        "634": "7.5",
        "635": "7.5",
        "636": "7.5",
        "637": "7.5",
        "638": "7.6",
        "639": "7.7",
        "640": "7.7",
        "641": "8.1",
        "642": "8.1",
        "643": "8.2",
        "644": "8.2",
        "645": "8.2",
        "646": "8.2",
        "647": "8.2",
        "648": "8.2",
        "649": "8.2",
        "650": "8.2",
        "651": "8.2",
        "652": "8.2",
        "653": "8.2",
        "654": "8.2",
        "655": "8.2",
        "656": "8.2",
        "657": "8.2",
        "658": "8.2",
        "659": "8.2",
        "660": "8.2",
        "661": "8.4",
        "662": "8.4",
        "663": "8.4",
        "664": "8.4",
        "665": "8.4",
        "666": "8.4",
        "667": "8.4",
        "668": "8.4",
        "669": "8.4",
        "670": "8.4",
        "671": "8.4",
        "672": "8.4",
        "673": "8.4",
        "674": "8.4",
        "675": "8.4",
        "676": "8.4",
        "677": "8.4",
        "678": "8.4",
        "679": "8.4",
        "680": "8.4",
        "681": "8.5",
        "682": "8.5",
        "683": "8.6",
        "684": "8.6",
        "685": "8.6",
        "686": "8.6",
        "687": "8.6",
        "688": "8.6",
        "689": "8.6",
        "690": "8.6",
        "691": "8.6",
        "692": "8.6",
        "693": "8.6",
        "694": "8.6",
        "695": "8.6",
        "696": "8.6",
        "697": "8.6",
        "698": "8.6",
        "699": "8.6",
        "700": "8.6",
        "701": "8.6",
        "702": "8.6",
        "703": "8.6",
        "704": "8.6",
        "705": "8.6",
        "706": "8.6",
        "707": "8.6",
        "708": "8.6",
        "709": "8.6",
        "710": "8.6",
        "711": "8.6",
        "712": "8.6",
        "713": "8.6",
        "714": "8.6",
        "715": "8.7",
        "716": "8.7",
        "717": "8.7",
        "718": "8.7",
        "719": "8.7",
        "720": "8.7",
        "721": "8.7",
        "722": "8.7",
        "723": "8.7",
        "724": "8.7",
        "725": "8.8",
        "726": "8.8",
        "727": "8.8",
        "728": "8.8",
        "729": "8.8",
        "730": "8.8",
        "731": "8.8",
        "732": "8.8",
        "733": "8.8",
        "734": "8.8",
        "735": "8.8",
        "736": "8.8",
        "737": "8.8",
        "738": "8.8",
        "739": "8.8",
        "740": "8.9",
        "741": "8.9",
        "742": "8.9",
        "743": "8.9",
        "744": "8.9",
        "745": "8.9",
        "746": "8.9",
        "747": "8.9",
        "748": "8.9",
        "749": "8.9",
        "750": "8.9",
        "751": "8.9",
        "752": "8.9",
        "753": "8.9",
        "754": "8.9",
        "755": "8.9",
        "756": "8.9",
        "757": "8.9",
        "758": "8.9",
        "759": "8.9",
        "760": "8.9",
        "761": "8.9",
        "762": "8.9",
        "763": "8.9",
        "764": "8.9",
        "765": "8.9",
        "766": "8.9",
        "767": "8.9",
        "768": "8.9",
        "769": "8.9",
        "770": "8.9",
        "771": "8.10",
        "772": "8.10",
        "773": "8.10",
        "774": "8.10",
        "775": "8.10",
        "776": "8.10",
        "777": "8.10",
        "778": "8.10",
        "779": "8.10",
        "780": "8.11",
        "781": "9.0",
        "782": "9.0",
        "783": "9.1",
        "784": "9.1",
        "785": "9.1",
        "786": "9.1",
        "787": "9.1",
        "788": "9.2",
        "789": "9.2",
        "790": "9.2",
        "791": "9.2",
        "792": "9.2",
        "793": "9.2",
        "794": "9.2",
        "795": "9.2",
        "796": "9.3",
        "797": "9.3",
        "798": "9.3",
        "799": "9.3",
        "800": "9.3",
        "801": "9.3",
        "802": "9.3",
        "803": "9.4",
        "804": "9.4",
        "805": "9.4",
        "806": "9.4",
        "807": "9.4",
        "808": "9.4",
        "809": "9.4",
        "810": "9.4",
        "811": "9.5",
        "812": "9.5",
        "813": "9.5",
        "814": "9.5",
        "815": "9.5",
        "816": "9.5",
        "817": "9.5",
        "818": "9.5",
        "819": "9.5",
        "820": "9.5",
        "821": "9.5",
        "822": "9.5",
        "823": "9.6",
        "824": "9.6",
        "825": "9.6",
        "826": "9.6",
        "827": "9.6",
        "828": "9.6",
        "829": "9.6",
        "830": "9.6",
        "831": "9.6",
        "832": "9.6",
        "833": "9.6",
        "834": "9.6",
        "835": "9.6",
        "836": "9.6",
        "837": "9.6",
        "838": "9.6",
        "839": "9.7",
        "840": "9.7",
        "841": "9.7",
        "842": "9.7",
        "843": "9.7",
        "844": "9.7",
        "845": "9.7",
        "846": "9.7",
        "847": "9.7",
        "848": "9.7",
        "849": "9.8",
        "850": "9.8",
        "851": "9.8",
        "852": "9.8",
        "853": "9.8",
        "854": "9.8",
        "855": "9.8",
        "856": "9.8",
        "857": "9.8",
        "858": "9.8",
        "859": "9.8",
        "860": "9.9",
        "861": "9.9",
        "862": "9.9",
        "863": "9.9",
        "864": "9.9",
        "865": "9.9",
        "866": "9.10",
        "867": "9.10",
        "868": "9.10",
        "869": "9.10",
        "870": "9.10",
        "871": "9.10",
        "872": "9.10",
        "873": "9.10",
        "874": "10.0",
        "875": "10.0",
        "876": "10.0",
        "877": "10.1",
        "878": "10.1",
        "879": "10.1",
        "880": "10.1",
        "881": "10.1",
        "882": "10.1",
        "883": "10.2",
        "884": "10.2",
        "885": "10.2",
        "886": "10.2",
        "887": "10.2",
        "888": "10.2",
        "889": "10.2",
        "890": "10.2",
        "891": "10.2",
        "892": "10.2",
        "893": "10.2",
        "894": "10.2",
        "895": "10.2",
        "896": "10.2",
        "897": "10.2",
        "898": "10.2",
        "899": "10.2",
        "900": "10.2",
        "901": "10.3",
        "902": "10.3",
        "903": "10.3",
        "904": "10.3",
        "905": "10.3",
        "906": "10.3",
        "907": "10.3",
        "908": "10.3",
        "909": "10.3",
        "910": "10.3",
        "911": "10.3",
        "912": "10.3",
        "913": "10.3",
        "914": "10.3",
        "915": "10.3",
        "916": "10.3",
        "917": "10.3",
        "918": "10.3",
        "919": "10.3",
        "920": "10.3",
        "921": "10.3",
        "922": "10.4",
        "923": "10.4",
        "924": "10.4",
        "925": "10.4",
        "926": "10.4",
        "927": "10.4",
        "928": "10.4",
        "929": "10.4",
        "930": "10.4",
        "931": "10.4",
        "932": "10.4",
        "933": "10.4",
        "934": "10.5",
        "935": "10.5",
        "936": "10.5",
        "937": "10.5",
        "938": "10.5",
        "939": "10.5",
        "940": "10.5",
        "941": "10.5",
        "942": "10.5",
        "943": "10.5",
        "944": "10.5",
        "945": "10.6",
        "946": "10.6",
        "947": "10.6",
        "948": "10.6",
        "949": "10.7",
        "950": "10.7",
        "951": "10.7",
        "952": "10.7",
        "953": "10.7",
        "954": "10.7",
        "955": "10.7",
        "956": "10.7",
        "957": "10.7",
        "958": "10.7",
        "959": "10.8",
        "960": "10.8",
        "961": "10.8",
        "962": "10.8",
        "963": "10.8",
        "964": "10.9",
        "965": "10.9",
        "966": "10.9",
        "967": "10.9",
        "968": "10.9",
        "969": "10.9",
        "970": "10.9",
        "971": "10.9",
        "972": "10.9",
        "973": "10.9",
        "974": "10.9",
        "975": "10.9",
        "976": "10.9",
        "977": "10.9",
        "978": "10.9",
        "979": "10.9",
        "980": "10.9",
        "981": "10.10",
        "982": "10.10",
        "983": "10.10",
        "984": "10.10",
        "985": "10.10",
        "986": "10.10",
        "987": "10.10",
        "988": "10.10",
        "989": "10.10",
        "990": "10.10",
        "991": "10.10",
        "992": "10.10",
        "993": "10.10",
        "994": "10.11",
        "995": "10.11",
        "996": "10.11",
        "997": "10.11",
        "998": "10.12",
        "999": "10.12",
        "1000": "10.12",
        "1001": "10.12",
        "1002": "10.12",
        "1003": "10.12"
    }
}